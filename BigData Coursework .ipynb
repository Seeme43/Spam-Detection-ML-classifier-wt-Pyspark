{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='EC2C04'>  SPAM DETECTION SYSTEM with Ling-Spam COLLECTION </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes the tasks performed in the **second part** of the Resit Coursework of the Big Data Applications module. For a complete description of the purposes of the analysis, please consult the PDF document provided which also represent the first part of the Coursework.\n",
    "This script goes through the entire process that is necessary to create and obtain the optimal model able to identify whether a message represents a Spam or Ham.\n",
    "In particular, the phases in which the document is organised are summarised as following:\n",
    "\n",
    "1. **Access and Import the File**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/98/244399c0daa7894cdf387e7007d5e8b3710a79b67f3fd991c0b0b644822d/pyspark-2.4.3.tar.gz (215.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 215.6MB 156kB/s ta 0:00:011    84% |███████████████████████████     | 182.1MB 11.6MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K    100% |████████████████████████████████| 204kB 7.6MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Running setup.py bdist_wheel for pyspark ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/simonezanetti/Library/Caches/pip/wheels/8d/20/f0/b30e2024226dc112e256930dd2cd4f06d00ab053c86278dcf3\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.3\n"
     ]
    }
   ],
   "source": [
    "# PYSPARK INSTALLATION\n",
    "!pip install pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE PACKAGES \n",
    "import pyspark\n",
    "from pathlib import Path\n",
    "import re\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE CONTEXT\n",
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/simonezanetti/Desktop/lingspam_public\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "cd /Users/simonezanetti/Desktop/lingspam_public\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This directory contains the Ling-Spam corpus, as described in the \r\n",
      "paper:\r\n",
      "\r\n",
      "I. Androutsopoulos, J. Koutsias, K.V. Chandrinos, George Paliouras, \r\n",
      "and C.D. Spyropoulos, \"An Evaluation of Naive Bayesian Anti-Spam \r\n",
      "Filtering\". In Potamias, G., Moustakis, V. and van Someren, M. (Eds.), \r\n",
      "Proceedings of the Workshop on Machine Learning in the New Information \r\n",
      "Age, 11th European Conference on Machine Learning (ECML 2000), \r\n",
      "Barcelona, Spain, pp. 9-17, 2000.\r\n",
      "\r\n",
      "There are four subdirectories, corresponding to four versions of \r\n",
      "the corpus:\r\n",
      "\r\n",
      "bare: Lemmatiser disabled, stop-list disabled.\r\n",
      "lemm: Lemmatiser enabled, stop-list disabled.\r\n",
      "lemm_stop: Lemmatiser enabled, stop-list enabled.\r\n",
      "stop: Lemmatiser disabled, stop-list enabled.\r\n",
      "\r\n",
      "Each one of these 4 directories contains 10 subdirectories (part1, \r\n",
      "..., part10). These correspond to the 10 partitions of the corpus \r\n",
      "that were used in the 10-fold experiments. In each repetition, one \r\n",
      "part was reserved for testing and the other 9 were used for training. \r\n",
      "\r\n",
      "Each one of the 10 subdirectories contains both spam and legitimate \r\n",
      "messages, one message in each file. Files whose names have the form\r\n",
      "spmsg*.txt are spam messages. All other files are legitimate messages.\r\n",
      "\r\n",
      "By obtaining a copy of this corpus you agree to acknowledge the use \r\n",
      "and origin of the corpus in any published work of yours that makes \r\n",
      "use of the corpus, and to notify the person below about this work.\r\n",
      "\r\n",
      "Ion Androutsopoulos \r\n",
      "http://www.aueb.gr/users/ion/\r\n",
      "Ling-Spam corpus last updated: July 17, 2000\r\n",
      "This file (readme.txt) last updated: July 30, 2003.\r\n"
     ]
    }
   ],
   "source": [
    "!cat readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssh szane001@dsm10.doc.gold.ac.uk\n",
    "#cd BigDataResit/lingspam_public\n",
    "#pyspark --master yarn\n",
    "#import pyspark\n",
    "#sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1) Read the dataset and create RDDs \n",
    "\n",
    "1) The first step is aimed to \n",
    "\n",
    "\n",
    "a) Start by reading the directory with text files from the file system (`/content/drive/My Drive/BigData/data/lingspam_public`). Load all text files per dirctory (part1,part2, ... ,part10) using `wholeTextFiles()`, which creates one RDD per part, containing tuples (filename,text). This is a good choice as the text files are small. (5%)\n",
    "\n",
    "b) We will use one of the RDDs as test set, the rest as training set. For the training set you need to create the union of the remaining RDDs. (5%)\n",
    "\n",
    "b) Remove the path and extension from the filename using the regular expression provided (5%).\n",
    "\n",
    "If the filename starts with 'spmsg' it is spam, otherwise it is not. We'll use that later to train a classifier. \n",
    "\n",
    "We will put the code in each cell into a function that we can reuse later. In this way we can develop the whole preprocessing with the smaller test set and apply it to the training set once we know that everything works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bare/part3 bare/part4 bare/part5 bare/part2 bare/part10 bare/part9 bare/part7 bare/part1 bare/part6 bare/part8\n",
      "[('file:/Users/simonezanetti/Desktop/lingspam_public/bare/part4/6-266msg3.txt', 'Subject: bisfai deadline extension !\\n\\nbisfai deadline extension ! the deadline for the bar - ilan symposium on foundations of artificial intelligence has been extended to february 27 . the conference itself will take place as scheduled , june 20-22 , in ramat - gan and jerusalem , israel . for more information contact : bisfai @ bimacs . cs . biu . ac . il daniel radzinski tovna translation machines jerusalem , israel dr @ tovna . co . il\\n')]\n"
     ]
    }
   ],
   "source": [
    "# EXTRACT THE TEXT FILES AND STORE THEM INTO RDD FILES\n",
    "\n",
    "# Goal is Extracting the text files from one of the subdirectories of the folder lingspam_public, by iterating and storing the\n",
    "# content of each of the directory created ( set in the code below ) as RDD file. Find the article that says it's the best choice\n",
    "# when files are small\n",
    "\n",
    "p = Path('bare')  # Define the Path that in this case is represented by the folder Bare in the lingspam_public folder/directory\n",
    "print(*p.iterdir()) # Check that this is going to do what you want -> Obtaining as directories all the parts within the folder\n",
    "                    # You will after iterate over all of them to assign their content into an RDD file.\n",
    "dirs = list(p.iterdir()) # Store each directory path in a list, in order to iterate that and be able to obtain the RDD's. \n",
    "# Set the loop that allows to store the content of each directory as Rdd within a RddList.\n",
    "rddList = []\n",
    "for d in dirs:\n",
    "    # iterate through the directories\n",
    "    rdd= sc.wholeTextFiles(str(d.resolve()))\n",
    "    rddList.append(rdd)\n",
    "# Check you have 10 directories in the list\n",
    "len(rddList)\n",
    "# Check that it worked \n",
    "print(rddList[1].take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/Users/simonezanetti/Desktop/lingspam_public/bare/part3/6-110msg3.txt',\n",
       "  \"Subject: job posting - apple-iss research center\\n\\ncontent - length : 3386 apple-iss research center a us $ 10 million joint venture between apple computer inc . and the institute of systems science of the national university of singapore , located in singapore , is looking for : a senior speech scientist - - - - - - - - - - - - - - - - - - - - - - - - - the successful candidate will have research expertise in computational linguistics , including natural language processing and * * english * * and * * chinese * * statistical language modeling . knowledge of state-of - the-art corpus-based n - gram language models , cache language models , and part-of - speech language models are required . a text - to - speech project leader - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - the successful candidate will have research expertise expertise in two or more of the following areas : computational linguistics , including natural language parsing , lexical database design , and statistical language modeling ; text tokenization and normalization ; prosodic analysis . substantial knowledge of the phonology , syntax , and semantics of chinese is required . knowledge of acoustic phonetics and / or speech signal processing is desirable . both candidates will have a phd with at least 2 to 4 years of relevant work experience , or a technical msc degree with at least 5 to 7 years of experienc e . very strong software engineering skills , including design and implementation , and productization are required in these positions . knowledge of c , c + + and unix are preferred . a unix & c programmer - - - - - - - - - - - - - - - - - - - - we are looking for an experienced unix & c programmer , preferably with good industry experience , to join us in breaking new frontiers . strong knowledge of unix tools ( compilers , linkers , make , x - windows , e - mac , . . . ) and experience in matlab required . sun and silicon graphic experience is an advantage . programmers with less than two years industry experience need not apply . these positions include interaction with scientists in the national university of singapore , and with apple 's speech research and productization efforts located in cupertino , california . attendance and publication in international scientific / engineering conferences is encouraged . benefits include an internationally competitive salary , housing subsidy , and relocation expenses . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ send a complete resume , enclosing personal particulars , qualifications , experience and contact telephone number to : mr jean - luc lebrun center manager apple - iss research center , institute of systems science heng mui keng terrace , singapore 0511 tel : ( 65 ) 772-6571 fax : ( 65 ) 776-4005 email : jllebrun @ iss . nus . sg\\n\")]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SPLIT TRAIN AND TEST DATA \n",
    "\n",
    "# Define your Test Data\n",
    "testRDD1 = rddList[9]\n",
    "\n",
    "\n",
    "trainRDD1 = sc.emptyRDD() # Set an empty Rdd and Unite all the Rdd's left for Training together \n",
    "for i in range(0,9):\n",
    "        trainRDD1 = trainRDD1.union(rddList[i])\n",
    "\n",
    "# Check how the content of the RDD looks like \n",
    "trainRDD1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each file still contains the path specification, which at this point is useless.\n",
    "# This will be removed using \n",
    "\n",
    "testRDD = testRDD1.map(lambda x: (re.split('[/\\.]', x[0])[-2], x[1]))  # <- CHECK THIS. The less clear to me \n",
    "trainRDD = trainRDD1.map(lambda x: (re.split('[/\\.]', x[0])[-2], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testRDD.count():  289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('6-110msg3',\n",
       "  \"Subject: job posting - apple-iss research center\\n\\ncontent - length : 3386 apple-iss research center a us $ 10 million joint venture between apple computer inc . and the institute of systems science of the national university of singapore , located in singapore , is looking for : a senior speech scientist - - - - - - - - - - - - - - - - - - - - - - - - - the successful candidate will have research expertise in computational linguistics , including natural language processing and * * english * * and * * chinese * * statistical language modeling . knowledge of state-of - the-art corpus-based n - gram language models , cache language models , and part-of - speech language models are required . a text - to - speech project leader - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - the successful candidate will have research expertise expertise in two or more of the following areas : computational linguistics , including natural language parsing , lexical database design , and statistical language modeling ; text tokenization and normalization ; prosodic analysis . substantial knowledge of the phonology , syntax , and semantics of chinese is required . knowledge of acoustic phonetics and / or speech signal processing is desirable . both candidates will have a phd with at least 2 to 4 years of relevant work experience , or a technical msc degree with at least 5 to 7 years of experienc e . very strong software engineering skills , including design and implementation , and productization are required in these positions . knowledge of c , c + + and unix are preferred . a unix & c programmer - - - - - - - - - - - - - - - - - - - - we are looking for an experienced unix & c programmer , preferably with good industry experience , to join us in breaking new frontiers . strong knowledge of unix tools ( compilers , linkers , make , x - windows , e - mac , . . . ) and experience in matlab required . sun and silicon graphic experience is an advantage . programmers with less than two years industry experience need not apply . these positions include interaction with scientists in the national university of singapore , and with apple 's speech research and productization efforts located in cupertino , california . attendance and publication in international scientific / engineering conferences is encouraged . benefits include an internationally competitive salary , housing subsidy , and relocation expenses . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ send a complete resume , enclosing personal particulars , qualifications , experience and contact telephone number to : mr jean - luc lebrun center manager apple - iss research center , institute of systems science heng mui keng terrace , singapore 0511 tel : ( 65 ) 772-6571 fax : ( 65 ) 776-4005 email : jllebrun @ iss . nus . sg\\n\"),\n",
       " ('6-126msg1',\n",
       "  'Subject: \\n\\nlang classification grimes , joseph e . and barbara f . grimes ; ethnologue language family index ; pb . isbn : 0-88312 - 708 - 3 ; vi , 116 pp . ; $ 14 . 00 . summer institute of linguistics . this companion volume to ethnologue : languages of the world , twelfth edition lists language families of the world with sub-groups shown in a tree arrangement under the broadest classification of language family . the language family index facilitates locating language names in the ethnologue , making the data there more accessible . internet : academic . books @ sil . org languages , reference lang & culture gregerson , marilyn ; ritual , belief , and kinship in sulawesi ; pb . : isbn : 0-88312 - 621 - 4 ; ix , 194 pp . ; $ 25 . 00 . summer institute of linguistics . seven articles discuss five language groups in sulawesi , indonesia ; the primary focus is on cultural matters , with some linguistic content . topics include traditional religion and beliefs , certain ceremonies , and kinship . internet : academic . books @ sil . org language and society , indonesia computers & ling weber , david j . , stephen r . mcconnel , diana d . weber , and beth j . bryson ; primer : a tool for developing early reading materials ; pb . : isbn : 0-88313 - 678 - 8 ; xvi , 266 pp . + ms-dos software ; $ 26 . 00 . summer institute of linguistics . the authors present a computer program and instructions for developing reading materials in languages with little or no background in literacy . the book is structured as a how-to manual with step by step procedures to establish an appropriate primer sequence and to organize words , phrases , and sentences that correlate with the sequence . it presupposes a thorough knowledge of linguistics . internet : academic . books @ sil . org literacy , computer\\n')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the length of both the files and have a look \n",
    "\n",
    "print('testRDD.count(): ',testRDD2.count()) # should be ~291 \n",
    "#print('trainRDD.count(): ',trainRDD.count()) # should be ~2602 - commented out to save time as it takes some time to create RDD from all the files\n",
    "\n",
    "trainRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testRDD.getNumPartitions() 2\n",
      "testRDD.getStorageLevel() Serialized 1x Replicated\n",
      "testRDD.take(1):  [('8-1064msg1', 'Subject: re : 8 . 1044 , disc : grammar in schools\\n\\n( re message from : linguist @ linguistlist . org ) > > linguist list : vol-8 - 1044 . sat jul 12 1997 . issn : 1068-4875 . > > subject : 8 . 1044 , disc : grammar in schools > > i know and teach that not all infinitives contain ` to \\' . i also give > the students examples ( e . g . ` i asked him to kindly apologise \\' ) where > placing the adverb anywhere else would cause ambiguity . > > jennifer chew an example i once concocted to justify \" splitting the infintive \" ( or not , as the case may be ) is : a ) after a heavy meal , i prepared slowly to go home digesting b ) after a heavy meal , i prepared to slowly go home digesting c ) after a heavy meal , i prepared to go home slowly digesting in this context , with the possible exception of the third case , the natural ( and therefore near-enough unambiguous ) association of the adverb is as follows : a ) after a heavy meal , i prepared _ slowly to go home digesting b ) after a heavy meal , i prepared to slowly _ go _ home digesting c ) after a heavy meal , i prepared to go home slowly _ digesting ( this was long ago , when you got glared at for splitting an infinitive regardless of whether it was the only place to put the adverb so as to express what you meant and not something else : this example achieved , as nearly as i could , three quite distinct and natural meanings for \" . . . slowly to go home . . . \" , \" . . . to slowly go home . . . \" and \" . . . to go home slowly . . . \" . i \\' m not 100 per cent happy with it , for obvious reasons , and it would be interesting to see if anyone can come up with a better , more clear-cut one ) . [ and , to really \" epater les bourgeois \" , i reckon you could even make a case for \" . . . i prepared to , slowly , go home digesting \" : the implication being that the meal was so very heavy that the walk home should be correpondingly delicate , as emphasised by the pause in rhythm marked by the commas ] . ted . ( ted . harding @ nessie . mcc . ac . uk )\\n'), ('6-806msg1', 'Subject: swadesh list\\n\\ndoes anyone have a copy of the swadesh word list at hand ? i should like to get a copy by email as soon as is practicable . thanks in advance . adams bodomo bodomo @ csli . stanford . edu\\n'), ('6-816msg1', \"Subject: summary : half a day\\n\\ndear readers , many thanks to all respondents ( 51 ) who sent replies to my query about time phrases and native-speaker judgments . there were too many replies to acknowledge individually . many of you asked for a summary , so here goes . # # = strongly preferred , # = good , ? = awkward , x = bad , yuck the figures following each phrase give the number of votes cast in each category . they do n't all add up to 51 because some respondents ( rs ) expressed only their first preference . # # # ? x 1 the family spent . . . . . . . in ipswich . a . a day and a half 24 26 0 0 b . one and a half days 1 31 7 7 c . thirty-six hours 1 29 6 9 this makes [ a ] a clear winner , but for many rs [ b ] and [ c ] are acceptable , too , depending on context . comments : a . implies enough time to socialize ; implies most of sunday spent in ipswich , leaving at noon on monday b . implies ipswich was part of a series of visits ; implies an overnight stay ; part of a list ; stilted c . military / aeronautical ; whirlwind tour ; every moment packed with feverish activity ; at a conference - one and a half days spent on syntax ; working under time pressure ; airplane layover / waystation ; ok for negative experiences ( flu / jail ) . ( i take it a ' layover ' is us for british 's topover ' , or maybe it 's a stopover with a visit to a girlfriend ? ) 2 . it took me . . . . . . . . to write the book . a . six months 16 34 0 0 b . a half-year 0 10 9 25 c . half a year 3 44 1 1 while there is a preference for [ a ] , [ c ] is not far behind . [ b ] is problematic . comments : a . feels shorter ; not as much effort required as ' half a year ' b . suggests an academic half-year ; yuck , unnatural ; sounds non-native or british ( from american rs ) ; sounds american ( from british rs ) ; ok in financial contexts - a half year is either the first or second half , not an arbitrary 6 month period c . sounds longer than six months ; emphatic , with stress on ' year ' - 3 we ' ll be leaving in . . . . . . . a . half an hour 6 44 0 0 b . a half-hour 2 24 10 11 [ a ] wins . [ b ] splits people into two roughly equal camps . comments : b . sounds formal ; awkward ; funny ; sounds normal - we ' ll be leaving inna haf our ( from now ) . 4 tom worked for . . . . . . . . . in a lab a . a year and a half 13 37 1 0 b . one and a half years 0 32 6 7 c . eighteen months 3 39 6 0 while [ a ] wins , the other two are n't so far behind . comments : a . least exact . b . more exact ; part of calculation ( eg for pension ) ; stilted ; fussy . c . most exact ; emphasizes duration ; ok for children 's ages upto two years ; ok in contexts where precision is required ; suggests tom was less involved in the job ; maybe a temporary job . using months for time greater than a year , and hours for time greater than a day makes the time seem more rushed ; half an inch is ok but not half a foot , but half a yard is ok if bying cloth even though we do n't normally speak of half a yard . the overall impression that i get is that context and pragmatic considerations determine which lexical item will be acceptable in any given slot , and even then there is considerably more tolerance for some expressions than i would have thought possible / probable . three rs said all eleven phrases are 100 % ok . i have n't given details of rs ' background / nationality etc , since not all rs gave me details . however , about three-quarters of replies came from the usa . i hope this has been of some interest . many thanks for your response . roger maylor dept of linguistics and english language university of durham , uk\\n\")]\n"
     ]
    }
   ],
   "source": [
    "print('testRDD.getNumPartitions()',testRDD2.getNumPartitions()) # normally 2 on Colab (single machine)\n",
    "print('testRDD.getStorageLevel()',testRDD2.getStorageLevel()) # Serialized, 1x Replicated \n",
    "print('testRDD.take(1): ',testRDD2.take(3)) # should be (filename,[tokens]) \n",
    "rdd1 = testRDD # use this for developemnt in the next tasks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2) Tokenize and remove punctuation\n",
    "\n",
    "Now we need to split the words, a process called *tokenization* by linguists, and remove punctuation. \n",
    "\n",
    "We will use the Python [Natural Language Toolkit](http://www.nltk.org) *NLTK* to do the tokenization (rather than splitting ourselves, as these specialist tools usually do that better than we can ourselves). We use the NLTK function word_tokenize, see here for a code example: [http://www.nltk.org/book/ch03.html](http://www.nltk.org/book/ch03.html). 5%\n",
    "\n",
    "Then we will remove punctuation. There is no specific funtion for this, so we use a regular expression (see here for info [https://docs.python.org/3/library/re.html?highlight=re#module-re](https://docs.python.org/3/library/re.html?highlight=re#module-re)) in a list comprehension (here's a nice visual explanation: [http://treyhunner.com/2015/12/python-list-comprehensions-now-in-color/](http://treyhunner.com/2015/12/python-list-comprehensions-now-in-color/)). 5% \n",
    "\n",
    "We use a new technique here: we separate keys and values of the RDD, using the RDD functions `keys()` and `values()`, which yield each a new RDD. Then we process the values and *zip* them together with the keys again. See here for documentation: [http://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.zip](http://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.zip).  We wrap the whole sequence into one function `prepareTokenRDD` for later use. 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/simonezanetti/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/simonezanetti/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\" Apply the nltk.word_tokenize() method to our text, return the token list. \"\"\"\n",
    "nltk.download('punkt') # this loads the standard NLTK tokenizer model \n",
    "    # it is important that this is done here in the function, as it needs to be done on every worker.\n",
    "    # If we do the download outside a this function, it would only be executed on the driver     \n",
    "#>>>    return ... # use the nltk function word_tokenize\n",
    "nltk.word_tokenize(text)\n",
    "    \n",
    "def removePunctuation(tokens):\n",
    "    \"\"\" Remove punctuation characters from all tokens in a provided list. \"\"\"\n",
    "    # this will remove all punctiation from string s: re.sub('[()\\[\\],.?!\";_]','',s)\n",
    "#>>>    tokens2 =  [re.sub('[()\\[\\],.?!\";_]','',s) for i in s ] # use a list comprehension to remove punctuaton\n",
    "    tokens2 =  [re.sub('[()\\[\\],.?!\";_]','',s) for s in tokens ]\n",
    "    return tokens2\n",
    "    \n",
    "def prepareTokenRDD(fn_txt_RDD):\n",
    "    \"\"\" Take an RDD with (filename,text) elements and transform it into a (filename,[token ...]) RDD without punctuation characters. \"\"\"\n",
    "\n",
    "    \n",
    "nltk.download('punkt')   \n",
    "rdd_vals2 = rdd1.values() # It's convenient to process only the values. \n",
    "rdd_vals3 = rdd_vals2.map(nltk.word_tokenize) # Create a tokenised version of the values by mapping\n",
    "\n",
    "\n",
    "#tokens2 =  [re.sub('[()\\[\\],.?!\";_]','',s) for s in rdd_vals3 ]\n",
    "#rdd_vals4 = rdd_vals3.map([re.sub('[()\\[\\],.?!\";_]','')])# remove punctuation from the values\n",
    "rdd_kv = rdd1.keys().zip(rdd_vals3)\n",
    "                           \n",
    "                           \n",
    "                           # we zip the two RDDs together \n",
    "# i.e. produce tuples with one itme from each RDD.\n",
    "# This works because we have only applied mappings to the values, \n",
    "# therefore the items in both RDDs are still aligned.\n",
    "# now remove any empty value strings (i.e. length 0) that we may have created by removing punctiation.\n",
    "# >>> now remove any empty strings (i.e. length 0) that we may have\n",
    "# created by removing punctuation, and resulting entries without words left.\n",
    "#rdd_kvr = rdd_kv.map(lambda x: (x[0], [s for s in x[1] if len(s)>0])) # remove empty strings using RDD.map and a lambda. TIP len(s) gives you the lenght of string. \n",
    "#rdd_kvrf = rdd_kvr.filter(lambda x: len(x[0]) > 0) # remove items without tokens using RDD.filter and a lambda. \n",
    "#rdd2 = rdd_kvrf    \n",
    "    \n",
    "    \n",
    "    # >>> Question: why should this be filtering done after zipping the keys and values together?\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    # Answer : \n",
    "    # The main reason to do the filtering step after the zipping is because we need to have the same number of values and keys.\n",
    "    # The filtering step should be done after the zipping because if we would have used the zipping and then the tokenization/remove-punctuation we would risk to loose the pairing keys\n",
    "    # \n",
    "    # After applying tokenization and remove punctuation only in the values, and after zipping together the values and the keys, we produce tuples with one item from each RDD.\n",
    "    # We can now remove empty string created after the removal of punctuation.\n",
    "    #\n",
    "   \n",
    "    #return rdd_kvrf \n",
    "\n",
    "#rdd2 = prepareTokenRDD(rdd1) # Use a small RDD for testing.\n",
    "#print(rdd2.take(1)) # For checking result of task 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('8-1064msg1',\n",
       "  'Subject: re : 8 . 1044 , disc : grammar in schools\\n\\n( re message from : linguist @ linguistlist . org ) > > linguist list : vol-8 - 1044 . sat jul 12 1997 . issn : 1068-4875 . > > subject : 8 . 1044 , disc : grammar in schools > > i know and teach that not all infinitives contain ` to \\' . i also give > the students examples ( e . g . ` i asked him to kindly apologise \\' ) where > placing the adverb anywhere else would cause ambiguity . > > jennifer chew an example i once concocted to justify \" splitting the infintive \" ( or not , as the case may be ) is : a ) after a heavy meal , i prepared slowly to go home digesting b ) after a heavy meal , i prepared to slowly go home digesting c ) after a heavy meal , i prepared to go home slowly digesting in this context , with the possible exception of the third case , the natural ( and therefore near-enough unambiguous ) association of the adverb is as follows : a ) after a heavy meal , i prepared _ slowly to go home digesting b ) after a heavy meal , i prepared to slowly _ go _ home digesting c ) after a heavy meal , i prepared to go home slowly _ digesting ( this was long ago , when you got glared at for splitting an infinitive regardless of whether it was the only place to put the adverb so as to express what you meant and not something else : this example achieved , as nearly as i could , three quite distinct and natural meanings for \" . . . slowly to go home . . . \" , \" . . . to slowly go home . . . \" and \" . . . to go home slowly . . . \" . i \\' m not 100 per cent happy with it , for obvious reasons , and it would be interesting to see if anyone can come up with a better , more clear-cut one ) . [ and , to really \" epater les bourgeois \" , i reckon you could even make a case for \" . . . i prepared to , slowly , go home digesting \" : the implication being that the meal was so very heavy that the walk home should be correpondingly delicate , as emphasised by the pause in rhythm marked by the commas ] . ted . ( ted . harding @ nessie . mcc . ac . uk )\\n')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/simonezanetti/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # Apply the Punkt sentence tokenizer: the standard tokenizer for English language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_vals2 = rdd1.values() # This leaves out the key ( the filename. ex: '8-1064msg1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subject: re : 8 . 1044 , disc : grammar in schools\\n\\n( re message from : linguist @ linguistlist . org ) > > linguist list : vol-8 - 1044 . sat jul 12 1997 . issn : 1068-4875 . > > subject : 8 . 1044 , disc : grammar in schools > > i know and teach that not all infinitives contain ` to \\' . i also give > the students examples ( e . g . ` i asked him to kindly apologise \\' ) where > placing the adverb anywhere else would cause ambiguity . > > jennifer chew an example i once concocted to justify \" splitting the infintive \" ( or not , as the case may be ) is : a ) after a heavy meal , i prepared slowly to go home digesting b ) after a heavy meal , i prepared to slowly go home digesting c ) after a heavy meal , i prepared to go home slowly digesting in this context , with the possible exception of the third case , the natural ( and therefore near-enough unambiguous ) association of the adverb is as follows : a ) after a heavy meal , i prepared _ slowly to go home digesting b ) after a heavy meal , i prepared to slowly _ go _ home digesting c ) after a heavy meal , i prepared to go home slowly _ digesting ( this was long ago , when you got glared at for splitting an infinitive regardless of whether it was the only place to put the adverb so as to express what you meant and not something else : this example achieved , as nearly as i could , three quite distinct and natural meanings for \" . . . slowly to go home . . . \" , \" . . . to slowly go home . . . \" and \" . . . to go home slowly . . . \" . i \\' m not 100 per cent happy with it , for obvious reasons , and it would be interesting to see if anyone can come up with a better , more clear-cut one ) . [ and , to really \" epater les bourgeois \" , i reckon you could even make a case for \" . . . i prepared to , slowly , go home digesting \" : the implication being that the meal was so very heavy that the walk home should be correpondingly delicate , as emphasised by the pause in rhythm marked by the commas ] . ted . ( ted . harding @ nessie . mcc . ac . uk )\\n']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.values().take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_vals3 = rdd_vals2.map(nltk.word_tokenize)  # This tokenise the RDDs by applying work_tokenize to each of them through the map function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Subject',\n",
       "  ':',\n",
       "  're',\n",
       "  ':',\n",
       "  '8',\n",
       "  '.',\n",
       "  '1044',\n",
       "  ',',\n",
       "  'disc',\n",
       "  ':',\n",
       "  'grammar',\n",
       "  'in',\n",
       "  'schools',\n",
       "  '(',\n",
       "  're',\n",
       "  'message',\n",
       "  'from',\n",
       "  ':',\n",
       "  'linguist',\n",
       "  '@',\n",
       "  'linguistlist',\n",
       "  '.',\n",
       "  'org',\n",
       "  ')',\n",
       "  '>',\n",
       "  '>',\n",
       "  'linguist',\n",
       "  'list',\n",
       "  ':',\n",
       "  'vol-8',\n",
       "  '-',\n",
       "  '1044',\n",
       "  '.',\n",
       "  'sat',\n",
       "  'jul',\n",
       "  '12',\n",
       "  '1997',\n",
       "  '.',\n",
       "  'issn',\n",
       "  ':',\n",
       "  '1068-4875',\n",
       "  '.',\n",
       "  '>',\n",
       "  '>',\n",
       "  'subject',\n",
       "  ':',\n",
       "  '8',\n",
       "  '.',\n",
       "  '1044',\n",
       "  ',',\n",
       "  'disc',\n",
       "  ':',\n",
       "  'grammar',\n",
       "  'in',\n",
       "  'schools',\n",
       "  '>',\n",
       "  '>',\n",
       "  'i',\n",
       "  'know',\n",
       "  'and',\n",
       "  'teach',\n",
       "  'that',\n",
       "  'not',\n",
       "  'all',\n",
       "  'infinitives',\n",
       "  'contain',\n",
       "  '`',\n",
       "  'to',\n",
       "  \"'\",\n",
       "  '.',\n",
       "  'i',\n",
       "  'also',\n",
       "  'give',\n",
       "  '>',\n",
       "  'the',\n",
       "  'students',\n",
       "  'examples',\n",
       "  '(',\n",
       "  'e',\n",
       "  '.',\n",
       "  'g',\n",
       "  '.',\n",
       "  '`',\n",
       "  'i',\n",
       "  'asked',\n",
       "  'him',\n",
       "  'to',\n",
       "  'kindly',\n",
       "  'apologise',\n",
       "  \"'\",\n",
       "  ')',\n",
       "  'where',\n",
       "  '>',\n",
       "  'placing',\n",
       "  'the',\n",
       "  'adverb',\n",
       "  'anywhere',\n",
       "  'else',\n",
       "  'would',\n",
       "  'cause',\n",
       "  'ambiguity',\n",
       "  '.',\n",
       "  '>',\n",
       "  '>',\n",
       "  'jennifer',\n",
       "  'chew',\n",
       "  'an',\n",
       "  'example',\n",
       "  'i',\n",
       "  'once',\n",
       "  'concocted',\n",
       "  'to',\n",
       "  'justify',\n",
       "  '``',\n",
       "  'splitting',\n",
       "  'the',\n",
       "  'infintive',\n",
       "  '``',\n",
       "  '(',\n",
       "  'or',\n",
       "  'not',\n",
       "  ',',\n",
       "  'as',\n",
       "  'the',\n",
       "  'case',\n",
       "  'may',\n",
       "  'be',\n",
       "  ')',\n",
       "  'is',\n",
       "  ':',\n",
       "  'a',\n",
       "  ')',\n",
       "  'after',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'meal',\n",
       "  ',',\n",
       "  'i',\n",
       "  'prepared',\n",
       "  'slowly',\n",
       "  'to',\n",
       "  'go',\n",
       "  'home',\n",
       "  'digesting',\n",
       "  'b',\n",
       "  ')',\n",
       "  'after',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'meal',\n",
       "  ',',\n",
       "  'i',\n",
       "  'prepared',\n",
       "  'to',\n",
       "  'slowly',\n",
       "  'go',\n",
       "  'home',\n",
       "  'digesting',\n",
       "  'c',\n",
       "  ')',\n",
       "  'after',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'meal',\n",
       "  ',',\n",
       "  'i',\n",
       "  'prepared',\n",
       "  'to',\n",
       "  'go',\n",
       "  'home',\n",
       "  'slowly',\n",
       "  'digesting',\n",
       "  'in',\n",
       "  'this',\n",
       "  'context',\n",
       "  ',',\n",
       "  'with',\n",
       "  'the',\n",
       "  'possible',\n",
       "  'exception',\n",
       "  'of',\n",
       "  'the',\n",
       "  'third',\n",
       "  'case',\n",
       "  ',',\n",
       "  'the',\n",
       "  'natural',\n",
       "  '(',\n",
       "  'and',\n",
       "  'therefore',\n",
       "  'near-enough',\n",
       "  'unambiguous',\n",
       "  ')',\n",
       "  'association',\n",
       "  'of',\n",
       "  'the',\n",
       "  'adverb',\n",
       "  'is',\n",
       "  'as',\n",
       "  'follows',\n",
       "  ':',\n",
       "  'a',\n",
       "  ')',\n",
       "  'after',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'meal',\n",
       "  ',',\n",
       "  'i',\n",
       "  'prepared',\n",
       "  '_',\n",
       "  'slowly',\n",
       "  'to',\n",
       "  'go',\n",
       "  'home',\n",
       "  'digesting',\n",
       "  'b',\n",
       "  ')',\n",
       "  'after',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'meal',\n",
       "  ',',\n",
       "  'i',\n",
       "  'prepared',\n",
       "  'to',\n",
       "  'slowly',\n",
       "  '_',\n",
       "  'go',\n",
       "  '_',\n",
       "  'home',\n",
       "  'digesting',\n",
       "  'c',\n",
       "  ')',\n",
       "  'after',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'meal',\n",
       "  ',',\n",
       "  'i',\n",
       "  'prepared',\n",
       "  'to',\n",
       "  'go',\n",
       "  'home',\n",
       "  'slowly',\n",
       "  '_',\n",
       "  'digesting',\n",
       "  '(',\n",
       "  'this',\n",
       "  'was',\n",
       "  'long',\n",
       "  'ago',\n",
       "  ',',\n",
       "  'when',\n",
       "  'you',\n",
       "  'got',\n",
       "  'glared',\n",
       "  'at',\n",
       "  'for',\n",
       "  'splitting',\n",
       "  'an',\n",
       "  'infinitive',\n",
       "  'regardless',\n",
       "  'of',\n",
       "  'whether',\n",
       "  'it',\n",
       "  'was',\n",
       "  'the',\n",
       "  'only',\n",
       "  'place',\n",
       "  'to',\n",
       "  'put',\n",
       "  'the',\n",
       "  'adverb',\n",
       "  'so',\n",
       "  'as',\n",
       "  'to',\n",
       "  'express',\n",
       "  'what',\n",
       "  'you',\n",
       "  'meant',\n",
       "  'and',\n",
       "  'not',\n",
       "  'something',\n",
       "  'else',\n",
       "  ':',\n",
       "  'this',\n",
       "  'example',\n",
       "  'achieved',\n",
       "  ',',\n",
       "  'as',\n",
       "  'nearly',\n",
       "  'as',\n",
       "  'i',\n",
       "  'could',\n",
       "  ',',\n",
       "  'three',\n",
       "  'quite',\n",
       "  'distinct',\n",
       "  'and',\n",
       "  'natural',\n",
       "  'meanings',\n",
       "  'for',\n",
       "  '``',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'slowly',\n",
       "  'to',\n",
       "  'go',\n",
       "  'home',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '``',\n",
       "  ',',\n",
       "  '``',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'to',\n",
       "  'slowly',\n",
       "  'go',\n",
       "  'home',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '``',\n",
       "  'and',\n",
       "  '``',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'to',\n",
       "  'go',\n",
       "  'home',\n",
       "  'slowly',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '``',\n",
       "  '.',\n",
       "  'i',\n",
       "  \"'\",\n",
       "  'm',\n",
       "  'not',\n",
       "  '100',\n",
       "  'per',\n",
       "  'cent',\n",
       "  'happy',\n",
       "  'with',\n",
       "  'it',\n",
       "  ',',\n",
       "  'for',\n",
       "  'obvious',\n",
       "  'reasons',\n",
       "  ',',\n",
       "  'and',\n",
       "  'it',\n",
       "  'would',\n",
       "  'be',\n",
       "  'interesting',\n",
       "  'to',\n",
       "  'see',\n",
       "  'if',\n",
       "  'anyone',\n",
       "  'can',\n",
       "  'come',\n",
       "  'up',\n",
       "  'with',\n",
       "  'a',\n",
       "  'better',\n",
       "  ',',\n",
       "  'more',\n",
       "  'clear-cut',\n",
       "  'one',\n",
       "  ')',\n",
       "  '.',\n",
       "  '[',\n",
       "  'and',\n",
       "  ',',\n",
       "  'to',\n",
       "  'really',\n",
       "  '``',\n",
       "  'epater',\n",
       "  'les',\n",
       "  'bourgeois',\n",
       "  '``',\n",
       "  ',',\n",
       "  'i',\n",
       "  'reckon',\n",
       "  'you',\n",
       "  'could',\n",
       "  'even',\n",
       "  'make',\n",
       "  'a',\n",
       "  'case',\n",
       "  'for',\n",
       "  '``',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'i',\n",
       "  'prepared',\n",
       "  'to',\n",
       "  ',',\n",
       "  'slowly',\n",
       "  ',',\n",
       "  'go',\n",
       "  'home',\n",
       "  'digesting',\n",
       "  '``',\n",
       "  ':',\n",
       "  'the',\n",
       "  'implication',\n",
       "  'being',\n",
       "  'that',\n",
       "  'the',\n",
       "  'meal',\n",
       "  'was',\n",
       "  'so',\n",
       "  'very',\n",
       "  'heavy',\n",
       "  'that',\n",
       "  'the',\n",
       "  'walk',\n",
       "  'home',\n",
       "  'should',\n",
       "  'be',\n",
       "  'correpondingly',\n",
       "  'delicate',\n",
       "  ',',\n",
       "  'as',\n",
       "  'emphasised',\n",
       "  'by',\n",
       "  'the',\n",
       "  'pause',\n",
       "  'in',\n",
       "  'rhythm',\n",
       "  'marked',\n",
       "  'by',\n",
       "  'the',\n",
       "  'commas',\n",
       "  ']',\n",
       "  '.',\n",
       "  'ted',\n",
       "  '.',\n",
       "  '(',\n",
       "  'ted',\n",
       "  '.',\n",
       "  'harding',\n",
       "  '@',\n",
       "  'nessie',\n",
       "  '.',\n",
       "  'mcc',\n",
       "  '.',\n",
       "  'ac',\n",
       "  '.',\n",
       "  'uk',\n",
       "  ')']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_vals3.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(tokens):  # FIND OUT HOW TO CHANGE THIS FUNCTION !!!! \n",
    "    \"\"\" Remove punctuation characters from all tokens in a provided list. \"\"\"\n",
    "    # this will remove all punctiation from string s: re.sub('[()\\[\\],.?!\";_]','',s)\n",
    "#>>>    tokens2 =  [re.sub('[()\\[\\],.?!\";_]','',s) for i in s ] # use a list comprehension to remove punctuaton\n",
    "    tokens2 =  [re.sub('[()\\[\\],.\";_]','',s) for s in tokens ]\n",
    "    return tokens2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_vals4 = rdd_vals3.map(removePunctuation) # This is helpful to remove noise from the data, i.e we remove the punctuation that does not lead\n",
    "# any significant inside to the model.\n",
    "# However, we keep the ! and ? as spam tend to often have these simbols as a way for advertisment to give enthusiasm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Subject',\n",
       "  ':',\n",
       "  're',\n",
       "  ':',\n",
       "  '8',\n",
       "  '',\n",
       "  '1044',\n",
       "  '',\n",
       "  'disc',\n",
       "  ':',\n",
       "  'grammar',\n",
       "  'in',\n",
       "  'schools',\n",
       "  '',\n",
       "  're',\n",
       "  'message',\n",
       "  'from',\n",
       "  ':',\n",
       "  'linguist',\n",
       "  '@',\n",
       "  'linguistlist',\n",
       "  '',\n",
       "  'org',\n",
       "  '',\n",
       "  '>',\n",
       "  '>',\n",
       "  'linguist',\n",
       "  'list',\n",
       "  ':',\n",
       "  'vol-8',\n",
       "  '-',\n",
       "  '1044',\n",
       "  '',\n",
       "  'sat',\n",
       "  'jul',\n",
       "  '12',\n",
       "  '1997',\n",
       "  '',\n",
       "  'issn',\n",
       "  ':',\n",
       "  '1068-4875',\n",
       "  '',\n",
       "  '>',\n",
       "  '>',\n",
       "  'subject',\n",
       "  ':',\n",
       "  '8',\n",
       "  '',\n",
       "  '1044',\n",
       "  '',\n",
       "  'disc',\n",
       "  ':',\n",
       "  'grammar',\n",
       "  'in',\n",
       "  'schools',\n",
       "  '>',\n",
       "  '>',\n",
       "  'i',\n",
       "  'know',\n",
       "  'and',\n",
       "  'teach',\n",
       "  'that',\n",
       "  'not',\n",
       "  'all',\n",
       "  'infinitives',\n",
       "  'contain',\n",
       "  '`',\n",
       "  'to',\n",
       "  \"'\",\n",
       "  '',\n",
       "  'i',\n",
       "  'also',\n",
       "  'give',\n",
       "  '>',\n",
       "  'the',\n",
       "  'students',\n",
       "  'examples',\n",
       "  '',\n",
       "  'e',\n",
       "  '',\n",
       "  'g',\n",
       "  '',\n",
       "  '`',\n",
       "  'i',\n",
       "  'asked',\n",
       "  'him',\n",
       "  'to',\n",
       "  'kindly',\n",
       "  'apologise',\n",
       "  \"'\",\n",
       "  '',\n",
       "  'where',\n",
       "  '>',\n",
       "  'placing',\n",
       "  'the',\n",
       "  'adverb',\n",
       "  'anywhere',\n",
       "  'else',\n",
       "  'would',\n",
       "  'cause',\n",
       "  'ambiguity',\n",
       "  '',\n",
       "  '>',\n",
       "  '>',\n",
       "  'jennifer',\n",
       "  'chew',\n",
       "  'an',\n",
       "  'example',\n",
       "  'i',\n",
       "  'once',\n",
       "  'concocted',\n",
       "  'to',\n",
       "  'justify',\n",
       "  '``',\n",
       "  'splitting',\n",
       "  'the',\n",
       "  'infintive',\n",
       "  '``',\n",
       "  '',\n",
       "  'or',\n",
       "  'not',\n",
       "  '',\n",
       "  'as',\n",
       "  'the',\n",
       "  'case',\n",
       "  'may',\n",
       "  'be',\n",
       "  '',\n",
       "  'is',\n",
       "  ':',\n",
       "  'a',\n",
       "  '',\n",
       "  'after',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'meal',\n",
       "  '',\n",
       "  'i',\n",
       "  'prepared',\n",
       "  'slowly',\n",
       "  'to',\n",
       "  'go',\n",
       "  'home',\n",
       "  'digesting',\n",
       "  'b',\n",
       "  '',\n",
       "  'after',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'meal',\n",
       "  '',\n",
       "  'i',\n",
       "  'prepared',\n",
       "  'to',\n",
       "  'slowly',\n",
       "  'go',\n",
       "  'home',\n",
       "  'digesting',\n",
       "  'c',\n",
       "  '',\n",
       "  'after',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'meal',\n",
       "  '',\n",
       "  'i',\n",
       "  'prepared',\n",
       "  'to',\n",
       "  'go',\n",
       "  'home',\n",
       "  'slowly',\n",
       "  'digesting',\n",
       "  'in',\n",
       "  'this',\n",
       "  'context',\n",
       "  '',\n",
       "  'with',\n",
       "  'the',\n",
       "  'possible',\n",
       "  'exception',\n",
       "  'of',\n",
       "  'the',\n",
       "  'third',\n",
       "  'case',\n",
       "  '',\n",
       "  'the',\n",
       "  'natural',\n",
       "  '',\n",
       "  'and',\n",
       "  'therefore',\n",
       "  'near-enough',\n",
       "  'unambiguous',\n",
       "  '',\n",
       "  'association',\n",
       "  'of',\n",
       "  'the',\n",
       "  'adverb',\n",
       "  'is',\n",
       "  'as',\n",
       "  'follows',\n",
       "  ':',\n",
       "  'a',\n",
       "  '',\n",
       "  'after',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'meal',\n",
       "  '',\n",
       "  'i',\n",
       "  'prepared',\n",
       "  '',\n",
       "  'slowly',\n",
       "  'to',\n",
       "  'go',\n",
       "  'home',\n",
       "  'digesting',\n",
       "  'b',\n",
       "  '',\n",
       "  'after',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'meal',\n",
       "  '',\n",
       "  'i',\n",
       "  'prepared',\n",
       "  'to',\n",
       "  'slowly',\n",
       "  '',\n",
       "  'go',\n",
       "  '',\n",
       "  'home',\n",
       "  'digesting',\n",
       "  'c',\n",
       "  '',\n",
       "  'after',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'meal',\n",
       "  '',\n",
       "  'i',\n",
       "  'prepared',\n",
       "  'to',\n",
       "  'go',\n",
       "  'home',\n",
       "  'slowly',\n",
       "  '',\n",
       "  'digesting',\n",
       "  '',\n",
       "  'this',\n",
       "  'was',\n",
       "  'long',\n",
       "  'ago',\n",
       "  '',\n",
       "  'when',\n",
       "  'you',\n",
       "  'got',\n",
       "  'glared',\n",
       "  'at',\n",
       "  'for',\n",
       "  'splitting',\n",
       "  'an',\n",
       "  'infinitive',\n",
       "  'regardless',\n",
       "  'of',\n",
       "  'whether',\n",
       "  'it',\n",
       "  'was',\n",
       "  'the',\n",
       "  'only',\n",
       "  'place',\n",
       "  'to',\n",
       "  'put',\n",
       "  'the',\n",
       "  'adverb',\n",
       "  'so',\n",
       "  'as',\n",
       "  'to',\n",
       "  'express',\n",
       "  'what',\n",
       "  'you',\n",
       "  'meant',\n",
       "  'and',\n",
       "  'not',\n",
       "  'something',\n",
       "  'else',\n",
       "  ':',\n",
       "  'this',\n",
       "  'example',\n",
       "  'achieved',\n",
       "  '',\n",
       "  'as',\n",
       "  'nearly',\n",
       "  'as',\n",
       "  'i',\n",
       "  'could',\n",
       "  '',\n",
       "  'three',\n",
       "  'quite',\n",
       "  'distinct',\n",
       "  'and',\n",
       "  'natural',\n",
       "  'meanings',\n",
       "  'for',\n",
       "  '``',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'slowly',\n",
       "  'to',\n",
       "  'go',\n",
       "  'home',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '``',\n",
       "  '',\n",
       "  '``',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'to',\n",
       "  'slowly',\n",
       "  'go',\n",
       "  'home',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '``',\n",
       "  'and',\n",
       "  '``',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'to',\n",
       "  'go',\n",
       "  'home',\n",
       "  'slowly',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '``',\n",
       "  '',\n",
       "  'i',\n",
       "  \"'\",\n",
       "  'm',\n",
       "  'not',\n",
       "  '100',\n",
       "  'per',\n",
       "  'cent',\n",
       "  'happy',\n",
       "  'with',\n",
       "  'it',\n",
       "  '',\n",
       "  'for',\n",
       "  'obvious',\n",
       "  'reasons',\n",
       "  '',\n",
       "  'and',\n",
       "  'it',\n",
       "  'would',\n",
       "  'be',\n",
       "  'interesting',\n",
       "  'to',\n",
       "  'see',\n",
       "  'if',\n",
       "  'anyone',\n",
       "  'can',\n",
       "  'come',\n",
       "  'up',\n",
       "  'with',\n",
       "  'a',\n",
       "  'better',\n",
       "  '',\n",
       "  'more',\n",
       "  'clear-cut',\n",
       "  'one',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'and',\n",
       "  '',\n",
       "  'to',\n",
       "  'really',\n",
       "  '``',\n",
       "  'epater',\n",
       "  'les',\n",
       "  'bourgeois',\n",
       "  '``',\n",
       "  '',\n",
       "  'i',\n",
       "  'reckon',\n",
       "  'you',\n",
       "  'could',\n",
       "  'even',\n",
       "  'make',\n",
       "  'a',\n",
       "  'case',\n",
       "  'for',\n",
       "  '``',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'i',\n",
       "  'prepared',\n",
       "  'to',\n",
       "  '',\n",
       "  'slowly',\n",
       "  '',\n",
       "  'go',\n",
       "  'home',\n",
       "  'digesting',\n",
       "  '``',\n",
       "  ':',\n",
       "  'the',\n",
       "  'implication',\n",
       "  'being',\n",
       "  'that',\n",
       "  'the',\n",
       "  'meal',\n",
       "  'was',\n",
       "  'so',\n",
       "  'very',\n",
       "  'heavy',\n",
       "  'that',\n",
       "  'the',\n",
       "  'walk',\n",
       "  'home',\n",
       "  'should',\n",
       "  'be',\n",
       "  'correpondingly',\n",
       "  'delicate',\n",
       "  '',\n",
       "  'as',\n",
       "  'emphasised',\n",
       "  'by',\n",
       "  'the',\n",
       "  'pause',\n",
       "  'in',\n",
       "  'rhythm',\n",
       "  'marked',\n",
       "  'by',\n",
       "  'the',\n",
       "  'commas',\n",
       "  '',\n",
       "  '',\n",
       "  'ted',\n",
       "  '',\n",
       "  '',\n",
       "  'ted',\n",
       "  '',\n",
       "  'harding',\n",
       "  '@',\n",
       "  'nessie',\n",
       "  '',\n",
       "  'mcc',\n",
       "  '',\n",
       "  'ac',\n",
       "  '',\n",
       "  'uk',\n",
       "  '']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_vals4.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_kv = rdd1.keys().zip(rdd_vals4) # we zip the two RDDs together \n",
    "    # i.e. produce tuples with one item from each RDD.\n",
    "    # This works because we have only applied mappings to the values, \n",
    "    # therefore the items in both RDDs are still aligned.\n",
    "    # >>> now remove any empty strings (i.e. length 0) that we may have\n",
    "    # created by removing punctuation, and resulting entries without words left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('8-1064msg1',\n",
       "  ['Subject',\n",
       "   ':',\n",
       "   're',\n",
       "   ':',\n",
       "   '8',\n",
       "   '',\n",
       "   '1044',\n",
       "   '',\n",
       "   'disc',\n",
       "   ':',\n",
       "   'grammar',\n",
       "   'in',\n",
       "   'schools',\n",
       "   '',\n",
       "   're',\n",
       "   'message',\n",
       "   'from',\n",
       "   ':',\n",
       "   'linguist',\n",
       "   '@',\n",
       "   'linguistlist',\n",
       "   '',\n",
       "   'org',\n",
       "   '',\n",
       "   '>',\n",
       "   '>',\n",
       "   'linguist',\n",
       "   'list',\n",
       "   ':',\n",
       "   'vol-8',\n",
       "   '-',\n",
       "   '1044',\n",
       "   '',\n",
       "   'sat',\n",
       "   'jul',\n",
       "   '12',\n",
       "   '1997',\n",
       "   '',\n",
       "   'issn',\n",
       "   ':',\n",
       "   '1068-4875',\n",
       "   '',\n",
       "   '>',\n",
       "   '>',\n",
       "   'subject',\n",
       "   ':',\n",
       "   '8',\n",
       "   '',\n",
       "   '1044',\n",
       "   '',\n",
       "   'disc',\n",
       "   ':',\n",
       "   'grammar',\n",
       "   'in',\n",
       "   'schools',\n",
       "   '>',\n",
       "   '>',\n",
       "   'i',\n",
       "   'know',\n",
       "   'and',\n",
       "   'teach',\n",
       "   'that',\n",
       "   'not',\n",
       "   'all',\n",
       "   'infinitives',\n",
       "   'contain',\n",
       "   '`',\n",
       "   'to',\n",
       "   \"'\",\n",
       "   '',\n",
       "   'i',\n",
       "   'also',\n",
       "   'give',\n",
       "   '>',\n",
       "   'the',\n",
       "   'students',\n",
       "   'examples',\n",
       "   '',\n",
       "   'e',\n",
       "   '',\n",
       "   'g',\n",
       "   '',\n",
       "   '`',\n",
       "   'i',\n",
       "   'asked',\n",
       "   'him',\n",
       "   'to',\n",
       "   'kindly',\n",
       "   'apologise',\n",
       "   \"'\",\n",
       "   '',\n",
       "   'where',\n",
       "   '>',\n",
       "   'placing',\n",
       "   'the',\n",
       "   'adverb',\n",
       "   'anywhere',\n",
       "   'else',\n",
       "   'would',\n",
       "   'cause',\n",
       "   'ambiguity',\n",
       "   '',\n",
       "   '>',\n",
       "   '>',\n",
       "   'jennifer',\n",
       "   'chew',\n",
       "   'an',\n",
       "   'example',\n",
       "   'i',\n",
       "   'once',\n",
       "   'concocted',\n",
       "   'to',\n",
       "   'justify',\n",
       "   '``',\n",
       "   'splitting',\n",
       "   'the',\n",
       "   'infintive',\n",
       "   '``',\n",
       "   '',\n",
       "   'or',\n",
       "   'not',\n",
       "   '',\n",
       "   'as',\n",
       "   'the',\n",
       "   'case',\n",
       "   'may',\n",
       "   'be',\n",
       "   '',\n",
       "   'is',\n",
       "   ':',\n",
       "   'a',\n",
       "   '',\n",
       "   'after',\n",
       "   'a',\n",
       "   'heavy',\n",
       "   'meal',\n",
       "   '',\n",
       "   'i',\n",
       "   'prepared',\n",
       "   'slowly',\n",
       "   'to',\n",
       "   'go',\n",
       "   'home',\n",
       "   'digesting',\n",
       "   'b',\n",
       "   '',\n",
       "   'after',\n",
       "   'a',\n",
       "   'heavy',\n",
       "   'meal',\n",
       "   '',\n",
       "   'i',\n",
       "   'prepared',\n",
       "   'to',\n",
       "   'slowly',\n",
       "   'go',\n",
       "   'home',\n",
       "   'digesting',\n",
       "   'c',\n",
       "   '',\n",
       "   'after',\n",
       "   'a',\n",
       "   'heavy',\n",
       "   'meal',\n",
       "   '',\n",
       "   'i',\n",
       "   'prepared',\n",
       "   'to',\n",
       "   'go',\n",
       "   'home',\n",
       "   'slowly',\n",
       "   'digesting',\n",
       "   'in',\n",
       "   'this',\n",
       "   'context',\n",
       "   '',\n",
       "   'with',\n",
       "   'the',\n",
       "   'possible',\n",
       "   'exception',\n",
       "   'of',\n",
       "   'the',\n",
       "   'third',\n",
       "   'case',\n",
       "   '',\n",
       "   'the',\n",
       "   'natural',\n",
       "   '',\n",
       "   'and',\n",
       "   'therefore',\n",
       "   'near-enough',\n",
       "   'unambiguous',\n",
       "   '',\n",
       "   'association',\n",
       "   'of',\n",
       "   'the',\n",
       "   'adverb',\n",
       "   'is',\n",
       "   'as',\n",
       "   'follows',\n",
       "   ':',\n",
       "   'a',\n",
       "   '',\n",
       "   'after',\n",
       "   'a',\n",
       "   'heavy',\n",
       "   'meal',\n",
       "   '',\n",
       "   'i',\n",
       "   'prepared',\n",
       "   '',\n",
       "   'slowly',\n",
       "   'to',\n",
       "   'go',\n",
       "   'home',\n",
       "   'digesting',\n",
       "   'b',\n",
       "   '',\n",
       "   'after',\n",
       "   'a',\n",
       "   'heavy',\n",
       "   'meal',\n",
       "   '',\n",
       "   'i',\n",
       "   'prepared',\n",
       "   'to',\n",
       "   'slowly',\n",
       "   '',\n",
       "   'go',\n",
       "   '',\n",
       "   'home',\n",
       "   'digesting',\n",
       "   'c',\n",
       "   '',\n",
       "   'after',\n",
       "   'a',\n",
       "   'heavy',\n",
       "   'meal',\n",
       "   '',\n",
       "   'i',\n",
       "   'prepared',\n",
       "   'to',\n",
       "   'go',\n",
       "   'home',\n",
       "   'slowly',\n",
       "   '',\n",
       "   'digesting',\n",
       "   '',\n",
       "   'this',\n",
       "   'was',\n",
       "   'long',\n",
       "   'ago',\n",
       "   '',\n",
       "   'when',\n",
       "   'you',\n",
       "   'got',\n",
       "   'glared',\n",
       "   'at',\n",
       "   'for',\n",
       "   'splitting',\n",
       "   'an',\n",
       "   'infinitive',\n",
       "   'regardless',\n",
       "   'of',\n",
       "   'whether',\n",
       "   'it',\n",
       "   'was',\n",
       "   'the',\n",
       "   'only',\n",
       "   'place',\n",
       "   'to',\n",
       "   'put',\n",
       "   'the',\n",
       "   'adverb',\n",
       "   'so',\n",
       "   'as',\n",
       "   'to',\n",
       "   'express',\n",
       "   'what',\n",
       "   'you',\n",
       "   'meant',\n",
       "   'and',\n",
       "   'not',\n",
       "   'something',\n",
       "   'else',\n",
       "   ':',\n",
       "   'this',\n",
       "   'example',\n",
       "   'achieved',\n",
       "   '',\n",
       "   'as',\n",
       "   'nearly',\n",
       "   'as',\n",
       "   'i',\n",
       "   'could',\n",
       "   '',\n",
       "   'three',\n",
       "   'quite',\n",
       "   'distinct',\n",
       "   'and',\n",
       "   'natural',\n",
       "   'meanings',\n",
       "   'for',\n",
       "   '``',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   'slowly',\n",
       "   'to',\n",
       "   'go',\n",
       "   'home',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '``',\n",
       "   '',\n",
       "   '``',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   'to',\n",
       "   'slowly',\n",
       "   'go',\n",
       "   'home',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '``',\n",
       "   'and',\n",
       "   '``',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   'to',\n",
       "   'go',\n",
       "   'home',\n",
       "   'slowly',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '``',\n",
       "   '',\n",
       "   'i',\n",
       "   \"'\",\n",
       "   'm',\n",
       "   'not',\n",
       "   '100',\n",
       "   'per',\n",
       "   'cent',\n",
       "   'happy',\n",
       "   'with',\n",
       "   'it',\n",
       "   '',\n",
       "   'for',\n",
       "   'obvious',\n",
       "   'reasons',\n",
       "   '',\n",
       "   'and',\n",
       "   'it',\n",
       "   'would',\n",
       "   'be',\n",
       "   'interesting',\n",
       "   'to',\n",
       "   'see',\n",
       "   'if',\n",
       "   'anyone',\n",
       "   'can',\n",
       "   'come',\n",
       "   'up',\n",
       "   'with',\n",
       "   'a',\n",
       "   'better',\n",
       "   '',\n",
       "   'more',\n",
       "   'clear-cut',\n",
       "   'one',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   'and',\n",
       "   '',\n",
       "   'to',\n",
       "   'really',\n",
       "   '``',\n",
       "   'epater',\n",
       "   'les',\n",
       "   'bourgeois',\n",
       "   '``',\n",
       "   '',\n",
       "   'i',\n",
       "   'reckon',\n",
       "   'you',\n",
       "   'could',\n",
       "   'even',\n",
       "   'make',\n",
       "   'a',\n",
       "   'case',\n",
       "   'for',\n",
       "   '``',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   'i',\n",
       "   'prepared',\n",
       "   'to',\n",
       "   '',\n",
       "   'slowly',\n",
       "   '',\n",
       "   'go',\n",
       "   'home',\n",
       "   'digesting',\n",
       "   '``',\n",
       "   ':',\n",
       "   'the',\n",
       "   'implication',\n",
       "   'being',\n",
       "   'that',\n",
       "   'the',\n",
       "   'meal',\n",
       "   'was',\n",
       "   'so',\n",
       "   'very',\n",
       "   'heavy',\n",
       "   'that',\n",
       "   'the',\n",
       "   'walk',\n",
       "   'home',\n",
       "   'should',\n",
       "   'be',\n",
       "   'correpondingly',\n",
       "   'delicate',\n",
       "   '',\n",
       "   'as',\n",
       "   'emphasised',\n",
       "   'by',\n",
       "   'the',\n",
       "   'pause',\n",
       "   'in',\n",
       "   'rhythm',\n",
       "   'marked',\n",
       "   'by',\n",
       "   'the',\n",
       "   'commas',\n",
       "   '',\n",
       "   '',\n",
       "   'ted',\n",
       "   '',\n",
       "   '',\n",
       "   'ted',\n",
       "   '',\n",
       "   'harding',\n",
       "   '@',\n",
       "   'nessie',\n",
       "   '',\n",
       "   'mcc',\n",
       "   '',\n",
       "   'ac',\n",
       "   '',\n",
       "   'uk',\n",
       "   ''])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_kv.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_kvr = rdd_kv.map(lambda x: (x[0], [s for s in x[1] if len(s)>0])) # remove empty strings using RDD.map and a lambda. TIP len(s) gives you the lenght of string. \n",
    "# S. It means that for each variable I want the x[0] ( key ) and x[1] value only if the length of the string is more than 0.\n",
    "# This helps to avoid strings accidentaly created to be included\n",
    "\n",
    "rdd_kvrf = rdd_kvr.filter(lambda x: len(x[0]) > 0) # remove items without tokens using RDD.filter and a lambda. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('8-1064msg1', ['Subject', ':', 're', ':', '8', '1044', 'disc', ':', 'grammar', 'in', 'schools', 're', 'message', 'from', ':', 'linguist', '@', 'linguistlist', 'org', '>', '>', 'linguist', 'list', ':', 'vol-8', '-', '1044', 'sat', 'jul', '12', '1997', 'issn', ':', '1068-4875', '>', '>', 'subject', ':', '8', '1044', 'disc', ':', 'grammar', 'in', 'schools', '>', '>', 'i', 'know', 'and', 'teach', 'that', 'not', 'all', 'infinitives', 'contain', '`', 'to', \"'\", 'i', 'also', 'give', '>', 'the', 'students', 'examples', 'e', 'g', '`', 'i', 'asked', 'him', 'to', 'kindly', 'apologise', \"'\", 'where', '>', 'placing', 'the', 'adverb', 'anywhere', 'else', 'would', 'cause', 'ambiguity', '>', '>', 'jennifer', 'chew', 'an', 'example', 'i', 'once', 'concocted', 'to', 'justify', '``', 'splitting', 'the', 'infintive', '``', 'or', 'not', 'as', 'the', 'case', 'may', 'be', 'is', ':', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'in', 'this', 'context', 'with', 'the', 'possible', 'exception', 'of', 'the', 'third', 'case', 'the', 'natural', 'and', 'therefore', 'near-enough', 'unambiguous', 'association', 'of', 'the', 'adverb', 'is', 'as', 'follows', ':', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'this', 'was', 'long', 'ago', 'when', 'you', 'got', 'glared', 'at', 'for', 'splitting', 'an', 'infinitive', 'regardless', 'of', 'whether', 'it', 'was', 'the', 'only', 'place', 'to', 'put', 'the', 'adverb', 'so', 'as', 'to', 'express', 'what', 'you', 'meant', 'and', 'not', 'something', 'else', ':', 'this', 'example', 'achieved', 'as', 'nearly', 'as', 'i', 'could', 'three', 'quite', 'distinct', 'and', 'natural', 'meanings', 'for', '``', 'slowly', 'to', 'go', 'home', '``', '``', 'to', 'slowly', 'go', 'home', '``', 'and', '``', 'to', 'go', 'home', 'slowly', '``', 'i', \"'\", 'm', 'not', '100', 'per', 'cent', 'happy', 'with', 'it', 'for', 'obvious', 'reasons', 'and', 'it', 'would', 'be', 'interesting', 'to', 'see', 'if', 'anyone', 'can', 'come', 'up', 'with', 'a', 'better', 'more', 'clear-cut', 'one', 'and', 'to', 'really', '``', 'epater', 'les', 'bourgeois', '``', 'i', 'reckon', 'you', 'could', 'even', 'make', 'a', 'case', 'for', '``', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', '``', ':', 'the', 'implication', 'being', 'that', 'the', 'meal', 'was', 'so', 'very', 'heavy', 'that', 'the', 'walk', 'home', 'should', 'be', 'correpondingly', 'delicate', 'as', 'emphasised', 'by', 'the', 'pause', 'in', 'rhythm', 'marked', 'by', 'the', 'commas', 'ted', 'ted', 'harding', '@', 'nessie', 'mcc', 'ac', 'uk'])]\n"
     ]
    }
   ],
   "source": [
    "print(rdd_kvrf.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('8-1064msg1', 'Subject: re : 8 . 1044 , disc : grammar in schools\\n\\n( re message from : linguist @ linguistlist . org ) > > linguist list : vol-8 - 1044 . sat jul 12 1997 . issn : 1068-4875 . > > subject : 8 . 1044 , disc : grammar in schools > > i know and teach that not all infinitives contain ` to \\' . i also give > the students examples ( e . g . ` i asked him to kindly apologise \\' ) where > placing the adverb anywhere else would cause ambiguity . > > jennifer chew an example i once concocted to justify \" splitting the infintive \" ( or not , as the case may be ) is : a ) after a heavy meal , i prepared slowly to go home digesting b ) after a heavy meal , i prepared to slowly go home digesting c ) after a heavy meal , i prepared to go home slowly digesting in this context , with the possible exception of the third case , the natural ( and therefore near-enough unambiguous ) association of the adverb is as follows : a ) after a heavy meal , i prepared _ slowly to go home digesting b ) after a heavy meal , i prepared to slowly _ go _ home digesting c ) after a heavy meal , i prepared to go home slowly _ digesting ( this was long ago , when you got glared at for splitting an infinitive regardless of whether it was the only place to put the adverb so as to express what you meant and not something else : this example achieved , as nearly as i could , three quite distinct and natural meanings for \" . . . slowly to go home . . . \" , \" . . . to slowly go home . . . \" and \" . . . to go home slowly . . . \" . i \\' m not 100 per cent happy with it , for obvious reasons , and it would be interesting to see if anyone can come up with a better , more clear-cut one ) . [ and , to really \" epater les bourgeois \" , i reckon you could even make a case for \" . . . i prepared to , slowly , go home digesting \" : the implication being that the meal was so very heavy that the walk home should be correpondingly delicate , as emphasised by the pause in rhythm marked by the commas ] . ted . ( ted . harding @ nessie . mcc . ac . uk )\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(rdd1.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STORE TEXT ( See how to call them ) Into  VECTORS of Fixed Length\n",
    "\n",
    "Task 3) Creating normalised TF.IDF vectors of defined dimensionality, measure the effect of caching.\n",
    "\n",
    "We use the hashing trick to create fixed size TF vectors directly from the word list now (slightly different from the previous lab, where we used *(word,count)* pairs.). Write a bit of code as needed. (5%)\n",
    "\n",
    "Then we'll use the IDF and Normalizer functions provided by Spark. They use a slightly different pattern than RDD.map and reduce, have a look at the examples here in the documentation for Normalizer  and IDF:\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.feature.Normalizer](http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.feature.Normalizer), [http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.feature.IDF](http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.feature.IDF) (5%)\n",
    "\n",
    "We want control of the dimensionality in the `normTFIDF` function, so we introduce an argument into our functions that enables us to vary dimensionalty later. Here is also an opportunity to benefit from caching, i.e. persisting the RDD after use, so that it will not be recomputed.  (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vector: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Hashed word: 3563408940069010972\n",
      "Hashed word % N: 2\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Hashed word: 4787531584920406966\n",
      "Hashed word % N: 6\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
      "Hashed word: -7280757250879665682\n",
      "Hashed word % N: 8\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 1, 0]\n",
      "Hashed word: -7280757250879665682\n",
      "Hashed word % N: 8\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 2, 0]\n",
      "Hashed word: -7614204387349661840\n",
      "Hashed word % N: 0\n",
      "[1, 0, 1, 0, 0, 0, 1, 0, 2, 0]\n",
      "Vector identifying the Word Hello : [1, 0, 1, 0, 0, 0, 1, 0, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# HERE YOU CAN EXPLAIN THE CONCEPT OF VECTORIZE A WORD\n",
    "\n",
    "#def hashing_vectorize(text,N): # arguments: the list and the size of the output vector\n",
    "\n",
    "\n",
    "# hash return an encode for each word ( sequence of number and words. See after)\n",
    "\n",
    "#example: \n",
    "text = 'Hello'\n",
    "N = 10\n",
    "v = [0] * N  # create vector of 0s of the dimension we define \n",
    "print('Initial vector:',v)\n",
    "for word in text: # iterate through the words \n",
    "#>>>              # get the hash value\n",
    "    h = hash(word)  \n",
    "    print('Hashed word:', h)\n",
    "    print('Hashed word % N:', h % N)\n",
    "#>>>              # add 1 at the hashed address \n",
    "    v[h % N] = v[h % N] + 1 # You add one to this position of the vector\n",
    "    print(v)\n",
    "print('Vector identifying the Word', text, ':',v)\n",
    " # return hashed word vector\n",
    "\n",
    "\n",
    "# So regardless the hash generated you may end up having same vectors for diff.\n",
    "# Letters ?\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens before Vectorization: [['Subject', ':', 're', ':', '8', '1044', 'disc', ':', 'grammar', 'in', 'schools', 're', 'message', 'from', ':', 'linguist', '@', 'linguistlist', 'org', '>', '>', 'linguist', 'list', ':', 'vol-8', '-', '1044', 'sat', 'jul', '12', '1997', 'issn', ':', '1068-4875', '>', '>', 'subject', ':', '8', '1044', 'disc', ':', 'grammar', 'in', 'schools', '>', '>', 'i', 'know', 'and', 'teach', 'that', 'not', 'all', 'infinitives', 'contain', '`', 'to', \"'\", 'i', 'also', 'give', '>', 'the', 'students', 'examples', 'e', 'g', '`', 'i', 'asked', 'him', 'to', 'kindly', 'apologise', \"'\", 'where', '>', 'placing', 'the', 'adverb', 'anywhere', 'else', 'would', 'cause', 'ambiguity', '>', '>', 'jennifer', 'chew', 'an', 'example', 'i', 'once', 'concocted', 'to', 'justify', '``', 'splitting', 'the', 'infintive', '``', 'or', 'not', 'as', 'the', 'case', 'may', 'be', 'is', ':', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'in', 'this', 'context', 'with', 'the', 'possible', 'exception', 'of', 'the', 'third', 'case', 'the', 'natural', 'and', 'therefore', 'near-enough', 'unambiguous', 'association', 'of', 'the', 'adverb', 'is', 'as', 'follows', ':', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'this', 'was', 'long', 'ago', 'when', 'you', 'got', 'glared', 'at', 'for', 'splitting', 'an', 'infinitive', 'regardless', 'of', 'whether', 'it', 'was', 'the', 'only', 'place', 'to', 'put', 'the', 'adverb', 'so', 'as', 'to', 'express', 'what', 'you', 'meant', 'and', 'not', 'something', 'else', ':', 'this', 'example', 'achieved', 'as', 'nearly', 'as', 'i', 'could', 'three', 'quite', 'distinct', 'and', 'natural', 'meanings', 'for', '``', 'slowly', 'to', 'go', 'home', '``', '``', 'to', 'slowly', 'go', 'home', '``', 'and', '``', 'to', 'go', 'home', 'slowly', '``', 'i', \"'\", 'm', 'not', '100', 'per', 'cent', 'happy', 'with', 'it', 'for', 'obvious', 'reasons', 'and', 'it', 'would', 'be', 'interesting', 'to', 'see', 'if', 'anyone', 'can', 'come', 'up', 'with', 'a', 'better', 'more', 'clear-cut', 'one', 'and', 'to', 'really', '``', 'epater', 'les', 'bourgeois', '``', 'i', 'reckon', 'you', 'could', 'even', 'make', 'a', 'case', 'for', '``', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', '``', ':', 'the', 'implication', 'being', 'that', 'the', 'meal', 'was', 'so', 'very', 'heavy', 'that', 'the', 'walk', 'home', 'should', 'be', 'correpondingly', 'delicate', 'as', 'emphasised', 'by', 'the', 'pause', 'in', 'rhythm', 'marked', 'by', 'the', 'commas', 'ted', 'ted', 'harding', '@', 'nessie', 'mcc', 'ac', 'uk']]\n",
      "Tokens after Vectorization: [[45, 27, 41, 45, 38, 45, 29, 47, 26, 31]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "from pyspark.mllib.feature import IDF, Normalizer\n",
    "\n",
    "\n",
    "\n",
    "# The MAIN GOAL IS TO HAVE RETURNED NUMERICAL INPUTS THAT CAN BE ABLE TO \n",
    "# 1. BE INPUT INTO MODELS\n",
    "# 2. HAVE THE SAME VECTOR SIZE THAT I DEFINE WITH THE ARGUMENT N \n",
    "\n",
    "def hashing_vectorize(text,N): # arguments: the list and the size of the output vector\n",
    "    v = [0] * N  # create vector of 0s of the dimension we define \n",
    "    for word in text: # iterate through the words \n",
    "#>>>              # get the hash value\n",
    "        h = hash(word) # EVERY WORD OBTAIN A HASH: Look at youtube video so useful\n",
    "#>>>              # add 1 at the hashed address \n",
    "        v[h % N] = v[h % N] + 1\n",
    "    return v # return hashed word vector\n",
    "\n",
    "# https://www.youtube.com/watch?v=2BldESGZKB8 Super useful to explain what this is \n",
    "\n",
    "\n",
    "testDim = 10\n",
    "rdd2 = rdd_kvrf # Use a small RDD for testing.\n",
    "\n",
    "keysRDD = rdd2.keys() # These are the labels that identify whether is spam or ham\n",
    "tokensRDD = rdd2.values() # I select only the values \n",
    "\n",
    "tfVecRDD = tokensRDD.map(lambda tokens: hashing_vectorize(tokens,testDim)) #>>> passing the vecDim value. TIP: you need a lambda.\n",
    "print('Tokens before Vectorization:', tokensRDD.take(1))\n",
    "print('Tokens after Vectorization:', tfVecRDD.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STILL NEED TO UNDERSTAND THIS PART !\n",
    "\n",
    "# since we will read more than once, \n",
    "# caching in Memory will make things quicker.\n",
    "# https://www.quora.com/What-will-be-the-default-storage-level-for-RDD-in-Spark\n",
    "\n",
    "\n",
    "\n",
    "tfVecRDD.persist(StorageLevel.MEMORY_ONLY) \n",
    "idf = IDF() # create IDF object\n",
    "idfModel = idf.fit(tfVecRDD) # calculate IDF values\n",
    "tfIdfRDD = idfModel.transform(tfVecRDD) # 2nd pass needed (see lecture slides), transforms RDD\n",
    "#>>>    norm = idf... # create a Normalizer object like in the example linked above\n",
    "norm = Normalizer()\n",
    "#>>>    normTfIdfRDD = norm. ... # and apply it to the tfIdfRDD \n",
    "normTfIdfRDD = norm.transform(tfIdfRDD)\n",
    "#>>>    zippedRDD = ... # zip the keys and values together\n",
    "zippedRDD = keysRDD.zip(normTfIdfRDD)\n",
    "rdd3 = zippedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('8-1064msg1',\n",
       "  DenseVector([0.6222, 0.124, 0.1883, 0.0, 0.0, 0.4141, 0.5355, 0.2159, 0.2392, 0.0]))]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.take(1) \n",
    "# I vectorised it and normalized it, linked the keys together and check what else\n",
    "# Check what is Dense Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It would definitely make sense to define everything within a function \n",
    "# which will make sense when changing and defining the Vector dimension"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use this later:\n",
    "# Caching is a process to speed up applications that access the same RDD multiple times. Without caching, an RDD is re-evaluated each time that an action is needed on that RDD.\n",
    "# Caching is recommended in machine learning applications because they are usually computational expensive. In this example we can see that when we call the action '.collect()' on the \n",
    "# testRDD, the trials with caching on average are more than 3 time faster than trials without caching.\n",
    "# We can also see at the previous function  that we created 'normTFIDF', where we set tha value caching=True. In that case, we used the persist function on the tfIdfRDD to store the memory\n",
    "# since we will read more than once, caching in Memory make things quicker.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the labels\n",
    "\n",
    " Task 4) Create LabeledPoints \n",
    "\n",
    "Determine whether the file is spam (i.e. the filename contains ’spmsg’) and replace the filename by a 1 (spam) or 0 (non-spam) accordingly. Use `RDD.map()` to create an RDD of LabeledPoint objects. See here [http://spark.apache.org/docs/2.1.0/mllib-linear-methods.html#logistic-regression](http://spark.apache.org/docs/2.1.0/mllib-linear-methods.html#logistic-regression) for an example, and here [http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint](http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint) for the `LabeledPoint` documentation. (10%)\n",
    "\n",
    "There is a handy function of Python strings called startswith: e.g. 'abc'.startswith('ab) will return true. The relevant Python syntax here is a conditional expression: **``<a> if <yourCondition> else <b>``**, i.e. 1 if the filename starts with 'spmsg' and otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('8-1064msg1', DenseVector([0.6222, 0.124, 0.1883, 0.0, 0.0, 0.4141, 0.5355, 0.2159, 0.2392, 0.0]))]\n",
      "[LabeledPoint(0.0, [0.6221689348917344,0.12400296977829081,0.18830080595962678,0.0,0.0,0.4140596001139784,0.5355363340306855,0.21585702146591362,0.23923443562140975,0.0])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "print(rdd3.take(1)) \n",
    "\n",
    "cls_vec_RDD = rdd3.map(lambda x: (1 if x[0].startswith('spm') else 0, x[1]))\n",
    "  \n",
    "# now we can create the LabeledPoint objects with (class,vector) arguments\n",
    "# It creates a LabeledPoint <- I don't know why: You need to check why it does this. \n",
    "lp_RDD = cls_vec_RDD.map(lambda cls_vec: LabeledPoint(cls_vec[0],cls_vec[1]) ) \n",
    "\n",
    "print(lp_RDD.take(1))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing \n",
    "\n",
    "Task 5) Complete the preprocessing \n",
    "\n",
    "It will be useful to have a single function to do the preprocessing. So integrate everything here. (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here it wraps everything up and define function to be able to import new data super quickly \n",
    "# and eventually change the size of the Vector\n",
    "\n",
    "# -> I think you need to do everything again by including TRAIN and TEST A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6) Train some classifiers \n",
    "\n",
    "Use the `LabeledPoint` objects to train a classifier, specifically the *LogisticRegression*, *Naive Bayes*, and *Support Vector Machine*. Calculate the accuracy of the model on the training set (again, follow this example [http://spark.apache.org/docs/2.1.0/ml-classification-regression.html#logistic-regression](http://spark.apache.org/docs/2.0.0/ml-classification-regression.html#logistic-regression) and here is the documentation for the classifiers [LogisticRegressionWithLBFGS](http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithLBFGS), [NaiveBayes](http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.classification.NaiveBayes), [SVMWithSGD](http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.classification.SVMWithSGD).  (10%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
