{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='EC2C04'>  SPAM DETECTION SYSTEM with Ling-Spam COLLECTION </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes the tasks performed in the **second part** of the Resit Coursework of the Big Data Applications module. For a complete description of the purposes of the analysis, please consult the PDF document provided which also represent the first part of the Coursework.\n",
    "From that file, you will be also able to have a detailed the description of the corpus used for this analysis. Nevertheless, you can access to a summary of the file from the **[part 1]**(# 1.Access and Import the Files) of this script, in which the Readme.txt file of the dataset is accessed and shown.\n",
    "This script goes through the entire process that is necessary to create and obtain the optimal model able to identify whether a message represents a Spam or Ham.\n",
    "In particular, the phases in which the document is organised are summarised as following:\n",
    "\n",
    "1. **Access and Import the File**  \n",
    "2. **Clean the Text files**\n",
    "3. **Convert Text files into Vectors of fixed dimensions and Encode the Target into a Binary variable**\n",
    "4. **Normalise the Values and Turn the tuple Label/Dense Vector into LabeledPoints local vector**\n",
    "5. **Classification models**\n",
    "\n",
    "\n",
    "**! NOTE:** To better understand and expose the abilities learnt with this coursework I believe it may be useful to work in two directions:\n",
    "1. Start with simple/small attempts for each function in order to verify the results of my actions\n",
    "2. Give the possibility of these results to be reproducible ( when importing new data or modify parameters ) by defining functions that can include and iterate the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/simonezanetti/anaconda3/lib/python3.6/site-packages (2.4.3)\n",
      "Requirement already satisfied: py4j==0.10.7 in /Users/simonezanetti/anaconda3/lib/python3.6/site-packages (from pyspark) (0.10.7)\n"
     ]
    }
   ],
   "source": [
    "# PYSPARK INSTALLATION\n",
    "!pip install pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE PACKAGES \n",
    "import pyspark\n",
    "from pathlib import Path\n",
    "import re\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.mllib.feature import IDF, Normalizer\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE CONTEXT\n",
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Access and Import the Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/simonezanetti/Desktop/lingspam_public\n"
     ]
    }
   ],
   "source": [
    "# Define the Directory \n",
    "%cd /Users/simonezanetti/Desktop/lingspam_public\n",
    "\n",
    "# Define Your own directory\n",
    "# %cd /BigDataResitCw/lingspam_public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This directory contains the Ling-Spam corpus, as described in the \r\n",
      "paper:\r\n",
      "\r\n",
      "I. Androutsopoulos, J. Koutsias, K.V. Chandrinos, George Paliouras, \r\n",
      "and C.D. Spyropoulos, \"An Evaluation of Naive Bayesian Anti-Spam \r\n",
      "Filtering\". In Potamias, G., Moustakis, V. and van Someren, M. (Eds.), \r\n",
      "Proceedings of the Workshop on Machine Learning in the New Information \r\n",
      "Age, 11th European Conference on Machine Learning (ECML 2000), \r\n",
      "Barcelona, Spain, pp. 9-17, 2000.\r\n",
      "\r\n",
      "There are four subdirectories, corresponding to four versions of \r\n",
      "the corpus:\r\n",
      "\r\n",
      "bare: Lemmatiser disabled, stop-list disabled.\r\n",
      "lemm: Lemmatiser enabled, stop-list disabled.\r\n",
      "lemm_stop: Lemmatiser enabled, stop-list enabled.\r\n",
      "stop: Lemmatiser disabled, stop-list enabled.\r\n",
      "\r\n",
      "Each one of these 4 directories contains 10 subdirectories (part1, \r\n",
      "..., part10). These correspond to the 10 partitions of the corpus \r\n",
      "that were used in the 10-fold experiments. In each repetition, one \r\n",
      "part was reserved for testing and the other 9 were used for training. \r\n",
      "\r\n",
      "Each one of the 10 subdirectories contains both spam and legitimate \r\n",
      "messages, one message in each file. Files whose names have the form\r\n",
      "spmsg*.txt are spam messages. All other files are legitimate messages.\r\n",
      "\r\n",
      "By obtaining a copy of this corpus you agree to acknowledge the use \r\n",
      "and origin of the corpus in any published work of yours that makes \r\n",
      "use of the corpus, and to notify the person below about this work.\r\n",
      "\r\n",
      "Ion Androutsopoulos \r\n",
      "http://www.aueb.gr/users/ion/\r\n",
      "Ling-Spam corpus last updated: July 17, 2000\r\n",
      "This file (readme.txt) last updated: July 30, 2003.\r\n"
     ]
    }
   ],
   "source": [
    "# Access to the Readme.txt file to access to a description of the Corpus \n",
    "# used for this task.\n",
    "!cat readme.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!  NOTE:** For this task the **Bare** subdirectory will be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bare/part3 bare/part4 bare/part5 bare/part2 bare/part10 bare/part9 bare/part7 bare/part1 bare/part6 bare/part8\n"
     ]
    }
   ],
   "source": [
    "# Goal is Extracting the text files contained in each of the 10 subdirectories \n",
    "# contained in the 'bare' directory that has been chosen to be used in this analysis.\n",
    "# I could use also the package glob ( and glob.glob. Check in case )\n",
    "\n",
    "\n",
    "path = Path('bare') # Set the directory bare as path\n",
    "print(*path.iterdir()) # Check that this is going to do what you want: \n",
    "                    # -> Obtaining as (sub)directories all the 10 parts within the folder bare\n",
    "                    # to then iterate over all of them to assign their content into an RDD file.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bare/part3\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part3\n",
      "bare/part4\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part4\n",
      "bare/part5\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part5\n",
      "bare/part2\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part2\n",
      "bare/part10\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part10\n",
      "bare/part9\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part9\n",
      "bare/part7\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part7\n",
      "bare/part1\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part1\n",
      "bare/part6\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part6\n",
      "bare/part8\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part8\n"
     ]
    }
   ],
   "source": [
    "# Store each directory path in a list, in order to iterate that and be able to obtain the RDD's. \n",
    "dr = list(path.iterdir()) \n",
    "\n",
    "# Make sure the function .resolve() works in connecting\n",
    "# the path with each directory (x) and its turned into a str\n",
    "# This will be useful in the loop set in the following cell\n",
    "for i in range(0,len(dr)):\n",
    "    print(dr[i])\n",
    "    print(dr[i].resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length file/Number of Rdd's 10 \n",
      "\n",
      "\n",
      "Right number!\n",
      "\n",
      "\n",
      "[('file:/Users/simonezanetti/Desktop/lingspam_public/bare/part4/6-266msg3.txt', 'Subject: bisfai deadline extension !\\n\\nbisfai deadline extension ! the deadline for the bar - ilan symposium on foundations of artificial intelligence has been extended to february 27 . the conference itself will take place as scheduled , june 20-22 , in ramat - gan and jerusalem , israel . for more information contact : bisfai @ bimacs . cs . biu . ac . il daniel radzinski tovna translation machines jerusalem , israel dr @ tovna . co . il\\n'), ('file:/Users/simonezanetti/Desktop/lingspam_public/bare/part4/8-1074msg1.txt', 'Subject: 8th international conference on functional grammar\\n\\neighth international conference on functional grammar , july 6th-9th , 1998 the biennial series on conferences on functional grammar will be continued in 1998 at the vrije universiteit amsterdam ( netherlands ) , where a four-day conference will be held from 6th to 9th july 1998 . the conference will be held on the campus of the vrije universiteit and will comprise a number of plenary lectures , parallel sessions , poster sessions and workshops , as well as a range of social activities . all the papers at the conference will address issues arising within the theory of functional grammar , as presented in simon c . dik , * the theory of functional grammar * ( 2 parts ) , which is to be published ( posthumously ) by mouton de gruyter , berlin in the autumn of 1997 . a thematically based selection of the papers will , it is hoped , be prepared for publication in book form . the first call for papers will be sent out in august 1997 . those not already on the functional grammar mailing list and interested in receiving the first call or other information regarding the conference , should contact : prof . j . l . mackenzie department of english faculty of letters vrije universiteit de boelelaan 1105 1081 hv amsterdam the netherlands e-mail : mackenzi @ let . vu . nl fax : + 31-20 - 444 6500\\n')]\n"
     ]
    }
   ],
   "source": [
    "# Set the loop that allows to store the content of each directory as Rdd within a List.\n",
    "store_rdds = []\n",
    "\n",
    "# From the (sub)directories to Rdd files\n",
    "for x in dr:  # iterate through the directories \n",
    "    rdd = sc.wholeTextFiles(str(x.resolve())) \n",
    "    store_rdds.append(rdd) # I append each rdd obtained with wholeTextFile() in the empty list created \n",
    "\n",
    "# Check that I have 10 directories in the list\n",
    "\n",
    "print(\"Length file/Number of Rdd's\",len(store_rdds),'\\n\\n')\n",
    "if len(store_rdds) == 10:\n",
    "    print('Right number!\\n\\n')\n",
    "else:\n",
    "    print('Wrong number. Check what happened.\\n\\n')\n",
    "# Check that it worked \n",
    "print(store_rdds[1].take(2)) # I take the first subdirectory(turned into an Rdd)\n",
    "                             # and have a look at first two messages.\n",
    "    \n",
    "    \n",
    "# I can see from the last print that the each tuple includes the file path.\n",
    "# ex. 'file:/Users/simonezanetti/Desktop/lingspam_public/bare/part4/6-266msg3.txt'\n",
    "# This is not useful. By considering the example shown, what we want is only\n",
    "# 6-266msg3. This is useful later on to identify the labels as filenames \n",
    "# starting with 'spmsg' identify the spam messages. ( next .. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:/Users/simonezanetti/Desktop/lingspam_public/bare/part4/6-266msg3.txt\n",
      "['file:', 'Users', 'simonezanetti', 'Desktop', 'lingspam_public', 'bare', 'part4', '6-266msg3', 'txt']\n",
      "file:/Users/simonezanetti/Desktop/lingspam_public/bare/part4/8-1074msg1.txt\n",
      "['file:', 'Users', 'simonezanetti', 'Desktop', 'lingspam_public', 'bare', 'part4', '8-1074msg1', 'txt']\n",
      "file:/Users/simonezanetti/Desktop/lingspam_public/bare/part4/6-353msg2.txt\n",
      "['file:', 'Users', 'simonezanetti', 'Desktop', 'lingspam_public', 'bare', 'part4', '6-353msg2', 'txt']\n",
      "file:/Users/simonezanetti/Desktop/lingspam_public/bare/part4/6-864msg1.txt\n",
      "['file:', 'Users', 'simonezanetti', 'Desktop', 'lingspam_public', 'bare', 'part4', '6-864msg1', 'txt']\n",
      "\n",
      "\n",
      "\n",
      "SUMMARY:\n",
      " I need this: 6-864msg1\n"
     ]
    }
   ],
   "source": [
    "a = store_rdds[1].take(4)\n",
    "for x in a:\n",
    "    print(x[0])\n",
    "    a = re.split('[/.]',x[0])  # square brackets indicate every of the symbols inside can be used to split\n",
    "    print(a)\n",
    "\n",
    "# I want to keep only the second last element of this list ( index -2)    \n",
    "print('\\n\\n\\nSUMMARY:\\n I need this:',a[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if it worked:\n",
      "\n",
      " [('6-266msg3', 'Subject: bisfai deadline extension !\\n\\nbisfai deadline extension ! the deadline for the bar - ilan symposium on foundations of artificial intelligence has been extended to february 27 . the conference itself will take place as scheduled , june 20-22 , in ramat - gan and jerusalem , israel . for more information contact : bisfai @ bimacs . cs . biu . ac . il daniel radzinski tovna translation machines jerusalem , israel dr @ tovna . co . il\\n')]\n",
      "\n",
      "\n",
      "Well done !\n"
     ]
    }
   ],
   "source": [
    "# .. To solve this issue it is possible to use the re.split function from the package\n",
    "# NLTK ( tackled on the INTRODUCTION TO NATURAL LANGUAGE PROCESSING module)\n",
    "# lambda function will help to iterate the function over the sequence of text within each RDD\n",
    "# map function allows me to apply a function to the Rdd file.\n",
    "\n",
    "\n",
    "# A loop that iterates over each of the 10 Rdd's: \n",
    "# I think this is not the most elegant way, but I could not find another\n",
    "store_rdd = []\n",
    "for i in range(0,len(store_rdds)):\n",
    "    a = store_rdds[i]\n",
    "    a =  a.map(lambda x: (re.split('[/\\.]', x[0])[-2], x[1])) # x[1] is the text itself\n",
    "    store_rdd.append(a)\n",
    "\n",
    "print('Check if it worked:\\n\\n',store_rdd[1].take(1))\n",
    "print('\\n\\nWell done !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify if the directory is properly set:\n",
      "bare/part3\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part3\n",
      "Verify if the directory is properly set:\n",
      "bare/part4\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part4\n",
      "Verify if the directory is properly set:\n",
      "bare/part5\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part5\n",
      "Verify if the directory is properly set:\n",
      "bare/part2\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part2\n",
      "Verify if the directory is properly set:\n",
      "bare/part10\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part10\n",
      "Verify if the directory is properly set:\n",
      "bare/part9\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part9\n",
      "Verify if the directory is properly set:\n",
      "bare/part7\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part7\n",
      "Verify if the directory is properly set:\n",
      "bare/part1\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part1\n",
      "Verify if the directory is properly set:\n",
      "bare/part6\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part6\n",
      "Verify if the directory is properly set:\n",
      "bare/part8\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part8\n",
      "\n",
      "\n",
      "\n",
      "Check if split function worked:\n",
      "\n",
      " [('6-266msg3', 'Subject: bisfai deadline extension !\\n\\nbisfai deadline extension ! the deadline for the bar - ilan symposium on foundations of artificial intelligence has been extended to february 27 . the conference itself will take place as scheduled , june 20-22 , in ramat - gan and jerusalem , israel . for more information contact : bisfai @ bimacs . cs . biu . ac . il daniel radzinski tovna translation machines jerusalem , israel dr @ tovna . co . il\\n')]\n"
     ]
    }
   ],
   "source": [
    "# At this point I should turn the results of this phase into a function, \n",
    "# which will help me to be able to import a file and obtain these results\n",
    "# by only setting the path/folder name\n",
    "\n",
    "def access_and_import(path):\n",
    "    path = Path('bare')\n",
    "    dr = list(path.iterdir()) \n",
    "    for i in range(0,len(dr)):\n",
    "        print('Verify if the directory is properly set:')\n",
    "        print(dr[i])\n",
    "        print(dr[i].resolve())\n",
    "    print('\\n\\n')\n",
    "    store_rdds = []\n",
    "    for x in dr:  \n",
    "        rdd = sc.wholeTextFiles(str(x.resolve())) \n",
    "        store_rdds.append(rdd) \n",
    "    store_rdd = []\n",
    "    for i in range(0,len(store_rdds)):\n",
    "        a = store_rdds[i]\n",
    "        a =  a.map(lambda x: (re.split('[/\\.]', x[0])[-2], x[1])) # x[1] is the text itself\n",
    "        store_rdd.append(a)\n",
    "    print('Check if split function worked:\\n\\n',store_rdd[1].take(1))\n",
    "    return store_rdd\n",
    "# Check if it worked \n",
    "store_rdd = access_and_import('bare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(store_rdd, ntest): \n",
    "#ntest: number of rdds I want to keep for test set\n",
    "    ntrain = len(store_rdd)- ntest\n",
    "    print('Num Rdd used for Train:',ntrain)\n",
    "    train=sc.emptyRDD() # I set an empty RDD and I then unite the train RDDs together\n",
    "    for x in range(0,ntrain):\n",
    "        train = train.union(store_rdd[x])\n",
    "    test = sc.emptyRDD() # Same for test\n",
    "    for x in range(ntrain,len(store_rdd)):\n",
    "        test = test.union(store_rdd[x])\n",
    "    #union = train.union(test) # Not sure so far but it may be useful to perform operation when still together and split after\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Rdd used for Train: 9\n"
     ]
    }
   ],
   "source": [
    "train,test = split_train_test(store_rdd, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Apply NLP techniques to clean the strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject: re : 8 . 1044 , disc : grammar in schools\\n\\n( re message from : linguist @ linguistlist . org ) > > linguist list : vol-8 - 1044 . sat jul 12 1997 . issn : 1068-4875 . > > subject : 8 . 1044 , disc : grammar in schools > > i know and teach that not all infinitives contain ` to \\' . i also give > the students examples ( e . g . ` i asked him to kindly apologise \\' ) where > placing the adverb anywhere else would cause ambiguity . > > jennifer chew an example i once concocted to justify \" splitting the infintive \" ( or not , as the case may be ) is : a ) after a heavy meal , i prepared slowly to go home digesting b ) after a heavy meal , i prepared to slowly go home digesting c ) after a heavy meal , i prepared to go home slowly digesting in this context , with the possible exception of the third case , the natural ( and therefore near-enough unambiguous ) association of the adverb is as follows : a ) after a heavy meal , i prepared _ slowly to go home digesting b ) after a heavy meal , i prepared to slowly _ go _ home digesting c ) after a heavy meal , i prepared to go home slowly _ digesting ( this was long ago , when you got glared at for splitting an infinitive regardless of whether it was the only place to put the adverb so as to express what you meant and not something else : this example achieved , as nearly as i could , three quite distinct and natural meanings for \" . . . slowly to go home . . . \" , \" . . . to slowly go home . . . \" and \" . . . to go home slowly . . . \" . i \\' m not 100 per cent happy with it , for obvious reasons , and it would be interesting to see if anyone can come up with a better , more clear-cut one ) . [ and , to really \" epater les bourgeois \" , i reckon you could even make a case for \" . . . i prepared to , slowly , go home digesting \" : the implication being that the meal was so very heavy that the walk home should be correpondingly delicate , as emphasised by the pause in rhythm marked by the commas ] . ted . ( ted . harding @ nessie . mcc . ac . uk )\\n']\n"
     ]
    }
   ],
   "source": [
    "# I use only the test set not but later I will create a function to iterate everything\n",
    "# and this will allow to have results over train and test data.\n",
    "\n",
    "print(test.values().take(1)) # Function values leaves out the key \n",
    "test_val = test.values() # This leaves out the key ( the filename. ex: '8-1064msg1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Subject re  8  1044  disc  grammar in schools\\n\\n re message from  linguist @ linguistlist  org  > > linguist list  vol-8 - 1044  sat jul 12 1997  issn  1068-4875  > > subject  8  1044  disc  grammar in schools > > i know and teach that not all infinitives contain ` to '  i also give > the students examples  e  g  ` i asked him to kindly apologise '  where > placing the adverb anywhere else would cause ambiguity  > > jennifer chew an example i once concocted to justify  splitting the infintive   or not  as the case may be  is  a  after a heavy meal  i prepared slowly to go home digesting b  after a heavy meal  i prepared to slowly go home digesting c  after a heavy meal  i prepared to go home slowly digesting in this context  with the possible exception of the third case  the natural  and therefore near-enough unambiguous  association of the adverb is as follows  a  after a heavy meal  i prepared  slowly to go home digesting b  after a heavy meal  i prepared to slowly  go  home digesting c  after a heavy meal  i prepared to go home slowly  digesting  this was long ago  when you got glared at for splitting an infinitive regardless of whether it was the only place to put the adverb so as to express what you meant and not something else  this example achieved  as nearly as i could  three quite distinct and natural meanings for     slowly to go home          to slowly go home     and     to go home slowly      i ' m not 100 per cent happy with it  for obvious reasons  and it would be interesting to see if anyone can come up with a better  more clear-cut one    and  to really  epater les bourgeois   i reckon you could even make a case for     i prepared to  slowly  go home digesting   the implication being that the meal was so very heavy that the walk home should be correpondingly delicate  as emphasised by the pause in rhythm marked by the commas   ted   ted  harding @ nessie  mcc  ac  uk \\n\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. REMOVE PUNCTUATION \n",
    "# This is helpful to remove noise from the data, i.e we remove the punctuation that does not lead\n",
    "# any significant inside to the model.\n",
    "# However, I keep the ! and ? because spam tend to often have these simbols as a way for advertisment to give enthusiasm\n",
    "\n",
    "test_val =  test_val.map(lambda x: re.sub('[:()\\[\\],.?!\";_]','',x))\n",
    "test_val.take(1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subject',\n",
       " 're',\n",
       " '8',\n",
       " '1044',\n",
       " 'disc',\n",
       " 'grammar',\n",
       " 'in',\n",
       " 'schools',\n",
       " 're',\n",
       " 'message']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. TOKENISATION\n",
    "\n",
    "#nltk.download('punkt') # Apply the Punkt sentence tokenizer: the standard tokenizer for English language\n",
    "test_val = test_val.map(nltk.word_tokenize) # This tokenises the RDD by applying word_tokenize to each of the tests through the map function\n",
    "# Verify the result\n",
    "test_val.take(1)[0][:10] # Narrow the output print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I need to unite again together the key and its values\n",
    "test = test.keys().zip(test_val)\n",
    "\n",
    "# We may have risked to create lots of empty strings.\n",
    "# Find a way to filter them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Look of data after tokenisation and punctuation removed:\n",
      " [('6-110msg3', ['Subject', 'job', 'posting', '-', 'apple-iss', 'research', 'center', 'content', '-', 'length', '3386', 'apple-iss', 'research', 'center', 'a', 'us', '$', '10', 'million', 'joint', 'venture', 'between', 'apple', 'computer', 'inc', 'and', 'the', 'institute', 'of', 'systems', 'science', 'of', 'the', 'national', 'university', 'of', 'singapore', 'located', 'in', 'singapore', 'is', 'looking', 'for', 'a', 'senior', 'speech', 'scientist', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'in', 'computational', 'linguistics', 'including', 'natural', 'language', 'processing', 'and', '*', '*', 'english', '*', '*', 'and', '*', '*', 'chinese', '*', '*', 'statistical', 'language', 'modeling', 'knowledge', 'of', 'state-of', '-', 'the-art', 'corpus-based', 'n', '-', 'gram', 'language', 'models', 'cache', 'language', 'models', 'and', 'part-of', '-', 'speech', 'language', 'models', 'are', 'required', 'a', 'text', '-', 'to', '-', 'speech', 'project', 'leader', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'expertise', 'in', 'two', 'or', 'more', 'of', 'the', 'following', 'areas', 'computational', 'linguistics', 'including', 'natural', 'language', 'parsing', 'lexical', 'database', 'design', 'and', 'statistical', 'language', 'modeling', 'text', 'tokenization', 'and', 'normalization', 'prosodic', 'analysis', 'substantial', 'knowledge', 'of', 'the', 'phonology', 'syntax', 'and', 'semantics', 'of', 'chinese', 'is', 'required', 'knowledge', 'of', 'acoustic', 'phonetics', 'and', '/', 'or', 'speech', 'signal', 'processing', 'is', 'desirable', 'both', 'candidates', 'will', 'have', 'a', 'phd', 'with', 'at', 'least', '2', 'to', '4', 'years', 'of', 'relevant', 'work', 'experience', 'or', 'a', 'technical', 'msc', 'degree', 'with', 'at', 'least', '5', 'to', '7', 'years', 'of', 'experienc', 'e', 'very', 'strong', 'software', 'engineering', 'skills', 'including', 'design', 'and', 'implementation', 'and', 'productization', 'are', 'required', 'in', 'these', 'positions', 'knowledge', 'of', 'c', 'c', '+', '+', 'and', 'unix', 'are', 'preferred', 'a', 'unix', '&', 'c', 'programmer', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'we', 'are', 'looking', 'for', 'an', 'experienced', 'unix', '&', 'c', 'programmer', 'preferably', 'with', 'good', 'industry', 'experience', 'to', 'join', 'us', 'in', 'breaking', 'new', 'frontiers', 'strong', 'knowledge', 'of', 'unix', 'tools', 'compilers', 'linkers', 'make', 'x', '-', 'windows', 'e', '-', 'mac', 'and', 'experience', 'in', 'matlab', 'required', 'sun', 'and', 'silicon', 'graphic', 'experience', 'is', 'an', 'advantage', 'programmers', 'with', 'less', 'than', 'two', 'years', 'industry', 'experience', 'need', 'not', 'apply', 'these', 'positions', 'include', 'interaction', 'with', 'scientists', 'in', 'the', 'national', 'university', 'of', 'singapore', 'and', 'with', 'apple', \"'s\", 'speech', 'research', 'and', 'productization', 'efforts', 'located', 'in', 'cupertino', 'california', 'attendance', 'and', 'publication', 'in', 'international', 'scientific', '/', 'engineering', 'conferences', 'is', 'encouraged', 'benefits', 'include', 'an', 'internationally', 'competitive', 'salary', 'housing', 'subsidy', 'and', 'relocation', 'expenses', 'send', 'a', 'complete', 'resume', 'enclosing', 'personal', 'particulars', 'qualifications', 'experience', 'and', 'contact', 'telephone', 'number', 'to', 'mr', 'jean', '-', 'luc', 'lebrun', 'center', 'manager', 'apple', '-', 'iss', 'research', 'center', 'institute', 'of', 'systems', 'science', 'heng', 'mui', 'keng', 'terrace', 'singapore', '0511', 'tel', '65', '772-6571', 'fax', '65', '776-4005', 'email', 'jllebrun', '@', 'iss', 'nus', 'sg']), ('6-126msg1', ['Subject', 'lang', 'classification', 'grimes', 'joseph', 'e', 'and', 'barbara', 'f', 'grimes', 'ethnologue', 'language', 'family', 'index', 'pb', 'isbn', '0-88312', '-', '708', '-', '3', 'vi', '116', 'pp', '$', '14', '00', 'summer', 'institute', 'of', 'linguistics', 'this', 'companion', 'volume', 'to', 'ethnologue', 'languages', 'of', 'the', 'world', 'twelfth', 'edition', 'lists', 'language', 'families', 'of', 'the', 'world', 'with', 'sub-groups', 'shown', 'in', 'a', 'tree', 'arrangement', 'under', 'the', 'broadest', 'classification', 'of', 'language', 'family', 'the', 'language', 'family', 'index', 'facilitates', 'locating', 'language', 'names', 'in', 'the', 'ethnologue', 'making', 'the', 'data', 'there', 'more', 'accessible', 'internet', 'academic', 'books', '@', 'sil', 'org', 'languages', 'reference', 'lang', '&', 'culture', 'gregerson', 'marilyn', 'ritual', 'belief', 'and', 'kinship', 'in', 'sulawesi', 'pb', 'isbn', '0-88312', '-', '621', '-', '4', 'ix', '194', 'pp', '$', '25', '00', 'summer', 'institute', 'of', 'linguistics', 'seven', 'articles', 'discuss', 'five', 'language', 'groups', 'in', 'sulawesi', 'indonesia', 'the', 'primary', 'focus', 'is', 'on', 'cultural', 'matters', 'with', 'some', 'linguistic', 'content', 'topics', 'include', 'traditional', 'religion', 'and', 'beliefs', 'certain', 'ceremonies', 'and', 'kinship', 'internet', 'academic', 'books', '@', 'sil', 'org', 'language', 'and', 'society', 'indonesia', 'computers', '&', 'ling', 'weber', 'david', 'j', 'stephen', 'r', 'mcconnel', 'diana', 'd', 'weber', 'and', 'beth', 'j', 'bryson', 'primer', 'a', 'tool', 'for', 'developing', 'early', 'reading', 'materials', 'pb', 'isbn', '0-88313', '-', '678', '-', '8', 'xvi', '266', 'pp', '+', 'ms-dos', 'software', '$', '26', '00', 'summer', 'institute', 'of', 'linguistics', 'the', 'authors', 'present', 'a', 'computer', 'program', 'and', 'instructions', 'for', 'developing', 'reading', 'materials', 'in', 'languages', 'with', 'little', 'or', 'no', 'background', 'in', 'literacy', 'the', 'book', 'is', 'structured', 'as', 'a', 'how-to', 'manual', 'with', 'step', 'by', 'step', 'procedures', 'to', 'establish', 'an', 'appropriate', 'primer', 'sequence', 'and', 'to', 'organize', 'words', 'phrases', 'and', 'sentences', 'that', 'correlate', 'with', 'the', 'sequence', 'it', 'presupposes', 'a', 'thorough', 'knowledge', 'of', 'linguistics', 'internet', 'academic', 'books', '@', 'sil', 'org', 'literacy', 'computer'])]\n"
     ]
    }
   ],
   "source": [
    "# DEFINE A FUNCTION THAT SUMMARISE THE PROCESS DONE IN PART 2\n",
    "\n",
    "def remove_punct_tokenisation(rdd):\n",
    "    rdd_val = rdd.values()\n",
    "    rdd_val =  rdd_val.map(lambda x: re.sub('[:()\\[\\],.?!\";_]','',x))\n",
    "    rdd_val = rdd_val.map(nltk.word_tokenize) # This tokenises the RDD by applying word_tokenize to each of the tests through the map function\n",
    "    rdd = rdd.keys().zip(rdd_val)\n",
    "    print('\\n\\nLook of data after tokenisation and punctuation removed:\\n', rdd.take(2))\n",
    "    return rdd\n",
    "\n",
    "# Run the function \n",
    "train = remove_punct_tokenisation(train)\n",
    "#train.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convert Text files into Vectors of fixed size and Encode the Target into a Binary variable\n",
    "\n",
    "This process is necessary to allow use the strings as useful inputs for Machine Learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vector: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \n",
      "\n",
      "\n",
      "Hashed word: -3636030952904220701\n",
      "Hashed word % N: 9\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "Hashed word: -5940189418225466560\n",
      "Hashed word % N: 0\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "Hashed word: -1407138643904109550\n",
      "Hashed word % N: 0\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "Hashed word: -1407138643904109550\n",
      "Hashed word % N: 0\n",
      "[3, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "Hashed word: 4880476542979782855\n",
      "Hashed word % N: 5\n",
      "[3, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
      "\n",
      "\n",
      "Vector identifying the Word Hello : [3, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Hash functions return a unique encode for each word.\n",
    "# www.youtube.com/watch?v=2BldESGZKB8 \n",
    "# Very useful reference for the concept\n",
    "\n",
    "\n",
    "# Let's see an example to observe if this concept could work: \n",
    "text = 'Hello'\n",
    "length = 10  # Length of the vectors. This parameter will be changed later on\n",
    "vec = [0] * length  # create vector of 0s of the dimension we define \n",
    "print('Initial vector:',vec, '\\n\\n')\n",
    "for word in text: \n",
    "                 \n",
    "    hsh = hash(word)  \n",
    "    print('Hashed word:', hsh)\n",
    "    print('Hashed word % N:', hsh % length)\n",
    "    vec[hsh % length] = vec[hsh % length] + 1 # You add one to this position of the vector\n",
    "    print(vec)\n",
    "print('\\n\\nVector identifying the Word', text, ':',vec)\n",
    " # return hashed word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens before Vectorization:\n",
      " [['Subject', 're', '8', '1044', 'disc', 'grammar', 'in', 'schools', 're', 'message', 'from', 'linguist', '@', 'linguistlist', 'org', '>', '>', 'linguist', 'list', 'vol-8', '-', '1044', 'sat', 'jul', '12', '1997', 'issn', '1068-4875', '>', '>', 'subject', '8', '1044', 'disc', 'grammar', 'in', 'schools', '>', '>', 'i', 'know', 'and', 'teach', 'that', 'not', 'all', 'infinitives', 'contain', '`', 'to', \"'\", 'i', 'also', 'give', '>', 'the', 'students', 'examples', 'e', 'g', '`', 'i', 'asked', 'him', 'to', 'kindly', 'apologise', \"'\", 'where', '>', 'placing', 'the', 'adverb', 'anywhere', 'else', 'would', 'cause', 'ambiguity', '>', '>', 'jennifer', 'chew', 'an', 'example', 'i', 'once', 'concocted', 'to', 'justify', 'splitting', 'the', 'infintive', 'or', 'not', 'as', 'the', 'case', 'may', 'be', 'is', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'in', 'this', 'context', 'with', 'the', 'possible', 'exception', 'of', 'the', 'third', 'case', 'the', 'natural', 'and', 'therefore', 'near-enough', 'unambiguous', 'association', 'of', 'the', 'adverb', 'is', 'as', 'follows', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'this', 'was', 'long', 'ago', 'when', 'you', 'got', 'glared', 'at', 'for', 'splitting', 'an', 'infinitive', 'regardless', 'of', 'whether', 'it', 'was', 'the', 'only', 'place', 'to', 'put', 'the', 'adverb', 'so', 'as', 'to', 'express', 'what', 'you', 'meant', 'and', 'not', 'something', 'else', 'this', 'example', 'achieved', 'as', 'nearly', 'as', 'i', 'could', 'three', 'quite', 'distinct', 'and', 'natural', 'meanings', 'for', 'slowly', 'to', 'go', 'home', 'to', 'slowly', 'go', 'home', 'and', 'to', 'go', 'home', 'slowly', 'i', \"'\", 'm', 'not', '100', 'per', 'cent', 'happy', 'with', 'it', 'for', 'obvious', 'reasons', 'and', 'it', 'would', 'be', 'interesting', 'to', 'see', 'if', 'anyone', 'can', 'come', 'up', 'with', 'a', 'better', 'more', 'clear-cut', 'one', 'and', 'to', 'really', 'epater', 'les', 'bourgeois', 'i', 'reckon', 'you', 'could', 'even', 'make', 'a', 'case', 'for', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'the', 'implication', 'being', 'that', 'the', 'meal', 'was', 'so', 'very', 'heavy', 'that', 'the', 'walk', 'home', 'should', 'be', 'correpondingly', 'delicate', 'as', 'emphasised', 'by', 'the', 'pause', 'in', 'rhythm', 'marked', 'by', 'the', 'commas', 'ted', 'ted', 'harding', '@', 'nessie', 'mcc', 'ac', 'uk']]\n",
      "\n",
      "\n",
      "Tokens after Vectorization:\n",
      " [[37, 47, 49, 41, 46, 57, 35, 38]]\n",
      "\n",
      "\n",
      "Tokens after Vectorization:\n",
      " [[9, 9, 12, 10, 2, 5, 19, 4, 12, 1, 3, 2, 13, 1, 6, 22, 2, 17, 5, 5, 5, 10, 2, 14, 9, 5, 4, 11, 1, 4, 10, 1, 9, 7, 4, 9, 2, 8, 1, 16, 6, 5, 5, 1, 17, 4, 2, 7, 7, 5]]\n"
     ]
    }
   ],
   "source": [
    "# Let's Try to apply this to our task\n",
    "\n",
    "# The MAIN GOAL IS TO HAVE RETURNED NUMERICAL INPUTS THAT CAN BE ABLE TO \n",
    "# 1. be input into the model\n",
    "# 2. Have the same vector size for each vector ( that I define with the parameter/variable N)\n",
    "\n",
    "def hash_vectors(text, vect_length): \n",
    "    vec = [0] * vect_length  # initialise a vector of all zeroes\n",
    "    for x in text: \n",
    "        hsh = hash(x) # Every word defined by its hash\n",
    "        vec[hsh % vect_length] = vec[hsh % vect_length] + 1 # This adds 1 in the index defined.\n",
    "                                                            # In case the letter appears again then 1 would be sum to 1 and so on.\n",
    "    return vec # vector identifying the word\n",
    "\n",
    "\n",
    "test_keys = test.keys() # These are the labels that identify whether is spam or ham\n",
    "test_val = test.values() # I select only the values \n",
    "test_val1 = test_val.map(lambda text: hash_vectors(text,8)) # Apply the function over the rdd values \n",
    "\n",
    "print('Tokens before Vectorization:\\n', test_val.take(1))\n",
    "print('\\n\\nTokens after Vectorization:\\n', test_val1.take(1))\n",
    "\n",
    "# Try another dimension of vector\n",
    "test_val1 = test_val.map(lambda text: hash_vectors(text,50)) \n",
    "print('\\n\\nTokens after Vectorization:\\n', test_val1.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the result over the first 10 targets:\n",
      "\n",
      " Before:\n",
      " ['8-1064msg1', '6-806msg1', '6-816msg1', '8-1208msg1', 'spmsgc111', 'spmsgc105', 'spmsgc139', 'spmsgc138', 'spmsgc104', 'spmsgc110'] \n",
      "\n",
      "After:\n",
      " [0, 0, 0, 0, 1, 1, 1, 1, 1, 1] \n",
      "\n",
      "\n",
      "Verify if the Labels and Values have been merged back:\n",
      " [(0, [78, 56, 88, 59, 69]), (0, [12, 8, 3, 4, 12])]\n"
     ]
    }
   ],
   "source": [
    "# ENCODE THE TARGET\n",
    "# I need to turn the keys into binary factors that identify whether the message\n",
    "# is Spam or not ( 1 or 0 )\n",
    "\n",
    "\n",
    "# Let's organize a way to replace the string with 1 if it's spam or 0 if it's not\n",
    "\n",
    "a = test_keys.take(4) # Example with only 4 elements in the list \n",
    "for z,i in zip(a, range(0,len(a))):\n",
    "    if z.startswith('spm'):\n",
    "        a[i] = 1\n",
    "    else:\n",
    "        a[i] = 0\n",
    "\n",
    "\n",
    "# Define a function to replace the element \n",
    "def binary_target(z):\n",
    "    #for z,i in zip(list_, range(0,len(list_))): # For future reference: This was the mistake\n",
    "                                                 # I was doing for an hour getting error. \n",
    "                                                 # Remember, when you operate a map\n",
    "                                                 # you are iterating over the token directly,\n",
    "                                                 # not the list. \n",
    "                                                \n",
    "    if z.startswith('spm'):\n",
    "        z = 1\n",
    "        #list_[i] = 1           # Future reference of what to NOT do\n",
    "    else:\n",
    "        z = 0\n",
    "        #list_[i] = 0\n",
    "    return z\n",
    "            \n",
    "\n",
    "# Make the function work for the Rdd file\n",
    "test_keys_binary = test_keys.map(binary_target)\n",
    "print('Check the result over the first 10 targets:\\n\\n','Before:\\n',test_keys.take(10),\n",
    "      '\\n\\nAfter:\\n',test_keys_binary.take(10),'\\n\\n')   \n",
    "#cls_vec_RDD = test_keys.map(lambda x: (1 if x.startswith('spm') else 0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# NOW, I ZIP THE LABELS AND THE INPUT BACK TOGETHER\n",
    "\n",
    "# Keep 5 as vector dimension so far\n",
    "test_val1 = test_val.map(lambda text: hash_vectors(text,5)) \n",
    "test = test_keys_binary.zip(test_val1)\n",
    "print('Verify if the Labels and Values have been merged back:\\n', test.take(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tokens before Vectorization:\n",
      " [['Subject', 'job', 'posting', '-', 'apple-iss', 'research', 'center', 'content', '-', 'length', '3386', 'apple-iss', 'research', 'center', 'a', 'us', '$', '10', 'million', 'joint', 'venture', 'between', 'apple', 'computer', 'inc', 'and', 'the', 'institute', 'of', 'systems', 'science', 'of', 'the', 'national', 'university', 'of', 'singapore', 'located', 'in', 'singapore', 'is', 'looking', 'for', 'a', 'senior', 'speech', 'scientist', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'in', 'computational', 'linguistics', 'including', 'natural', 'language', 'processing', 'and', '*', '*', 'english', '*', '*', 'and', '*', '*', 'chinese', '*', '*', 'statistical', 'language', 'modeling', 'knowledge', 'of', 'state-of', '-', 'the-art', 'corpus-based', 'n', '-', 'gram', 'language', 'models', 'cache', 'language', 'models', 'and', 'part-of', '-', 'speech', 'language', 'models', 'are', 'required', 'a', 'text', '-', 'to', '-', 'speech', 'project', 'leader', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'expertise', 'in', 'two', 'or', 'more', 'of', 'the', 'following', 'areas', 'computational', 'linguistics', 'including', 'natural', 'language', 'parsing', 'lexical', 'database', 'design', 'and', 'statistical', 'language', 'modeling', 'text', 'tokenization', 'and', 'normalization', 'prosodic', 'analysis', 'substantial', 'knowledge', 'of', 'the', 'phonology', 'syntax', 'and', 'semantics', 'of', 'chinese', 'is', 'required', 'knowledge', 'of', 'acoustic', 'phonetics', 'and', '/', 'or', 'speech', 'signal', 'processing', 'is', 'desirable', 'both', 'candidates', 'will', 'have', 'a', 'phd', 'with', 'at', 'least', '2', 'to', '4', 'years', 'of', 'relevant', 'work', 'experience', 'or', 'a', 'technical', 'msc', 'degree', 'with', 'at', 'least', '5', 'to', '7', 'years', 'of', 'experienc', 'e', 'very', 'strong', 'software', 'engineering', 'skills', 'including', 'design', 'and', 'implementation', 'and', 'productization', 'are', 'required', 'in', 'these', 'positions', 'knowledge', 'of', 'c', 'c', '+', '+', 'and', 'unix', 'are', 'preferred', 'a', 'unix', '&', 'c', 'programmer', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'we', 'are', 'looking', 'for', 'an', 'experienced', 'unix', '&', 'c', 'programmer', 'preferably', 'with', 'good', 'industry', 'experience', 'to', 'join', 'us', 'in', 'breaking', 'new', 'frontiers', 'strong', 'knowledge', 'of', 'unix', 'tools', 'compilers', 'linkers', 'make', 'x', '-', 'windows', 'e', '-', 'mac', 'and', 'experience', 'in', 'matlab', 'required', 'sun', 'and', 'silicon', 'graphic', 'experience', 'is', 'an', 'advantage', 'programmers', 'with', 'less', 'than', 'two', 'years', 'industry', 'experience', 'need', 'not', 'apply', 'these', 'positions', 'include', 'interaction', 'with', 'scientists', 'in', 'the', 'national', 'university', 'of', 'singapore', 'and', 'with', 'apple', \"'s\", 'speech', 'research', 'and', 'productization', 'efforts', 'located', 'in', 'cupertino', 'california', 'attendance', 'and', 'publication', 'in', 'international', 'scientific', '/', 'engineering', 'conferences', 'is', 'encouraged', 'benefits', 'include', 'an', 'internationally', 'competitive', 'salary', 'housing', 'subsidy', 'and', 'relocation', 'expenses', 'send', 'a', 'complete', 'resume', 'enclosing', 'personal', 'particulars', 'qualifications', 'experience', 'and', 'contact', 'telephone', 'number', 'to', 'mr', 'jean', '-', 'luc', 'lebrun', 'center', 'manager', 'apple', '-', 'iss', 'research', 'center', 'institute', 'of', 'systems', 'science', 'heng', 'mui', 'keng', 'terrace', 'singapore', '0511', 'tel', '65', '772-6571', 'fax', '65', '776-4005', 'email', 'jllebrun', '@', 'iss', 'nus', 'sg']]\n",
      "\n",
      "\n",
      "Tokens after Vectorization:\n",
      " [[78, 56, 88, 59, 69]]\n",
      "Check function worked:\n",
      " [(0, [78, 154, 70, 50, 42, 65]), (0, [44, 62, 50, 39, 37, 34]), (0, [28, 42, 63, 42, 30, 38])]\n"
     ]
    }
   ],
   "source": [
    "# Put into a FUNCTION \n",
    "\n",
    "def vector_input(rdd, vect_length):\n",
    "    def hash_vectors(text, vect_length): \n",
    "        vec = [0] * vect_length  # initialise a vector of all zeroes\n",
    "        for x in text: \n",
    "            hsh = hash(x) # Every word defined by its hash\n",
    "            vec[hsh % vect_length] = vec[hsh % vect_length] + 1 # This adds 1 in the index defined.\n",
    "                                                            # In case the letter appears again then 1 would be sum to 1 and so on.\n",
    "        return vec # vector identifying the word\n",
    "\n",
    "\n",
    "    rdd_keys = rdd.keys() # These are the labels that identify whether is spam or ham\n",
    "    rdd_val = rdd.values() # I select only the values \n",
    "    rdd_val1 = rdd_val.map(lambda text: hash_vectors(text, vect_length)) # Apply the function over the rdd values \n",
    "    rdd_keys_binary = rdd_keys.map(binary_target)\n",
    "    print('\\n\\nTokens before Vectorization:\\n', rdd_val.take(1))\n",
    "    print('\\n\\nTokens after Vectorization:\\n', test_val1.take(1))\n",
    "    rdd = rdd_keys_binary.zip(rdd_val1)\n",
    "    return rdd   \n",
    "\n",
    "# Run the Function \n",
    "train = vector_input(train, 6)\n",
    "print('Check function worked:\\n', train.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Normalise the Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the values are now Normalised:\n",
      " [DenseVector([0.4913, 0.3527, 0.5543, 0.3716, 0.4346]), DenseVector([0.618, 0.412, 0.1545, 0.206, 0.618]), DenseVector([0.4169, 0.3335, 0.4235, 0.4669, 0.5636])]\n",
      "\n",
      "\n",
      "Check the values are zipped back:\n",
      " [(0, DenseVector([0.4913, 0.3527, 0.5543, 0.3716, 0.4346])), (0, DenseVector([0.618, 0.412, 0.1545, 0.206, 0.618])), (0, DenseVector([0.4169, 0.3335, 0.4235, 0.4669, 0.5636]))]\n"
     ]
    }
   ],
   "source": [
    "# Split again the keys and the values.\n",
    "# I know this is reduntant but I think it may be better in the future in case of modification\n",
    "# and when I will set the functions.\n",
    "# I obviously don't need to Normalise the Target since it's a binary category of 0 and 1.\n",
    "\n",
    "test_val = test.values()\n",
    "normalizer1 = Normalizer() # set the normalizer \n",
    "test_val = normalizer1.transform(test_val)\n",
    "print('Check the values are now Normalised:\\n',test_val.take(3))\n",
    "\n",
    "# 'from 'https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html'\n",
    "# Each sample normalized using $L^2$ norm.\n",
    "\n",
    "test = test.keys().zip(test_val)\n",
    "print('\\n\\nCheck the values are zipped back:\\n',test.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if Lab_points function worked:\n",
      "\n",
      " [LabeledPoint(0.0, [0.4912953308535948,0.3527248529205296,0.5542819117322608,0.37162082718412937,0.4346074080627954]), LabeledPoint(0.0, [0.6180314431495256,0.4120209620996838,0.1545078607873814,0.2060104810498419,0.6180314431495256]), LabeledPoint(0.0, [0.4168635654068489,0.3334908523254791,0.4235333824533585,0.46688719325567074,0.5635995404300597])]\n",
      "\n",
      "\n",
      "Check if Lab_points_rdd function worked:\n",
      "\n",
      " [LabeledPoint(0.0, [0.4912953308535948,0.3527248529205296,0.5542819117322608,0.37162082718412937,0.4346074080627954]), LabeledPoint(0.0, [0.6180314431495256,0.4120209620996838,0.1545078607873814,0.2060104810498419,0.6180314431495256]), LabeledPoint(0.0, [0.4168635654068489,0.3334908523254791,0.4235333824533585,0.46688719325567074,0.5635995404300597])]\n"
     ]
    }
   ],
   "source": [
    "# I have the values back in the form (label,DenseVector(VectorValues))\n",
    "# I may need to turn them into LabeledPoints ( Label, VectorValues)\n",
    "# This is because Supervised Learning Machine Learning algorithms\n",
    "# require this type of input to work on (Py)spark.\n",
    "\n",
    "# From 'https://spark.apache.org/docs/2.2.0/mllib-data-types.html'\n",
    "# 'A labeled point is a local vector, either dense or sparse, associated with a \n",
    "# label/response. In MLlib, labeled points are used in supervised learning algorithms. \n",
    "# We use a double to store a label, so we can use labeled points in both regression and \n",
    "# classification. For binary classification, a label should be either 0 (negative) or 1 \n",
    "# (positive)' \n",
    "\n",
    "\n",
    "# Let's set the function starting with a small example\n",
    "#a = test.keys().take(3)\n",
    "#b = test.values().take(3)\n",
    "#c = test.take(1)\n",
    "    \n",
    "# I think the problem is that once I map the RDD I cannot use the argument .keys and values anymore\n",
    "# So I should probably work with index\n",
    "    \n",
    "    \n",
    "# Define the function that returns a LabeledPoint from each of the lists contained in the rdd\n",
    "# Then input this with a lambda\n",
    "def Lab_points(c):\n",
    "    c = LabeledPoint(c[0],c[1])\n",
    "    return c\n",
    " \n",
    "# Run the function \n",
    "TestLabPoint = test.map(Lab_points)\n",
    "print('Check if Lab_points function worked:\\n\\n',TestLabPoint.take(3))\n",
    "\n",
    "def Lab_points_rdd(rdd):\n",
    "    def Lab_points(c):\n",
    "        c = LabeledPoint(c[0],c[1])\n",
    "        return c\n",
    "    RDD = rdd.map(Lab_points)\n",
    "    return RDD\n",
    "\n",
    "# RUN THE FINAL FUNCTION that takes an rdd with (label,DenseVector) and returns LabeledPoints(label,vector)\n",
    "\n",
    "test = Lab_points_rdd(test)\n",
    "print('\\n\\nCheck if Lab_points_rdd function worked:\\n\\n',test.take(3))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Data are now like:\n",
      " [LabeledPoint(0.0, [0.37532546250723536,0.7410271952065929,0.33683054327572404,0.24059324519694572,0.20209832596543442,0.3127712187560294]), LabeledPoint(0.0, [0.396315989816045,0.5584452583771543,0.4503590793364148,0.3512800818824035,0.33326571870894695,0.3062441739487621]), LabeledPoint(0.0, [0.27163990004069294,0.4074598500610394,0.611189775091559,0.4074598500610394,0.2910427500435996,0.3686541500552261])]\n",
      "\n",
      "\n",
      "Check if function Norm_and_LabPoints worked:\n",
      "\n",
      " [LabeledPoint(0.0, [0.37532546250723536,0.7410271952065929,0.33683054327572404,0.24059324519694572,0.20209832596543442,0.3127712187560294]), LabeledPoint(0.0, [0.396315989816045,0.5584452583771543,0.4503590793364148,0.3512800818824035,0.33326571870894695,0.3062441739487621])]\n"
     ]
    }
   ],
   "source": [
    "# NOW CREATE A FUNCTION THAT NORMALISE AND GIVES YOU VALUES BACK INTO A LABELEDPOINT LOCAL VECTOR\n",
    "\n",
    "def Norm_and_LabPoint(rdd):\n",
    "    rdd_val = rdd.values()\n",
    "    normalizer1 = Normalizer() # set the normalizer \n",
    "    rdd_val = normalizer1.transform(rdd_val)\n",
    "    rdd = rdd.keys().zip(rdd_val)\n",
    "    rdd = Lab_points_rdd(rdd)\n",
    "    print('\\n\\n\\n\\nData are now like:\\n',rdd.take(3))\n",
    "    return rdd\n",
    "\n",
    "# Check on the Train data if it worked:\n",
    "train = Norm_and_LabPoint(train)\n",
    "print('\\n\\nCheck if function Norm_and_LabPoints worked:\\n\\n',train.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allow Reproducibility of Results by Defining a Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to access to directory: bare \n",
      "\n",
      "Verify if the directory is properly set:\n",
      "bare/part3\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part3\n",
      "Verify if the directory is properly set:\n",
      "bare/part4\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part4\n",
      "Verify if the directory is properly set:\n",
      "bare/part5\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part5\n",
      "Verify if the directory is properly set:\n",
      "bare/part2\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part2\n",
      "Verify if the directory is properly set:\n",
      "bare/part10\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part10\n",
      "Verify if the directory is properly set:\n",
      "bare/part9\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part9\n",
      "Verify if the directory is properly set:\n",
      "bare/part7\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part7\n",
      "Verify if the directory is properly set:\n",
      "bare/part1\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part1\n",
      "Verify if the directory is properly set:\n",
      "bare/part6\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part6\n",
      "Verify if the directory is properly set:\n",
      "bare/part8\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part8\n",
      "\n",
      "\n",
      "\n",
      "Check if split function worked:\n",
      "\n",
      " [('6-266msg3', 'Subject: bisfai deadline extension !\\n\\nbisfai deadline extension ! the deadline for the bar - ilan symposium on foundations of artificial intelligence has been extended to february 27 . the conference itself will take place as scheduled , june 20-22 , in ramat - gan and jerusalem , israel . for more information contact : bisfai @ bimacs . cs . biu . ac . il daniel radzinski tovna translation machines jerusalem , israel dr @ tovna . co . il\\n')]\n",
      "\n",
      "\n",
      "Data accessed and imported correctly\n",
      "\n",
      "Num Rdd used for Train: 8\n",
      "\n",
      "\n",
      "Train and Test split completed\n",
      "\n",
      "\n",
      "\n",
      "Look of data after tokenisation and punctuation removed:\n",
      " [('6-110msg3', ['Subject', 'job', 'posting', '-', 'apple-iss', 'research', 'center', 'content', '-', 'length', '3386', 'apple-iss', 'research', 'center', 'a', 'us', '$', '10', 'million', 'joint', 'venture', 'between', 'apple', 'computer', 'inc', 'and', 'the', 'institute', 'of', 'systems', 'science', 'of', 'the', 'national', 'university', 'of', 'singapore', 'located', 'in', 'singapore', 'is', 'looking', 'for', 'a', 'senior', 'speech', 'scientist', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'in', 'computational', 'linguistics', 'including', 'natural', 'language', 'processing', 'and', '*', '*', 'english', '*', '*', 'and', '*', '*', 'chinese', '*', '*', 'statistical', 'language', 'modeling', 'knowledge', 'of', 'state-of', '-', 'the-art', 'corpus-based', 'n', '-', 'gram', 'language', 'models', 'cache', 'language', 'models', 'and', 'part-of', '-', 'speech', 'language', 'models', 'are', 'required', 'a', 'text', '-', 'to', '-', 'speech', 'project', 'leader', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'expertise', 'in', 'two', 'or', 'more', 'of', 'the', 'following', 'areas', 'computational', 'linguistics', 'including', 'natural', 'language', 'parsing', 'lexical', 'database', 'design', 'and', 'statistical', 'language', 'modeling', 'text', 'tokenization', 'and', 'normalization', 'prosodic', 'analysis', 'substantial', 'knowledge', 'of', 'the', 'phonology', 'syntax', 'and', 'semantics', 'of', 'chinese', 'is', 'required', 'knowledge', 'of', 'acoustic', 'phonetics', 'and', '/', 'or', 'speech', 'signal', 'processing', 'is', 'desirable', 'both', 'candidates', 'will', 'have', 'a', 'phd', 'with', 'at', 'least', '2', 'to', '4', 'years', 'of', 'relevant', 'work', 'experience', 'or', 'a', 'technical', 'msc', 'degree', 'with', 'at', 'least', '5', 'to', '7', 'years', 'of', 'experienc', 'e', 'very', 'strong', 'software', 'engineering', 'skills', 'including', 'design', 'and', 'implementation', 'and', 'productization', 'are', 'required', 'in', 'these', 'positions', 'knowledge', 'of', 'c', 'c', '+', '+', 'and', 'unix', 'are', 'preferred', 'a', 'unix', '&', 'c', 'programmer', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'we', 'are', 'looking', 'for', 'an', 'experienced', 'unix', '&', 'c', 'programmer', 'preferably', 'with', 'good', 'industry', 'experience', 'to', 'join', 'us', 'in', 'breaking', 'new', 'frontiers', 'strong', 'knowledge', 'of', 'unix', 'tools', 'compilers', 'linkers', 'make', 'x', '-', 'windows', 'e', '-', 'mac', 'and', 'experience', 'in', 'matlab', 'required', 'sun', 'and', 'silicon', 'graphic', 'experience', 'is', 'an', 'advantage', 'programmers', 'with', 'less', 'than', 'two', 'years', 'industry', 'experience', 'need', 'not', 'apply', 'these', 'positions', 'include', 'interaction', 'with', 'scientists', 'in', 'the', 'national', 'university', 'of', 'singapore', 'and', 'with', 'apple', \"'s\", 'speech', 'research', 'and', 'productization', 'efforts', 'located', 'in', 'cupertino', 'california', 'attendance', 'and', 'publication', 'in', 'international', 'scientific', '/', 'engineering', 'conferences', 'is', 'encouraged', 'benefits', 'include', 'an', 'internationally', 'competitive', 'salary', 'housing', 'subsidy', 'and', 'relocation', 'expenses', 'send', 'a', 'complete', 'resume', 'enclosing', 'personal', 'particulars', 'qualifications', 'experience', 'and', 'contact', 'telephone', 'number', 'to', 'mr', 'jean', '-', 'luc', 'lebrun', 'center', 'manager', 'apple', '-', 'iss', 'research', 'center', 'institute', 'of', 'systems', 'science', 'heng', 'mui', 'keng', 'terrace', 'singapore', '0511', 'tel', '65', '772-6571', 'fax', '65', '776-4005', 'email', 'jllebrun', '@', 'iss', 'nus', 'sg']), ('6-126msg1', ['Subject', 'lang', 'classification', 'grimes', 'joseph', 'e', 'and', 'barbara', 'f', 'grimes', 'ethnologue', 'language', 'family', 'index', 'pb', 'isbn', '0-88312', '-', '708', '-', '3', 'vi', '116', 'pp', '$', '14', '00', 'summer', 'institute', 'of', 'linguistics', 'this', 'companion', 'volume', 'to', 'ethnologue', 'languages', 'of', 'the', 'world', 'twelfth', 'edition', 'lists', 'language', 'families', 'of', 'the', 'world', 'with', 'sub-groups', 'shown', 'in', 'a', 'tree', 'arrangement', 'under', 'the', 'broadest', 'classification', 'of', 'language', 'family', 'the', 'language', 'family', 'index', 'facilitates', 'locating', 'language', 'names', 'in', 'the', 'ethnologue', 'making', 'the', 'data', 'there', 'more', 'accessible', 'internet', 'academic', 'books', '@', 'sil', 'org', 'languages', 'reference', 'lang', '&', 'culture', 'gregerson', 'marilyn', 'ritual', 'belief', 'and', 'kinship', 'in', 'sulawesi', 'pb', 'isbn', '0-88312', '-', '621', '-', '4', 'ix', '194', 'pp', '$', '25', '00', 'summer', 'institute', 'of', 'linguistics', 'seven', 'articles', 'discuss', 'five', 'language', 'groups', 'in', 'sulawesi', 'indonesia', 'the', 'primary', 'focus', 'is', 'on', 'cultural', 'matters', 'with', 'some', 'linguistic', 'content', 'topics', 'include', 'traditional', 'religion', 'and', 'beliefs', 'certain', 'ceremonies', 'and', 'kinship', 'internet', 'academic', 'books', '@', 'sil', 'org', 'language', 'and', 'society', 'indonesia', 'computers', '&', 'ling', 'weber', 'david', 'j', 'stephen', 'r', 'mcconnel', 'diana', 'd', 'weber', 'and', 'beth', 'j', 'bryson', 'primer', 'a', 'tool', 'for', 'developing', 'early', 'reading', 'materials', 'pb', 'isbn', '0-88313', '-', '678', '-', '8', 'xvi', '266', 'pp', '+', 'ms-dos', 'software', '$', '26', '00', 'summer', 'institute', 'of', 'linguistics', 'the', 'authors', 'present', 'a', 'computer', 'program', 'and', 'instructions', 'for', 'developing', 'reading', 'materials', 'in', 'languages', 'with', 'little', 'or', 'no', 'background', 'in', 'literacy', 'the', 'book', 'is', 'structured', 'as', 'a', 'how-to', 'manual', 'with', 'step', 'by', 'step', 'procedures', 'to', 'establish', 'an', 'appropriate', 'primer', 'sequence', 'and', 'to', 'organize', 'words', 'phrases', 'and', 'sentences', 'that', 'correlate', 'with', 'the', 'sequence', 'it', 'presupposes', 'a', 'thorough', 'knowledge', 'of', 'linguistics', 'internet', 'academic', 'books', '@', 'sil', 'org', 'literacy', 'computer'])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Look of data after tokenisation and punctuation removed:\n",
      " [('9-1296msg1', ['Subject', 'new', 'book', 'australian', 'language', 'australian', 'language', 'nordlinger', 'rachel', 'max', 'planck', 'institute', 'for', 'psycholinguistics', 'nijmegen', 'constructive', 'case', 'isbn', '1-57586', '-', '134', '-', '8', 'paper', '1-57586', '-', '135', '-', '6', 'cloth', 'csli', 'publications', '1998', 'http', '/', '/', 'csli-www', 'stanford', 'edu', '/', 'publications', '/', 'email', 'pubs', '@', 'roslin', 'stanford', 'edu', 'australian', 'aboriginal', 'languages', 'have', 'many', 'interesting', 'grammatical', 'characteristics', 'that', 'challenge', 'some', 'of', 'the', 'central', 'assumptions', 'of', 'current', 'linguistic', 'theory', 'these', 'languages', 'exhibit', 'many', 'unusual', 'morphosyntactic', 'characteristics', 'that', 'have', 'not', 'yet', 'been', 'adequately', 'incorporated', 'into', 'current', 'linguistic', 'theory', 'this', 'volume', 'focuses', 'on', 'the', 'complex', 'properties', 'of', 'case', 'morphology', 'in', 'these', 'nonconfigurational', 'languages', 'including', 'extensive', 'case', 'stacking', 'and', 'the', 'use', 'of', 'case', 'to', 'mark', 'tense', '/', 'aspect', '/', 'mood', 'while', 'problematic', 'for', 'many', 'syntactic', 'approaches', 'these', 'case', 'properties', 'are', 'given', 'a', 'natural', 'and', 'unified', 'account', 'in', 'the', 'lexicalist', 'model', 'of', 'constructive', 'case', 'developed', 'in', 'this', 'book', 'which', 'allows', 'case', 'morphology', 'to', 'construct', 'the', 'larger', 'syntactic', 'context', 'independently', 'of', 'phrase', 'structure', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', 'csli', 'publications', 'ventura', 'hall', 'stanford', 'university', 'stanford', 'ca', '94305-4115', 'telephone', '650', '723-1839', 'fax', '650', '725-2166', 'http', '/', '/', 'csli-www', 'stanford', 'edu', '/', 'publications']), ('9-168msg1', ['Subject', '1998', 'tls', 'exploring', 'the', 'boundaries', 'between', 'phonetics', 'and', 'phonology', '1998', 'conference', 'of', 'the', 'texas', 'linguistics', 'society', 'exploring', 'the', 'boundaries', 'between', 'phonetics', 'and', 'phonology', 'march', '13-15', '1998', 'the', 'university', 'of', 'texas', 'at', 'austin', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'friday', 'march', '13th', '*', '8', '45', '-', '9', '45', 'registration', 'and', 'coffee', '*', '9', '45', '-', '10', '00', 'opening', 'remarks', '*', '10', '00', '-', '10', '40', 'natasha', 'warner', 'university', 'of', 'california', 'berkeley', 'o', 'integrating', 'speech', 'perception', 'and', 'formal', 'phonology', '*', '10', '40', '-', '11', '20', 'randall', 'gess', 'university', 'of', 'utah', 'o', 'phonetics', 'vs', 'phonology', 'in', 'sound', 'change', 'what', 'ot', 'has', 'to', 'say', '11', '20', '-', '11', '30', 'break', '*', '11', '30', '-', '12', '10', 'lisa', 'lavoie', 'cornell', 'university', 'o', 'effects', 'of', 'prosodic', 'structure', 'on', 'phonetic', 'and', 'phonological', 'consonant', 'weakening', '*', '12', '10', '-', '12', '50', 'tivoli', 'majors', 'university', 'of', 'texas', 'at', 'austin', 'o', 'the', 'parallel', 'role', 'of', 'stress', 'in', 'vowel', 'harmony', 'and', 'v', '-', 'v', 'coarticulation', '12', '50', '-', '2', '30', 'lunch', '*', '2', '30', '-', '3', '10', 'kenneth', 'de', 'jong', 'indiana', 'university', 'bushra', 'zawaydeh', 'indiana', 'university', 'o', 'a', 'sketch', 'of', 'arabic', 'stress', 'and', 'durational', 'structure', '*', '3', '10', '-', '3', '50', 'anna', 'bosch', 'university', 'of', 'kentucky', 'kenneth', 'de', 'jong', 'indiana', 'university', 'o', 'syllables', 'and', 'supersyllables', 'evidence', 'for', 'low', 'level', 'phonological', 'domains', '3', '50', '-', '4', '00', 'break', '*', '4', '00', '-', '5', '00', 'keynote', 'address', 'abigail', 'cohn', 'cornell', 'university', 'saturday', 'march', '14th', '8', '30', '-', '9', '00', 'registration', 'and', 'coffee', '*', '9', '00', '-', '10', '00', 'keynote', 'address', 'patricia', 'keating', 'ucla', '10', '00', '-', '10', '10', 'break', '*', '10', '10', '-', '10', '50', 'robert', 'j', 'podesva', 'cornell', 'university', 'o', 'an', 'acoustic', 'analysis', 'of', 'buginese', 'consonants', '*', '10', '50', '-', '11', '30', 'bill', 'ham', 'cornell', 'university', 'o', 'some', 'effects', 'of', 'language', '-', 'specific', 'timing', 'strategies', 'on', 'vowel', 'duration', '11', '30', '-', '1', '00', 'lunch', '*', '1', '00', '-', '1', '40', 'bushra', 'zawaydeh', 'indiana', 'university', 'o', 'the', 'natural', 'class', 'guttural', 'endoscopic', 'and', 'acoustic', 'evidence', '*', '1', '40', '-', '2', '20', 'madelaine', 'plauche', 'university', 'of', 'california', 'berkeley', 'o', 'glottalized', 'sonorants', 'in', 'yowlumne', '*', '2', '20', '-', '3', '00', 'ioana', 'chitoran', 'dartmouth', 'college', 'o', 'georgian', 'harmonic', 'clusters', 'phonetic', 'cues', 'to', 'phonological', 'patterns', '3', '00', '-', '3', '10', 'break', '*', '3', '10', '-', '3', '50', 'chip', 'gerfen', 'university', 'of', 'north', 'carolina', 'chapel', 'hill', 'paul', 'denisowski', 'university', 'of', 'north', 'carolina', 'chapel', 'hill', 'o', 'h', 'igh', '-', 'h', 'igh', 'l', 'ow', '-', 'l', 'ow', 'w', 'hats', '-', 'w', 'hat', '*', '3', '50', '-', '4', '30', 'scott', 'myers', 'university', 'of', 'texas', 'at', 'austin', 'o', 'surface', 'underspecification', 'of', 'tone', 'in', 'chichewa', 'saturday', 'night', 'party', 'sunday', 'march', '15th', '8', '30', '-', '9', '00', 'coffee', '*', '9', '00', '-', '10', '00', 'keynote', 'address', 'janet', 'pierrehumbert', 'northwestern', 'university', '10', '00', '-', '10', '10', 'break', '*', '10', '10', '-', '10', '50', 'ayako', 'tsuchida', 'rutgers', 'university', 'o', 'phonetic', 'and', 'phonological', 'vowel', 'devoicing', 'in', 'japanese', '*', '10', '50', '-', '11', '30', 'allyson', 'carter', 'university', 'of', 'arizona', 'o', 'the', 'phonetic', 'manifestation', 'of', 'unfooted', 'syllables', 'evidence', 'from', 'young', 'childrens', 'weak', 'syllable', 'omissions', '*', '11', '30', '-', '12', '10', 'iris', 'smorodinsky', 'haskins', 'laboratories', '&', 'yale', 'university', 'o', 'the', 'phonology', 'and', 'phonetics', 'of', 'schwa', 'in', 'parisian', 'french', 'an', 'articulatory', 'analysis', 'for', 'more', 'information', 'see', 'our', 'website', 'at', 'http', '/', '/', 'uts', 'cc', 'utexas', 'edu', '/', '~', 'tls', '/'])]\n",
      "\n",
      "\n",
      "Punctuation Removed and Tokenisation completed\n",
      "\n",
      "\n",
      "\n",
      "Tokens before Vectorization:\n",
      " [['Subject', 'job', 'posting', '-', 'apple-iss', 'research', 'center', 'content', '-', 'length', '3386', 'apple-iss', 'research', 'center', 'a', 'us', '$', '10', 'million', 'joint', 'venture', 'between', 'apple', 'computer', 'inc', 'and', 'the', 'institute', 'of', 'systems', 'science', 'of', 'the', 'national', 'university', 'of', 'singapore', 'located', 'in', 'singapore', 'is', 'looking', 'for', 'a', 'senior', 'speech', 'scientist', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'in', 'computational', 'linguistics', 'including', 'natural', 'language', 'processing', 'and', '*', '*', 'english', '*', '*', 'and', '*', '*', 'chinese', '*', '*', 'statistical', 'language', 'modeling', 'knowledge', 'of', 'state-of', '-', 'the-art', 'corpus-based', 'n', '-', 'gram', 'language', 'models', 'cache', 'language', 'models', 'and', 'part-of', '-', 'speech', 'language', 'models', 'are', 'required', 'a', 'text', '-', 'to', '-', 'speech', 'project', 'leader', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'expertise', 'in', 'two', 'or', 'more', 'of', 'the', 'following', 'areas', 'computational', 'linguistics', 'including', 'natural', 'language', 'parsing', 'lexical', 'database', 'design', 'and', 'statistical', 'language', 'modeling', 'text', 'tokenization', 'and', 'normalization', 'prosodic', 'analysis', 'substantial', 'knowledge', 'of', 'the', 'phonology', 'syntax', 'and', 'semantics', 'of', 'chinese', 'is', 'required', 'knowledge', 'of', 'acoustic', 'phonetics', 'and', '/', 'or', 'speech', 'signal', 'processing', 'is', 'desirable', 'both', 'candidates', 'will', 'have', 'a', 'phd', 'with', 'at', 'least', '2', 'to', '4', 'years', 'of', 'relevant', 'work', 'experience', 'or', 'a', 'technical', 'msc', 'degree', 'with', 'at', 'least', '5', 'to', '7', 'years', 'of', 'experienc', 'e', 'very', 'strong', 'software', 'engineering', 'skills', 'including', 'design', 'and', 'implementation', 'and', 'productization', 'are', 'required', 'in', 'these', 'positions', 'knowledge', 'of', 'c', 'c', '+', '+', 'and', 'unix', 'are', 'preferred', 'a', 'unix', '&', 'c', 'programmer', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'we', 'are', 'looking', 'for', 'an', 'experienced', 'unix', '&', 'c', 'programmer', 'preferably', 'with', 'good', 'industry', 'experience', 'to', 'join', 'us', 'in', 'breaking', 'new', 'frontiers', 'strong', 'knowledge', 'of', 'unix', 'tools', 'compilers', 'linkers', 'make', 'x', '-', 'windows', 'e', '-', 'mac', 'and', 'experience', 'in', 'matlab', 'required', 'sun', 'and', 'silicon', 'graphic', 'experience', 'is', 'an', 'advantage', 'programmers', 'with', 'less', 'than', 'two', 'years', 'industry', 'experience', 'need', 'not', 'apply', 'these', 'positions', 'include', 'interaction', 'with', 'scientists', 'in', 'the', 'national', 'university', 'of', 'singapore', 'and', 'with', 'apple', \"'s\", 'speech', 'research', 'and', 'productization', 'efforts', 'located', 'in', 'cupertino', 'california', 'attendance', 'and', 'publication', 'in', 'international', 'scientific', '/', 'engineering', 'conferences', 'is', 'encouraged', 'benefits', 'include', 'an', 'internationally', 'competitive', 'salary', 'housing', 'subsidy', 'and', 'relocation', 'expenses', 'send', 'a', 'complete', 'resume', 'enclosing', 'personal', 'particulars', 'qualifications', 'experience', 'and', 'contact', 'telephone', 'number', 'to', 'mr', 'jean', '-', 'luc', 'lebrun', 'center', 'manager', 'apple', '-', 'iss', 'research', 'center', 'institute', 'of', 'systems', 'science', 'heng', 'mui', 'keng', 'terrace', 'singapore', '0511', 'tel', '65', '772-6571', 'fax', '65', '776-4005', 'email', 'jllebrun', '@', 'iss', 'nus', 'sg']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tokens after Vectorization:\n",
      " [[78, 56, 88, 59, 69]]\n",
      "\n",
      "\n",
      "Tokens before Vectorization:\n",
      " [['Subject', 'new', 'book', 'australian', 'language', 'australian', 'language', 'nordlinger', 'rachel', 'max', 'planck', 'institute', 'for', 'psycholinguistics', 'nijmegen', 'constructive', 'case', 'isbn', '1-57586', '-', '134', '-', '8', 'paper', '1-57586', '-', '135', '-', '6', 'cloth', 'csli', 'publications', '1998', 'http', '/', '/', 'csli-www', 'stanford', 'edu', '/', 'publications', '/', 'email', 'pubs', '@', 'roslin', 'stanford', 'edu', 'australian', 'aboriginal', 'languages', 'have', 'many', 'interesting', 'grammatical', 'characteristics', 'that', 'challenge', 'some', 'of', 'the', 'central', 'assumptions', 'of', 'current', 'linguistic', 'theory', 'these', 'languages', 'exhibit', 'many', 'unusual', 'morphosyntactic', 'characteristics', 'that', 'have', 'not', 'yet', 'been', 'adequately', 'incorporated', 'into', 'current', 'linguistic', 'theory', 'this', 'volume', 'focuses', 'on', 'the', 'complex', 'properties', 'of', 'case', 'morphology', 'in', 'these', 'nonconfigurational', 'languages', 'including', 'extensive', 'case', 'stacking', 'and', 'the', 'use', 'of', 'case', 'to', 'mark', 'tense', '/', 'aspect', '/', 'mood', 'while', 'problematic', 'for', 'many', 'syntactic', 'approaches', 'these', 'case', 'properties', 'are', 'given', 'a', 'natural', 'and', 'unified', 'account', 'in', 'the', 'lexicalist', 'model', 'of', 'constructive', 'case', 'developed', 'in', 'this', 'book', 'which', 'allows', 'case', 'morphology', 'to', 'construct', 'the', 'larger', 'syntactic', 'context', 'independently', 'of', 'phrase', 'structure', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', 'csli', 'publications', 'ventura', 'hall', 'stanford', 'university', 'stanford', 'ca', '94305-4115', 'telephone', '650', '723-1839', 'fax', '650', '725-2166', 'http', '/', '/', 'csli-www', 'stanford', 'edu', '/', 'publications']]\n",
      "\n",
      "\n",
      "Tokens after Vectorization:\n",
      " [[78, 56, 88, 59, 69]]\n",
      "\n",
      "\n",
      "Train and Test turned into Vectors of Fixed Dimensions and Target labelled\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data are now like:\n",
      " [LabeledPoint(0.0, [0.28890474143582584,0.281496927552856,0.06667032494672905,0.08148595271266883,0.140748463776428,0.140748463776428,0.08889376659563873,0.10370939436157851,0.11852502212751831,0.16297190542533765,0.16297190542533765,0.7407813882969894,0.2296422303720667,0.10370939436157851,0.12593283601048819,0.06667032494672905,0.12593283601048819,0.140748463776428,0.059262511063759155,0.17037971930830756]), LabeledPoint(0.0, [0.3674477735111414,0.33549579320582473,0.2556158424425331,0.14378391137392488,0.14378391137392488,0.22366386213721648,0.1597599015265832,0.14378391137392488,0.1597599015265832,0.14378391137392488,0.2396398522898748,0.22366386213721648,0.20768787198455815,0.33549579320582473,0.20768787198455815,0.17573589167924153,0.11183193106860824,0.2556158424425331,0.2396398522898748,0.17573589167924153]), LabeledPoint(0.0, [0.3167659431565893,0.2463735113440139,0.1759810795314385,0.12318675567200695,0.2111772954377262,0.2111772954377262,0.08799053976571924,0.22877540339087005,0.2991678352034455,0.19357918748458236,0.2463735113440139,0.3167659431565893,0.19357918748458236,0.1759810795314385,0.15838297157829465,0.19357918748458236,0.2815697272503016,0.3167659431565893,0.15838297157829465,0.1407848636251508])]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data are now like:\n",
      " [LabeledPoint(0.0, [0.6555092709250385,0.12401526747230457,0.15944820103582016,0.07086586712703119,0.21259760138109357,0.10629880069054679,0.19488113459933576,0.10629880069054679,0.17716466781757798,0.17716466781757798,0.07086586712703119,0.42519520276218714,0.15944820103582016,0.14173173425406238,0.15944820103582016,0.19488113459933576,0.08858233390878899,0.15944820103582016,0.08858233390878899,0.14173173425406238]), LabeledPoint(0.0, [0.46862483977453506,0.13077902505335862,0.1035333948339089,0.16347378131669826,0.1035333948339089,0.1689229073605882,0.06538951252667931,0.13622815109724856,0.07083863857056925,0.190719411536148,0.08173689065834913,0.6920390075740227,0.07083863857056925,0.1035333948339089,0.07083863857056925,0.05449126043889942,0.22341416779948764,0.190719411536148,0.08173689065834913,0.1416772771411385]), LabeledPoint(0.0, [0.4447877773998133,0.22239388869990664,0.19459465261241832,0.13899618043744166,0.19459465261241832,0.3057915969623716,0.11119694434995332,0.19459465261241832,0.16679541652493,0.22239388869990664,0.25019312478739497,0.2779923608748833,0.13899618043744166,0.22239388869990664,0.11119694434995332,0.16679541652493,0.22239388869990664,0.25019312478739497,0.19459465261241832,0.19459465261241832])]\n",
      "\n",
      "\n",
      " Data Normalised and LabeledPoints Local Vector set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I need to take into consideration that I need to have the actions performed on both \n",
    "# Train and Test data at a certain point.\n",
    "# At this regards, I am not sure the best way is run the function twice,\n",
    "# But I think the possibility of splitting based on the number of subdirectories is simply \n",
    "# easy, and for this reason I sticked on that.\n",
    "\n",
    "def Before_Modeling(directory, ntest, vect_dimension):\n",
    "    print('Preparing to access to directory:', directory,'\\n')\n",
    "    store_rdd = access_and_import(directory)\n",
    "    print('\\n\\nData accessed and imported correctly\\n')\n",
    "    train,test = split_train_test(store_rdd, ntest)\n",
    "    print('\\n\\nTrain and Test split completed\\n')\n",
    "    train = remove_punct_tokenisation(train)\n",
    "    test = remove_punct_tokenisation(test)\n",
    "    print('\\n\\nPunctuation Removed and Tokenisation completed\\n')\n",
    "    train = vector_input(train, vect_dimension)\n",
    "    test = vector_input(test, vect_dimension)\n",
    "    print('\\n\\nTrain and Test turned into Vectors of Fixed Dimensions and Target labelled\\n')\n",
    "    train = Norm_and_LabPoint(train)\n",
    "    test = Norm_and_LabPoint(test)\n",
    "    print('\\n\\n Data Normalised and LabeledPoints Local Vector set\\n')\n",
    "    \n",
    "    return train,test\n",
    "\n",
    "# Try to verify if it worked ( Yes, At this point I am very afraid )\n",
    "train,test = Before_Modeling('bare', 2 , 20)\n",
    "\n",
    "\n",
    "# Ok, everything worked.\n",
    "\n",
    "# Note that I have tried to add some print comment to give me the possibility to check easily\n",
    "# at which stage the program was running.\n",
    "# However, since the functions are run both for test and train I will have the information\n",
    "# printed two times.\n",
    "# From one aspect, this is ok because it allows me to verify that the operation have worked\n",
    "# both for train and for test data.\n",
    "# On the other side, this may create to much mess when printing the function and risk to not follow\n",
    "# properly the results.\n",
    "\n",
    "# I could overcome this issue by setting in each function a parameter print_comments = True\n",
    "# and then set on the function something like\n",
    "# if print_comments = True:\n",
    "        #print('....')\n",
    "\n",
    "# However, at this stage the operation is a little bit too long and I don't have much time to do that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenced this documents to define the codes and understand the concepts:\n",
    "#'https://spark.apache.org/docs/latest/mllib-linear-methods.html'\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.classification import SVMWithSGD,SVMModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION\n",
    "\n",
    "# 1. Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8997840172786177\n",
      "Accuracy = 0.8719723183391004\n"
     ]
    }
   ],
   "source": [
    "# 2. Evaluating the model on training data\n",
    "labelsAndPreds = train.map(lambda p: (p.label, model.predict(p.features)))\n",
    "# The documentation on 'https://spark.apache.org/docs/latest/mllib-linear-methods.html' identifies\n",
    "# the error as following. However, I prefer to identify the correct answers in order to define an accuracy \n",
    "# trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(parsedData.count())\n",
    "\n",
    "correct = labelsAndPreds.filter(lambda lp: lp[0] == lp[1]).count() / float(train.count())\n",
    "print(\"Accuracy = \" + str(correct))\n",
    "\n",
    "# 3. Evaluating the model on test data\n",
    "labelsAndPreds = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "correct = labelsAndPreds.filter(lambda lp: lp[0] == lp[1]).count() / float(test.count())\n",
    "print(\"Accuracy = \" + str(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that automatically iterates the process\n",
    "\n",
    "def train_logistic(train,test):\n",
    "    model = LogisticRegressionWithLBFGS.train(train)\n",
    "    labelsAndPredsTrain = train.map(lambda p: (p.label, model.predict(p.features)))\n",
    "    correctTrain = labelsAndPredsTrain.filter(lambda lp: lp[0] == lp[1]).count() / float(train.count())\n",
    "    labelsAndPredsTest = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "    correctTest = labelsAndPredsTest.filter(lambda lp: lp[0] == lp[1]).count() / float(test.count())\n",
    "    print(\"Logistic Regression Accuracy:\\n. Training data = \" + str(correctTrain),'\\n. Test data = ',str(correctTest))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:\n",
      ". Training data = 0.8997840172786177 \n",
      ". Test data =  0.8719723183391004\n"
     ]
    }
   ],
   "source": [
    "# Attempt to run the function \n",
    "a = train_logistic(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8336933045356372\n"
     ]
    }
   ],
   "source": [
    "# SUPPORT VECTOR MACHINE\n",
    "\n",
    "# 1. Build the model\n",
    "model = SVMWithSGD.train(train)\n",
    "# 2. Evaluating the model on training data\n",
    "labelsAndPreds = train.map(lambda p: (p.label, model.predict(p.features)))\n",
    "\n",
    "correct = labelsAndPreds.filter(lambda lp: lp[0] == lp[1]).count() / float(train.count())\n",
    "print(\"Accuracy = \" + str(correct))\n",
    "\n",
    "# 3. Evaluating the model on test data\n",
    "labelsAndPreds = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "correct = labelsAndPreds.filter(lambda lp: lp[0] == lp[1]).count() / float(test.count())\n",
    "print(\"Accuracy = \" + str(correct))\n",
    "\n",
    "\n",
    "\n",
    "# I realise it matches everything as 0.\n",
    "# This may be cause data are highly unbalanced.\n",
    "# I should try to balance the data with ex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "def train_naive(train,test):\n",
    "    model = NaiveBayes.train(train)\n",
    "    labelsAndPredsTrain = train.map(lambda p: (p.label, model.predict(p.features)))\n",
    "    correctTrain = labelsAndPredsTrain.filter(lambda lp: lp[0] == lp[1]).count() / float(train.count())\n",
    "    labelsAndPredsTest = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "    correctTest = labelsAndPredsTest.filter(lambda lp: lp[0] == lp[1]).count() / float(test.count())\n",
    "    print(\"Naive Bayes Accuracy:\\n. Training data = \" + str(correctTrain),'\\n. Test data = ',str(correctTest))\n",
    "    return model\n",
    "\n",
    "# Actually I realised the structure of the codes is identical for both Logistic Regression Pipeline and Naive bayes.\n",
    "# So I could define a function splitting Model Creation and model evaluation to automate it.\n",
    "# This element will be taken into consideration in a future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy:\n",
      ". Training data = 0.8336933045356372 \n",
      ". Test data =  0.8339100346020761\n"
     ]
    }
   ],
   "source": [
    "b = train_naive(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine \n",
    "\n",
    "def train_svm(train,test):\n",
    "    model = SVMWithSGD.train(train, iterations= 50)\n",
    "    labelsAndPredsTrain = train.map(lambda p: (p.label, model.predict(p.features)))\n",
    "    correctTrain = labelsAndPredsTrain.filter(lambda lp: lp[0] == lp[1]).count() / float(train.count())\n",
    "    labelsAndPredsTest = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "    correctTest = labelsAndPredsTest.filter(lambda lp: lp[0] == lp[1]).count() / float(test.count())\n",
    "    print(\"Support Vector Machine Accuracy:\\n. Training data = \" + str(correctTrain),'\\n. Test data = ',str(correctTest))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Accuracy:\n",
      ". Training data = 0.8336933045356372 \n",
      ". Test data =  0.8339100346020761\n"
     ]
    }
   ],
   "source": [
    "c = train_svm(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that automatically iterates over the 3 models\n",
    "\n",
    "def try_models(train,test):\n",
    "    print('Training and Testing different models:\\n')\n",
    "    a = train_logistic(train,test)\n",
    "    print(' - - - - - - - - - - - - - - - - - - - \\n') # Create some space\n",
    "    b = train_naive(train,test)\n",
    "    print(' - - - - - - - - - - - - - - - - - - - \\n')\n",
    "    c = train_svm(train,test)\n",
    "    print(' - - - - - - - - - - - - - - - - - - - \\n')\n",
    "    \n",
    "    return [a,b,c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Testing different models:\n",
      "\n",
      "\n",
      "Logistic Regression Accuracy:\n",
      ". Training data = 0.8997840172786177 \n",
      ". Test data =  0.8719723183391004\n",
      " - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Naive Bayes Accuracy:\n",
      ". Training data = 0.8336933045356372 \n",
      ". Test data =  0.8339100346020761\n",
      " - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Support Vector Machine Accuracy:\n",
      ". Training data = 0.8336933045356372 \n",
      ". Test data =  0.8339100346020761\n",
      " - - - - - - - - - - - - - - - - - - - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and Test the 3 different Models \n",
    "a,b,c = try_models(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to modify the size of the Vector in order to verify the evolution of the accuracy over this modification\n",
    "# I modify the size of the test data, including only one directory, allowing more messages to be included in the training set.\n",
    "# I could also avoid to have to reimport everything, and directly start from the phase in which I vectorize the texts.\n",
    "\n",
    "\n",
    "def modify_vector_size(directory):\n",
    "    a = [100,500,1000,1500] # I could also put values lower than 20 but this would not make sense, and I believe the same\n",
    "                               # regards trying values close to that. I will start with 100 and move above.\n",
    "                               # values higher than 500/1000 but they may be highly expensive \n",
    "                               # This will create lots of mess in my notebook while running cause it will print \n",
    "    for x in a:\n",
    "        print('Attempt with VectorSize:',x)\n",
    "        train,test = Before_Modeling(directory, 1 , x) \n",
    "        a,b,c = try_models(train,test)\n",
    "        print(' - - - - - - - - - -  - - - - - - - - \\n')\n",
    "    # I am not interested into them to be returned, as I am interested in \n",
    "    # this to print the different accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt with VectorSize: 100\n",
      "Preparing to access to directory: bare \n",
      "\n",
      "Verify if the directory is properly set:\n",
      "bare/part3\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part3\n",
      "Verify if the directory is properly set:\n",
      "bare/part4\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part4\n",
      "Verify if the directory is properly set:\n",
      "bare/part5\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part5\n",
      "Verify if the directory is properly set:\n",
      "bare/part2\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part2\n",
      "Verify if the directory is properly set:\n",
      "bare/part10\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part10\n",
      "Verify if the directory is properly set:\n",
      "bare/part9\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part9\n",
      "Verify if the directory is properly set:\n",
      "bare/part7\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part7\n",
      "Verify if the directory is properly set:\n",
      "bare/part1\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part1\n",
      "Verify if the directory is properly set:\n",
      "bare/part6\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part6\n",
      "Verify if the directory is properly set:\n",
      "bare/part8\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part8\n",
      "\n",
      "\n",
      "\n",
      "Check if split function worked:\n",
      "\n",
      " [('6-266msg3', 'Subject: bisfai deadline extension !\\n\\nbisfai deadline extension ! the deadline for the bar - ilan symposium on foundations of artificial intelligence has been extended to february 27 . the conference itself will take place as scheduled , june 20-22 , in ramat - gan and jerusalem , israel . for more information contact : bisfai @ bimacs . cs . biu . ac . il daniel radzinski tovna translation machines jerusalem , israel dr @ tovna . co . il\\n')]\n",
      "\n",
      "\n",
      "Data accessed and imported correctly\n",
      "\n",
      "Num Rdd used for Train: 9\n",
      "\n",
      "\n",
      "Train and Test split completed\n",
      "\n",
      "\n",
      "\n",
      "Look of data after tokenisation and punctuation removed:\n",
      " [('6-110msg3', ['Subject', 'job', 'posting', '-', 'apple-iss', 'research', 'center', 'content', '-', 'length', '3386', 'apple-iss', 'research', 'center', 'a', 'us', '$', '10', 'million', 'joint', 'venture', 'between', 'apple', 'computer', 'inc', 'and', 'the', 'institute', 'of', 'systems', 'science', 'of', 'the', 'national', 'university', 'of', 'singapore', 'located', 'in', 'singapore', 'is', 'looking', 'for', 'a', 'senior', 'speech', 'scientist', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'in', 'computational', 'linguistics', 'including', 'natural', 'language', 'processing', 'and', '*', '*', 'english', '*', '*', 'and', '*', '*', 'chinese', '*', '*', 'statistical', 'language', 'modeling', 'knowledge', 'of', 'state-of', '-', 'the-art', 'corpus-based', 'n', '-', 'gram', 'language', 'models', 'cache', 'language', 'models', 'and', 'part-of', '-', 'speech', 'language', 'models', 'are', 'required', 'a', 'text', '-', 'to', '-', 'speech', 'project', 'leader', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'expertise', 'in', 'two', 'or', 'more', 'of', 'the', 'following', 'areas', 'computational', 'linguistics', 'including', 'natural', 'language', 'parsing', 'lexical', 'database', 'design', 'and', 'statistical', 'language', 'modeling', 'text', 'tokenization', 'and', 'normalization', 'prosodic', 'analysis', 'substantial', 'knowledge', 'of', 'the', 'phonology', 'syntax', 'and', 'semantics', 'of', 'chinese', 'is', 'required', 'knowledge', 'of', 'acoustic', 'phonetics', 'and', '/', 'or', 'speech', 'signal', 'processing', 'is', 'desirable', 'both', 'candidates', 'will', 'have', 'a', 'phd', 'with', 'at', 'least', '2', 'to', '4', 'years', 'of', 'relevant', 'work', 'experience', 'or', 'a', 'technical', 'msc', 'degree', 'with', 'at', 'least', '5', 'to', '7', 'years', 'of', 'experienc', 'e', 'very', 'strong', 'software', 'engineering', 'skills', 'including', 'design', 'and', 'implementation', 'and', 'productization', 'are', 'required', 'in', 'these', 'positions', 'knowledge', 'of', 'c', 'c', '+', '+', 'and', 'unix', 'are', 'preferred', 'a', 'unix', '&', 'c', 'programmer', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'we', 'are', 'looking', 'for', 'an', 'experienced', 'unix', '&', 'c', 'programmer', 'preferably', 'with', 'good', 'industry', 'experience', 'to', 'join', 'us', 'in', 'breaking', 'new', 'frontiers', 'strong', 'knowledge', 'of', 'unix', 'tools', 'compilers', 'linkers', 'make', 'x', '-', 'windows', 'e', '-', 'mac', 'and', 'experience', 'in', 'matlab', 'required', 'sun', 'and', 'silicon', 'graphic', 'experience', 'is', 'an', 'advantage', 'programmers', 'with', 'less', 'than', 'two', 'years', 'industry', 'experience', 'need', 'not', 'apply', 'these', 'positions', 'include', 'interaction', 'with', 'scientists', 'in', 'the', 'national', 'university', 'of', 'singapore', 'and', 'with', 'apple', \"'s\", 'speech', 'research', 'and', 'productization', 'efforts', 'located', 'in', 'cupertino', 'california', 'attendance', 'and', 'publication', 'in', 'international', 'scientific', '/', 'engineering', 'conferences', 'is', 'encouraged', 'benefits', 'include', 'an', 'internationally', 'competitive', 'salary', 'housing', 'subsidy', 'and', 'relocation', 'expenses', 'send', 'a', 'complete', 'resume', 'enclosing', 'personal', 'particulars', 'qualifications', 'experience', 'and', 'contact', 'telephone', 'number', 'to', 'mr', 'jean', '-', 'luc', 'lebrun', 'center', 'manager', 'apple', '-', 'iss', 'research', 'center', 'institute', 'of', 'systems', 'science', 'heng', 'mui', 'keng', 'terrace', 'singapore', '0511', 'tel', '65', '772-6571', 'fax', '65', '776-4005', 'email', 'jllebrun', '@', 'iss', 'nus', 'sg']), ('6-126msg1', ['Subject', 'lang', 'classification', 'grimes', 'joseph', 'e', 'and', 'barbara', 'f', 'grimes', 'ethnologue', 'language', 'family', 'index', 'pb', 'isbn', '0-88312', '-', '708', '-', '3', 'vi', '116', 'pp', '$', '14', '00', 'summer', 'institute', 'of', 'linguistics', 'this', 'companion', 'volume', 'to', 'ethnologue', 'languages', 'of', 'the', 'world', 'twelfth', 'edition', 'lists', 'language', 'families', 'of', 'the', 'world', 'with', 'sub-groups', 'shown', 'in', 'a', 'tree', 'arrangement', 'under', 'the', 'broadest', 'classification', 'of', 'language', 'family', 'the', 'language', 'family', 'index', 'facilitates', 'locating', 'language', 'names', 'in', 'the', 'ethnologue', 'making', 'the', 'data', 'there', 'more', 'accessible', 'internet', 'academic', 'books', '@', 'sil', 'org', 'languages', 'reference', 'lang', '&', 'culture', 'gregerson', 'marilyn', 'ritual', 'belief', 'and', 'kinship', 'in', 'sulawesi', 'pb', 'isbn', '0-88312', '-', '621', '-', '4', 'ix', '194', 'pp', '$', '25', '00', 'summer', 'institute', 'of', 'linguistics', 'seven', 'articles', 'discuss', 'five', 'language', 'groups', 'in', 'sulawesi', 'indonesia', 'the', 'primary', 'focus', 'is', 'on', 'cultural', 'matters', 'with', 'some', 'linguistic', 'content', 'topics', 'include', 'traditional', 'religion', 'and', 'beliefs', 'certain', 'ceremonies', 'and', 'kinship', 'internet', 'academic', 'books', '@', 'sil', 'org', 'language', 'and', 'society', 'indonesia', 'computers', '&', 'ling', 'weber', 'david', 'j', 'stephen', 'r', 'mcconnel', 'diana', 'd', 'weber', 'and', 'beth', 'j', 'bryson', 'primer', 'a', 'tool', 'for', 'developing', 'early', 'reading', 'materials', 'pb', 'isbn', '0-88313', '-', '678', '-', '8', 'xvi', '266', 'pp', '+', 'ms-dos', 'software', '$', '26', '00', 'summer', 'institute', 'of', 'linguistics', 'the', 'authors', 'present', 'a', 'computer', 'program', 'and', 'instructions', 'for', 'developing', 'reading', 'materials', 'in', 'languages', 'with', 'little', 'or', 'no', 'background', 'in', 'literacy', 'the', 'book', 'is', 'structured', 'as', 'a', 'how-to', 'manual', 'with', 'step', 'by', 'step', 'procedures', 'to', 'establish', 'an', 'appropriate', 'primer', 'sequence', 'and', 'to', 'organize', 'words', 'phrases', 'and', 'sentences', 'that', 'correlate', 'with', 'the', 'sequence', 'it', 'presupposes', 'a', 'thorough', 'knowledge', 'of', 'linguistics', 'internet', 'academic', 'books', '@', 'sil', 'org', 'literacy', 'computer'])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Look of data after tokenisation and punctuation removed:\n",
      " [('8-1064msg1', ['Subject', 're', '8', '1044', 'disc', 'grammar', 'in', 'schools', 're', 'message', 'from', 'linguist', '@', 'linguistlist', 'org', '>', '>', 'linguist', 'list', 'vol-8', '-', '1044', 'sat', 'jul', '12', '1997', 'issn', '1068-4875', '>', '>', 'subject', '8', '1044', 'disc', 'grammar', 'in', 'schools', '>', '>', 'i', 'know', 'and', 'teach', 'that', 'not', 'all', 'infinitives', 'contain', '`', 'to', \"'\", 'i', 'also', 'give', '>', 'the', 'students', 'examples', 'e', 'g', '`', 'i', 'asked', 'him', 'to', 'kindly', 'apologise', \"'\", 'where', '>', 'placing', 'the', 'adverb', 'anywhere', 'else', 'would', 'cause', 'ambiguity', '>', '>', 'jennifer', 'chew', 'an', 'example', 'i', 'once', 'concocted', 'to', 'justify', 'splitting', 'the', 'infintive', 'or', 'not', 'as', 'the', 'case', 'may', 'be', 'is', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'in', 'this', 'context', 'with', 'the', 'possible', 'exception', 'of', 'the', 'third', 'case', 'the', 'natural', 'and', 'therefore', 'near-enough', 'unambiguous', 'association', 'of', 'the', 'adverb', 'is', 'as', 'follows', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'this', 'was', 'long', 'ago', 'when', 'you', 'got', 'glared', 'at', 'for', 'splitting', 'an', 'infinitive', 'regardless', 'of', 'whether', 'it', 'was', 'the', 'only', 'place', 'to', 'put', 'the', 'adverb', 'so', 'as', 'to', 'express', 'what', 'you', 'meant', 'and', 'not', 'something', 'else', 'this', 'example', 'achieved', 'as', 'nearly', 'as', 'i', 'could', 'three', 'quite', 'distinct', 'and', 'natural', 'meanings', 'for', 'slowly', 'to', 'go', 'home', 'to', 'slowly', 'go', 'home', 'and', 'to', 'go', 'home', 'slowly', 'i', \"'\", 'm', 'not', '100', 'per', 'cent', 'happy', 'with', 'it', 'for', 'obvious', 'reasons', 'and', 'it', 'would', 'be', 'interesting', 'to', 'see', 'if', 'anyone', 'can', 'come', 'up', 'with', 'a', 'better', 'more', 'clear-cut', 'one', 'and', 'to', 'really', 'epater', 'les', 'bourgeois', 'i', 'reckon', 'you', 'could', 'even', 'make', 'a', 'case', 'for', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'the', 'implication', 'being', 'that', 'the', 'meal', 'was', 'so', 'very', 'heavy', 'that', 'the', 'walk', 'home', 'should', 'be', 'correpondingly', 'delicate', 'as', 'emphasised', 'by', 'the', 'pause', 'in', 'rhythm', 'marked', 'by', 'the', 'commas', 'ted', 'ted', 'harding', '@', 'nessie', 'mcc', 'ac', 'uk']), ('6-806msg1', ['Subject', 'swadesh', 'list', 'does', 'anyone', 'have', 'a', 'copy', 'of', 'the', 'swadesh', 'word', 'list', 'at', 'hand', 'i', 'should', 'like', 'to', 'get', 'a', 'copy', 'by', 'email', 'as', 'soon', 'as', 'is', 'practicable', 'thanks', 'in', 'advance', 'adams', 'bodomo', 'bodomo', '@', 'csli', 'stanford', 'edu'])]\n",
      "\n",
      "\n",
      "Punctuation Removed and Tokenisation completed\n",
      "\n",
      "\n",
      "\n",
      "Tokens before Vectorization:\n",
      " [['Subject', 'job', 'posting', '-', 'apple-iss', 'research', 'center', 'content', '-', 'length', '3386', 'apple-iss', 'research', 'center', 'a', 'us', '$', '10', 'million', 'joint', 'venture', 'between', 'apple', 'computer', 'inc', 'and', 'the', 'institute', 'of', 'systems', 'science', 'of', 'the', 'national', 'university', 'of', 'singapore', 'located', 'in', 'singapore', 'is', 'looking', 'for', 'a', 'senior', 'speech', 'scientist', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'in', 'computational', 'linguistics', 'including', 'natural', 'language', 'processing', 'and', '*', '*', 'english', '*', '*', 'and', '*', '*', 'chinese', '*', '*', 'statistical', 'language', 'modeling', 'knowledge', 'of', 'state-of', '-', 'the-art', 'corpus-based', 'n', '-', 'gram', 'language', 'models', 'cache', 'language', 'models', 'and', 'part-of', '-', 'speech', 'language', 'models', 'are', 'required', 'a', 'text', '-', 'to', '-', 'speech', 'project', 'leader', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'expertise', 'in', 'two', 'or', 'more', 'of', 'the', 'following', 'areas', 'computational', 'linguistics', 'including', 'natural', 'language', 'parsing', 'lexical', 'database', 'design', 'and', 'statistical', 'language', 'modeling', 'text', 'tokenization', 'and', 'normalization', 'prosodic', 'analysis', 'substantial', 'knowledge', 'of', 'the', 'phonology', 'syntax', 'and', 'semantics', 'of', 'chinese', 'is', 'required', 'knowledge', 'of', 'acoustic', 'phonetics', 'and', '/', 'or', 'speech', 'signal', 'processing', 'is', 'desirable', 'both', 'candidates', 'will', 'have', 'a', 'phd', 'with', 'at', 'least', '2', 'to', '4', 'years', 'of', 'relevant', 'work', 'experience', 'or', 'a', 'technical', 'msc', 'degree', 'with', 'at', 'least', '5', 'to', '7', 'years', 'of', 'experienc', 'e', 'very', 'strong', 'software', 'engineering', 'skills', 'including', 'design', 'and', 'implementation', 'and', 'productization', 'are', 'required', 'in', 'these', 'positions', 'knowledge', 'of', 'c', 'c', '+', '+', 'and', 'unix', 'are', 'preferred', 'a', 'unix', '&', 'c', 'programmer', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'we', 'are', 'looking', 'for', 'an', 'experienced', 'unix', '&', 'c', 'programmer', 'preferably', 'with', 'good', 'industry', 'experience', 'to', 'join', 'us', 'in', 'breaking', 'new', 'frontiers', 'strong', 'knowledge', 'of', 'unix', 'tools', 'compilers', 'linkers', 'make', 'x', '-', 'windows', 'e', '-', 'mac', 'and', 'experience', 'in', 'matlab', 'required', 'sun', 'and', 'silicon', 'graphic', 'experience', 'is', 'an', 'advantage', 'programmers', 'with', 'less', 'than', 'two', 'years', 'industry', 'experience', 'need', 'not', 'apply', 'these', 'positions', 'include', 'interaction', 'with', 'scientists', 'in', 'the', 'national', 'university', 'of', 'singapore', 'and', 'with', 'apple', \"'s\", 'speech', 'research', 'and', 'productization', 'efforts', 'located', 'in', 'cupertino', 'california', 'attendance', 'and', 'publication', 'in', 'international', 'scientific', '/', 'engineering', 'conferences', 'is', 'encouraged', 'benefits', 'include', 'an', 'internationally', 'competitive', 'salary', 'housing', 'subsidy', 'and', 'relocation', 'expenses', 'send', 'a', 'complete', 'resume', 'enclosing', 'personal', 'particulars', 'qualifications', 'experience', 'and', 'contact', 'telephone', 'number', 'to', 'mr', 'jean', '-', 'luc', 'lebrun', 'center', 'manager', 'apple', '-', 'iss', 'research', 'center', 'institute', 'of', 'systems', 'science', 'heng', 'mui', 'keng', 'terrace', 'singapore', '0511', 'tel', '65', '772-6571', 'fax', '65', '776-4005', 'email', 'jllebrun', '@', 'iss', 'nus', 'sg']]\n",
      "\n",
      "\n",
      "Tokens after Vectorization:\n",
      " [[78, 56, 88, 59, 69]]\n",
      "\n",
      "\n",
      "Tokens before Vectorization:\n",
      " [['Subject', 're', '8', '1044', 'disc', 'grammar', 'in', 'schools', 're', 'message', 'from', 'linguist', '@', 'linguistlist', 'org', '>', '>', 'linguist', 'list', 'vol-8', '-', '1044', 'sat', 'jul', '12', '1997', 'issn', '1068-4875', '>', '>', 'subject', '8', '1044', 'disc', 'grammar', 'in', 'schools', '>', '>', 'i', 'know', 'and', 'teach', 'that', 'not', 'all', 'infinitives', 'contain', '`', 'to', \"'\", 'i', 'also', 'give', '>', 'the', 'students', 'examples', 'e', 'g', '`', 'i', 'asked', 'him', 'to', 'kindly', 'apologise', \"'\", 'where', '>', 'placing', 'the', 'adverb', 'anywhere', 'else', 'would', 'cause', 'ambiguity', '>', '>', 'jennifer', 'chew', 'an', 'example', 'i', 'once', 'concocted', 'to', 'justify', 'splitting', 'the', 'infintive', 'or', 'not', 'as', 'the', 'case', 'may', 'be', 'is', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'in', 'this', 'context', 'with', 'the', 'possible', 'exception', 'of', 'the', 'third', 'case', 'the', 'natural', 'and', 'therefore', 'near-enough', 'unambiguous', 'association', 'of', 'the', 'adverb', 'is', 'as', 'follows', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'this', 'was', 'long', 'ago', 'when', 'you', 'got', 'glared', 'at', 'for', 'splitting', 'an', 'infinitive', 'regardless', 'of', 'whether', 'it', 'was', 'the', 'only', 'place', 'to', 'put', 'the', 'adverb', 'so', 'as', 'to', 'express', 'what', 'you', 'meant', 'and', 'not', 'something', 'else', 'this', 'example', 'achieved', 'as', 'nearly', 'as', 'i', 'could', 'three', 'quite', 'distinct', 'and', 'natural', 'meanings', 'for', 'slowly', 'to', 'go', 'home', 'to', 'slowly', 'go', 'home', 'and', 'to', 'go', 'home', 'slowly', 'i', \"'\", 'm', 'not', '100', 'per', 'cent', 'happy', 'with', 'it', 'for', 'obvious', 'reasons', 'and', 'it', 'would', 'be', 'interesting', 'to', 'see', 'if', 'anyone', 'can', 'come', 'up', 'with', 'a', 'better', 'more', 'clear-cut', 'one', 'and', 'to', 'really', 'epater', 'les', 'bourgeois', 'i', 'reckon', 'you', 'could', 'even', 'make', 'a', 'case', 'for', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'the', 'implication', 'being', 'that', 'the', 'meal', 'was', 'so', 'very', 'heavy', 'that', 'the', 'walk', 'home', 'should', 'be', 'correpondingly', 'delicate', 'as', 'emphasised', 'by', 'the', 'pause', 'in', 'rhythm', 'marked', 'by', 'the', 'commas', 'ted', 'ted', 'harding', '@', 'nessie', 'mcc', 'ac', 'uk']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tokens after Vectorization:\n",
      " [[78, 56, 88, 59, 69]]\n",
      "\n",
      "\n",
      "Train and Test turned into Vectors of Fixed Dimensions and Target labelled\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data are now like:\n",
      " [LabeledPoint(0.0, [0.07762601334258107,0.23287804002774323,0.0,0.029109755003467904,0.038813006671290534,0.07762601334258107,0.0,0.038813006671290534,0.029109755003467904,0.019406503335645267,0.06792276167475844,0.8635893984362144,0.048516258339113175,0.019406503335645267,0.06792276167475844,0.0,0.019406503335645267,0.06792276167475844,0.019406503335645267,0.06792276167475844,0.029109755003467904,0.029109755003467904,0.029109755003467904,0.0,0.038813006671290534,0.019406503335645267,0.048516258339113175,0.019406503335645267,0.0,0.048516258339113175,0.0,0.038813006671290534,0.019406503335645267,0.029109755003467904,0.009703251667822634,0.009703251667822634,0.0,0.029109755003467904,0.0,0.05821951000693581,0.08732926501040371,0.038813006671290534,0.0,0.009703251667822634,0.019406503335645267,0.0,0.009703251667822634,0.048516258339113175,0.048516258339113175,0.029109755003467904,0.0,0.009703251667822634,0.10673576834604898,0.048516258339113175,0.038813006671290534,0.019406503335645267,0.07762601334258107,0.009703251667822634,0.019406503335645267,0.009703251667822634,0.029109755003467904,0.048516258339113175,0.019406503335645267,0.029109755003467904,0.05821951000693581,0.07762601334258107,0.009703251667822634,0.009703251667822634,0.0,0.019406503335645267,0.1455487750173395,0.019406503335645267,0.06792276167475844,0.019406503335645267,0.019406503335645267,0.019406503335645267,0.029109755003467904,0.029109755003467904,0.029109755003467904,0.029109755003467904,0.15525202668516214,0.019406503335645267,0.038813006671290534,0.038813006671290534,0.029109755003467904,0.009703251667822634,0.048516258339113175,0.019406503335645267,0.07762601334258107,0.09703251667822635,0.0,0.038813006671290534,0.05821951000693581,0.019406503335645267,0.029109755003467904,0.038813006671290534,0.038813006671290534,0.048516258339113175,0.009703251667822634,0.05821951000693581]), LabeledPoint(0.0, [0.13315591032282686,0.2929430027102191,0.07989354619369612,0.07989354619369612,0.07989354619369612,0.026631182064565374,0.07989354619369612,0.07989354619369612,0.1065247282582615,0.0,0.026631182064565374,0.18641827445195763,0.1065247282582615,0.0,0.18641827445195763,0.0,0.05326236412913075,0.26631182064565373,0.18641827445195763,0.18641827445195763,0.07989354619369612,0.1065247282582615,0.05326236412913075,0.026631182064565374,0.0,0.05326236412913075,0.05326236412913075,0.05326236412913075,0.0,0.05326236412913075,0.026631182064565374,0.1065247282582615,0.0,0.13315591032282686,0.0,0.15978709238739225,0.13315591032282686,0.026631182064565374,0.07989354619369612,0.07989354619369612,0.18641827445195763,0.1065247282582615,0.1065247282582615,0.0,0.07989354619369612,0.0,0.026631182064565374,0.07989354619369612,0.1065247282582615,0.0,0.0,0.07989354619369612,0.18641827445195763,0.23968063858108837,0.026631182064565374,0.07989354619369612,0.0,0.026631182064565374,0.07989354619369612,0.026631182064565374,0.0,0.026631182064565374,0.13315591032282686,0.07989354619369612,0.0,0.26631182064565373,0.026631182064565374,0.026631182064565374,0.05326236412913075,0.026631182064565374,0.26631182064565373,0.0,0.026631182064565374,0.13315591032282686,0.05326236412913075,0.0,0.0,0.07989354619369612,0.0,0.0,0.213049456516523,0.026631182064565374,0.05326236412913075,0.05326236412913075,0.07989354619369612,0.026631182064565374,0.07989354619369612,0.0,0.0,0.15978709238739225,0.07989354619369612,0.0,0.026631182064565374,0.05326236412913075,0.07989354619369612,0.05326236412913075,0.0,0.026631182064565374,0.05326236412913075,0.0]), LabeledPoint(0.0, [0.08812102431690699,0.14686837386151166,0.0,0.08812102431690699,0.11749469908920933,0.0,0.08812102431690699,0.08812102431690699,0.11749469908920933,0.029373674772302332,0.0,0.35248409726762797,0.11749469908920933,0.029373674772302332,0.029373674772302332,0.0,0.17624204863381399,0.20561572340611634,0.058747349544604664,0.058747349544604664,0.029373674772302332,0.058747349544604664,0.058747349544604664,0.0,0.0,0.058747349544604664,0.029373674772302332,0.058747349544604664,0.11749469908920933,0.0,0.058747349544604664,0.029373674772302332,0.0,0.029373674772302332,0.029373674772302332,0.029373674772302332,0.029373674772302332,0.11749469908920933,0.029373674772302332,0.058747349544604664,0.264363072950721,0.17624204863381399,0.08812102431690699,0.0,0.08812102431690699,0.0,0.029373674772302332,0.14686837386151166,0.0,0.0,0.08812102431690699,0.14686837386151166,0.20561572340611634,0.08812102431690699,0.029373674772302332,0.058747349544604664,0.058747349544604664,0.11749469908920933,0.029373674772302332,0.029373674772302332,0.0,0.029373674772302332,0.11749469908920933,0.058747349544604664,0.08812102431690699,0.20561572340611634,0.0,0.029373674772302332,0.11749469908920933,0.029373674772302332,0.23498939817841866,0.0,0.0,0.08812102431690699,0.029373674772302332,0.058747349544604664,0.20561572340611634,0.058747349544604664,0.08812102431690699,0.058747349544604664,0.14686837386151166,0.0,0.029373674772302332,0.058747349544604664,0.058747349544604664,0.08812102431690699,0.0,0.058747349544604664,0.14686837386151166,0.264363072950721,0.029373674772302332,0.0,0.0,0.058747349544604664,0.14686837386151166,0.17624204863381399,0.0,0.029373674772302332,0.058747349544604664,0.029373674772302332])]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data are now like:\n",
      " [LabeledPoint(0.0, [0.16929979439493012,0.16929979439493012,0.03762217653220669,0.16929979439493012,0.018811088266103344,0.03762217653220669,0.2445441474593435,0.03762217653220669,0.0,0.0,0.0,0.03762217653220669,0.22573305919324013,0.018811088266103344,0.05643326479831003,0.05643326479831003,0.0,0.28216632399155017,0.03762217653220669,0.05643326479831003,0.018811088266103344,0.09405544133051673,0.0,0.018811088266103344,0.018811088266103344,0.03762217653220669,0.05643326479831003,0.09405544133051673,0.018811088266103344,0.0,0.13167761786272342,0.018811088266103344,0.0,0.0,0.018811088266103344,0.13167761786272342,0.0,0.05643326479831003,0.018811088266103344,0.05643326479831003,0.11286652959662007,0.05643326479831003,0.09405544133051673,0.0,0.3009774122576535,0.018811088266103344,0.018811088266103344,0.09405544133051673,0.018811088266103344,0.03762217653220669,0.0,0.0,0.18811088266103346,0.018811088266103344,0.018811088266103344,0.05643326479831003,0.11286652959662007,0.03762217653220669,0.22573305919324013,0.018811088266103344,0.05643326479831003,0.0,0.018811088266103344,0.0,0.05643326479831003,0.3574106770559636,0.03762217653220669,0.03762217653220669,0.05643326479831003,0.03762217653220669,0.07524435306441338,0.09405544133051673,0.03762217653220669,0.2445441474593435,0.15048870612882675,0.05643326479831003,0.018811088266103344,0.11286652959662007,0.0,0.07524435306441338,0.05643326479831003,0.0,0.16929979439493012,0.13167761786272342,0.05643326479831003,0.03762217653220669,0.03762217653220669,0.09405544133051673,0.0,0.2445441474593435,0.0,0.03762217653220669,0.0,0.018811088266103344,0.018811088266103344,0.05643326479831003,0.018811088266103344,0.03762217653220669,0.11286652959662007,0.05643326479831003]), LabeledPoint(0.0, [0.11396057645963795,0.0,0.0,0.0,0.11396057645963795,0.0,0.0,0.0,0.0,0.11396057645963795,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2279211529192759,0.0,0.0,0.0,0.0,0.0,0.11396057645963795,0.2279211529192759,0.0,0.0,0.0,0.0,0.0,0.0,0.11396057645963795,0.0,0.0,0.0,0.0,0.11396057645963795,0.0,0.0,0.0,0.0,0.2279211529192759,0.0,0.0,0.11396057645963795,0.3418817293789138,0.0,0.0,0.0,0.0,0.0,0.2279211529192759,0.11396057645963795,0.0,0.0,0.11396057645963795,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11396057645963795,0.11396057645963795,0.0,0.0,0.0,0.4558423058385518,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11396057645963795,0.2279211529192759,0.11396057645963795,0.0,0.0,0.0,0.0,0.0,0.11396057645963795,0.0,0.0,0.3418817293789138,0.0,0.0,0.0,0.0,0.2279211529192759,0.11396057645963795,0.0,0.0,0.2279211529192759,0.0]), LabeledPoint(0.0, [0.10879209623669538,0.1740673539787126,0.04351683849467815,0.19582577322605166,0.07615446736568676,0.05439604811834769,0.04351683849467815,0.05439604811834769,0.06527525774201723,0.05439604811834769,0.021758419247339074,0.11967130586036491,0.0,0.0,0.13055051548403446,0.06527525774201723,0.04351683849467815,0.13055051548403446,0.04351683849467815,0.0870336769893563,0.032637628871008614,0.10879209623669538,0.04351683849467815,0.04351683849467815,0.06527525774201723,0.032637628871008614,0.010879209623669537,0.31549707908641655,0.09791288661302583,0.16318814435504306,0.10879209623669538,0.021758419247339074,0.05439604811834769,0.10879209623669538,0.032637628871008614,0.010879209623669537,0.021758419247339074,0.10879209623669538,0.032637628871008614,0.06527525774201723,0.10879209623669538,0.0,0.021758419247339074,0.06527525774201723,0.0870336769893563,0.021758419247339074,0.05439604811834769,0.021758419247339074,0.10879209623669538,0.021758419247339074,0.010879209623669537,0.05439604811834769,0.10879209623669538,0.04351683849467815,0.010879209623669537,0.07615446736568676,0.09791288661302583,0.032637628871008614,0.010879209623669537,0.05439604811834769,0.010879209623669537,0.04351683849467815,0.09791288661302583,0.07615446736568676,0.141429725107704,0.10879209623669538,0.0,0.06527525774201723,0.010879209623669537,0.04351683849467815,0.18494656360238212,0.021758419247339074,0.0,0.09791288661302583,0.05439604811834769,0.0870336769893563,0.010879209623669537,0.07615446736568676,0.032637628871008614,0.021758419247339074,0.15230893473137352,0.06527525774201723,0.09791288661302583,0.2284634020970603,0.11967130586036491,0.032637628871008614,0.06527525774201723,0.010879209623669537,0.032637628871008614,0.5113228523124682,0.04351683849467815,0.021758419247339074,0.032637628871008614,0.0870336769893563,0.0870336769893563,0.0870336769893563,0.10879209623669538,0.06527525774201723,0.141429725107704,0.010879209623669537])]\n",
      "\n",
      "\n",
      " Data Normalised and LabeledPoints Local Vector set\n",
      "\n",
      "Training and Testing different models:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:\n",
      ". Training data = 0.9746543778801844 \n",
      ". Test data =  0.9550173010380623\n",
      " - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Naive Bayes Accuracy:\n",
      ". Training data = 0.8337173579109063 \n",
      ". Test data =  0.8339100346020761\n",
      " - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Support Vector Machine Accuracy:\n",
      ". Training data = 0.8337173579109063 \n",
      ". Test data =  0.8339100346020761\n",
      " - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      " - - - - - - - - - -  - - - - - - - - \n",
      "\n",
      "Attempt with VectorSize: 500\n",
      "Preparing to access to directory: bare \n",
      "\n",
      "Verify if the directory is properly set:\n",
      "bare/part3\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part3\n",
      "Verify if the directory is properly set:\n",
      "bare/part4\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part4\n",
      "Verify if the directory is properly set:\n",
      "bare/part5\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part5\n",
      "Verify if the directory is properly set:\n",
      "bare/part2\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part2\n",
      "Verify if the directory is properly set:\n",
      "bare/part10\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part10\n",
      "Verify if the directory is properly set:\n",
      "bare/part9\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part9\n",
      "Verify if the directory is properly set:\n",
      "bare/part7\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part7\n",
      "Verify if the directory is properly set:\n",
      "bare/part1\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part1\n",
      "Verify if the directory is properly set:\n",
      "bare/part6\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part6\n",
      "Verify if the directory is properly set:\n",
      "bare/part8\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part8\n",
      "\n",
      "\n",
      "\n",
      "Check if split function worked:\n",
      "\n",
      " [('6-266msg3', 'Subject: bisfai deadline extension !\\n\\nbisfai deadline extension ! the deadline for the bar - ilan symposium on foundations of artificial intelligence has been extended to february 27 . the conference itself will take place as scheduled , june 20-22 , in ramat - gan and jerusalem , israel . for more information contact : bisfai @ bimacs . cs . biu . ac . il daniel radzinski tovna translation machines jerusalem , israel dr @ tovna . co . il\\n')]\n",
      "\n",
      "\n",
      "Data accessed and imported correctly\n",
      "\n",
      "Num Rdd used for Train: 9\n",
      "\n",
      "\n",
      "Train and Test split completed\n",
      "\n",
      "\n",
      "\n",
      "Look of data after tokenisation and punctuation removed:\n",
      " [('6-110msg3', ['Subject', 'job', 'posting', '-', 'apple-iss', 'research', 'center', 'content', '-', 'length', '3386', 'apple-iss', 'research', 'center', 'a', 'us', '$', '10', 'million', 'joint', 'venture', 'between', 'apple', 'computer', 'inc', 'and', 'the', 'institute', 'of', 'systems', 'science', 'of', 'the', 'national', 'university', 'of', 'singapore', 'located', 'in', 'singapore', 'is', 'looking', 'for', 'a', 'senior', 'speech', 'scientist', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'in', 'computational', 'linguistics', 'including', 'natural', 'language', 'processing', 'and', '*', '*', 'english', '*', '*', 'and', '*', '*', 'chinese', '*', '*', 'statistical', 'language', 'modeling', 'knowledge', 'of', 'state-of', '-', 'the-art', 'corpus-based', 'n', '-', 'gram', 'language', 'models', 'cache', 'language', 'models', 'and', 'part-of', '-', 'speech', 'language', 'models', 'are', 'required', 'a', 'text', '-', 'to', '-', 'speech', 'project', 'leader', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'expertise', 'in', 'two', 'or', 'more', 'of', 'the', 'following', 'areas', 'computational', 'linguistics', 'including', 'natural', 'language', 'parsing', 'lexical', 'database', 'design', 'and', 'statistical', 'language', 'modeling', 'text', 'tokenization', 'and', 'normalization', 'prosodic', 'analysis', 'substantial', 'knowledge', 'of', 'the', 'phonology', 'syntax', 'and', 'semantics', 'of', 'chinese', 'is', 'required', 'knowledge', 'of', 'acoustic', 'phonetics', 'and', '/', 'or', 'speech', 'signal', 'processing', 'is', 'desirable', 'both', 'candidates', 'will', 'have', 'a', 'phd', 'with', 'at', 'least', '2', 'to', '4', 'years', 'of', 'relevant', 'work', 'experience', 'or', 'a', 'technical', 'msc', 'degree', 'with', 'at', 'least', '5', 'to', '7', 'years', 'of', 'experienc', 'e', 'very', 'strong', 'software', 'engineering', 'skills', 'including', 'design', 'and', 'implementation', 'and', 'productization', 'are', 'required', 'in', 'these', 'positions', 'knowledge', 'of', 'c', 'c', '+', '+', 'and', 'unix', 'are', 'preferred', 'a', 'unix', '&', 'c', 'programmer', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'we', 'are', 'looking', 'for', 'an', 'experienced', 'unix', '&', 'c', 'programmer', 'preferably', 'with', 'good', 'industry', 'experience', 'to', 'join', 'us', 'in', 'breaking', 'new', 'frontiers', 'strong', 'knowledge', 'of', 'unix', 'tools', 'compilers', 'linkers', 'make', 'x', '-', 'windows', 'e', '-', 'mac', 'and', 'experience', 'in', 'matlab', 'required', 'sun', 'and', 'silicon', 'graphic', 'experience', 'is', 'an', 'advantage', 'programmers', 'with', 'less', 'than', 'two', 'years', 'industry', 'experience', 'need', 'not', 'apply', 'these', 'positions', 'include', 'interaction', 'with', 'scientists', 'in', 'the', 'national', 'university', 'of', 'singapore', 'and', 'with', 'apple', \"'s\", 'speech', 'research', 'and', 'productization', 'efforts', 'located', 'in', 'cupertino', 'california', 'attendance', 'and', 'publication', 'in', 'international', 'scientific', '/', 'engineering', 'conferences', 'is', 'encouraged', 'benefits', 'include', 'an', 'internationally', 'competitive', 'salary', 'housing', 'subsidy', 'and', 'relocation', 'expenses', 'send', 'a', 'complete', 'resume', 'enclosing', 'personal', 'particulars', 'qualifications', 'experience', 'and', 'contact', 'telephone', 'number', 'to', 'mr', 'jean', '-', 'luc', 'lebrun', 'center', 'manager', 'apple', '-', 'iss', 'research', 'center', 'institute', 'of', 'systems', 'science', 'heng', 'mui', 'keng', 'terrace', 'singapore', '0511', 'tel', '65', '772-6571', 'fax', '65', '776-4005', 'email', 'jllebrun', '@', 'iss', 'nus', 'sg']), ('6-126msg1', ['Subject', 'lang', 'classification', 'grimes', 'joseph', 'e', 'and', 'barbara', 'f', 'grimes', 'ethnologue', 'language', 'family', 'index', 'pb', 'isbn', '0-88312', '-', '708', '-', '3', 'vi', '116', 'pp', '$', '14', '00', 'summer', 'institute', 'of', 'linguistics', 'this', 'companion', 'volume', 'to', 'ethnologue', 'languages', 'of', 'the', 'world', 'twelfth', 'edition', 'lists', 'language', 'families', 'of', 'the', 'world', 'with', 'sub-groups', 'shown', 'in', 'a', 'tree', 'arrangement', 'under', 'the', 'broadest', 'classification', 'of', 'language', 'family', 'the', 'language', 'family', 'index', 'facilitates', 'locating', 'language', 'names', 'in', 'the', 'ethnologue', 'making', 'the', 'data', 'there', 'more', 'accessible', 'internet', 'academic', 'books', '@', 'sil', 'org', 'languages', 'reference', 'lang', '&', 'culture', 'gregerson', 'marilyn', 'ritual', 'belief', 'and', 'kinship', 'in', 'sulawesi', 'pb', 'isbn', '0-88312', '-', '621', '-', '4', 'ix', '194', 'pp', '$', '25', '00', 'summer', 'institute', 'of', 'linguistics', 'seven', 'articles', 'discuss', 'five', 'language', 'groups', 'in', 'sulawesi', 'indonesia', 'the', 'primary', 'focus', 'is', 'on', 'cultural', 'matters', 'with', 'some', 'linguistic', 'content', 'topics', 'include', 'traditional', 'religion', 'and', 'beliefs', 'certain', 'ceremonies', 'and', 'kinship', 'internet', 'academic', 'books', '@', 'sil', 'org', 'language', 'and', 'society', 'indonesia', 'computers', '&', 'ling', 'weber', 'david', 'j', 'stephen', 'r', 'mcconnel', 'diana', 'd', 'weber', 'and', 'beth', 'j', 'bryson', 'primer', 'a', 'tool', 'for', 'developing', 'early', 'reading', 'materials', 'pb', 'isbn', '0-88313', '-', '678', '-', '8', 'xvi', '266', 'pp', '+', 'ms-dos', 'software', '$', '26', '00', 'summer', 'institute', 'of', 'linguistics', 'the', 'authors', 'present', 'a', 'computer', 'program', 'and', 'instructions', 'for', 'developing', 'reading', 'materials', 'in', 'languages', 'with', 'little', 'or', 'no', 'background', 'in', 'literacy', 'the', 'book', 'is', 'structured', 'as', 'a', 'how-to', 'manual', 'with', 'step', 'by', 'step', 'procedures', 'to', 'establish', 'an', 'appropriate', 'primer', 'sequence', 'and', 'to', 'organize', 'words', 'phrases', 'and', 'sentences', 'that', 'correlate', 'with', 'the', 'sequence', 'it', 'presupposes', 'a', 'thorough', 'knowledge', 'of', 'linguistics', 'internet', 'academic', 'books', '@', 'sil', 'org', 'literacy', 'computer'])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Look of data after tokenisation and punctuation removed:\n",
      " [('8-1064msg1', ['Subject', 're', '8', '1044', 'disc', 'grammar', 'in', 'schools', 're', 'message', 'from', 'linguist', '@', 'linguistlist', 'org', '>', '>', 'linguist', 'list', 'vol-8', '-', '1044', 'sat', 'jul', '12', '1997', 'issn', '1068-4875', '>', '>', 'subject', '8', '1044', 'disc', 'grammar', 'in', 'schools', '>', '>', 'i', 'know', 'and', 'teach', 'that', 'not', 'all', 'infinitives', 'contain', '`', 'to', \"'\", 'i', 'also', 'give', '>', 'the', 'students', 'examples', 'e', 'g', '`', 'i', 'asked', 'him', 'to', 'kindly', 'apologise', \"'\", 'where', '>', 'placing', 'the', 'adverb', 'anywhere', 'else', 'would', 'cause', 'ambiguity', '>', '>', 'jennifer', 'chew', 'an', 'example', 'i', 'once', 'concocted', 'to', 'justify', 'splitting', 'the', 'infintive', 'or', 'not', 'as', 'the', 'case', 'may', 'be', 'is', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'in', 'this', 'context', 'with', 'the', 'possible', 'exception', 'of', 'the', 'third', 'case', 'the', 'natural', 'and', 'therefore', 'near-enough', 'unambiguous', 'association', 'of', 'the', 'adverb', 'is', 'as', 'follows', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'this', 'was', 'long', 'ago', 'when', 'you', 'got', 'glared', 'at', 'for', 'splitting', 'an', 'infinitive', 'regardless', 'of', 'whether', 'it', 'was', 'the', 'only', 'place', 'to', 'put', 'the', 'adverb', 'so', 'as', 'to', 'express', 'what', 'you', 'meant', 'and', 'not', 'something', 'else', 'this', 'example', 'achieved', 'as', 'nearly', 'as', 'i', 'could', 'three', 'quite', 'distinct', 'and', 'natural', 'meanings', 'for', 'slowly', 'to', 'go', 'home', 'to', 'slowly', 'go', 'home', 'and', 'to', 'go', 'home', 'slowly', 'i', \"'\", 'm', 'not', '100', 'per', 'cent', 'happy', 'with', 'it', 'for', 'obvious', 'reasons', 'and', 'it', 'would', 'be', 'interesting', 'to', 'see', 'if', 'anyone', 'can', 'come', 'up', 'with', 'a', 'better', 'more', 'clear-cut', 'one', 'and', 'to', 'really', 'epater', 'les', 'bourgeois', 'i', 'reckon', 'you', 'could', 'even', 'make', 'a', 'case', 'for', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'the', 'implication', 'being', 'that', 'the', 'meal', 'was', 'so', 'very', 'heavy', 'that', 'the', 'walk', 'home', 'should', 'be', 'correpondingly', 'delicate', 'as', 'emphasised', 'by', 'the', 'pause', 'in', 'rhythm', 'marked', 'by', 'the', 'commas', 'ted', 'ted', 'harding', '@', 'nessie', 'mcc', 'ac', 'uk']), ('6-806msg1', ['Subject', 'swadesh', 'list', 'does', 'anyone', 'have', 'a', 'copy', 'of', 'the', 'swadesh', 'word', 'list', 'at', 'hand', 'i', 'should', 'like', 'to', 'get', 'a', 'copy', 'by', 'email', 'as', 'soon', 'as', 'is', 'practicable', 'thanks', 'in', 'advance', 'adams', 'bodomo', 'bodomo', '@', 'csli', 'stanford', 'edu'])]\n",
      "\n",
      "\n",
      "Punctuation Removed and Tokenisation completed\n",
      "\n",
      "\n",
      "\n",
      "Tokens before Vectorization:\n",
      " [['Subject', 'job', 'posting', '-', 'apple-iss', 'research', 'center', 'content', '-', 'length', '3386', 'apple-iss', 'research', 'center', 'a', 'us', '$', '10', 'million', 'joint', 'venture', 'between', 'apple', 'computer', 'inc', 'and', 'the', 'institute', 'of', 'systems', 'science', 'of', 'the', 'national', 'university', 'of', 'singapore', 'located', 'in', 'singapore', 'is', 'looking', 'for', 'a', 'senior', 'speech', 'scientist', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'in', 'computational', 'linguistics', 'including', 'natural', 'language', 'processing', 'and', '*', '*', 'english', '*', '*', 'and', '*', '*', 'chinese', '*', '*', 'statistical', 'language', 'modeling', 'knowledge', 'of', 'state-of', '-', 'the-art', 'corpus-based', 'n', '-', 'gram', 'language', 'models', 'cache', 'language', 'models', 'and', 'part-of', '-', 'speech', 'language', 'models', 'are', 'required', 'a', 'text', '-', 'to', '-', 'speech', 'project', 'leader', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'expertise', 'in', 'two', 'or', 'more', 'of', 'the', 'following', 'areas', 'computational', 'linguistics', 'including', 'natural', 'language', 'parsing', 'lexical', 'database', 'design', 'and', 'statistical', 'language', 'modeling', 'text', 'tokenization', 'and', 'normalization', 'prosodic', 'analysis', 'substantial', 'knowledge', 'of', 'the', 'phonology', 'syntax', 'and', 'semantics', 'of', 'chinese', 'is', 'required', 'knowledge', 'of', 'acoustic', 'phonetics', 'and', '/', 'or', 'speech', 'signal', 'processing', 'is', 'desirable', 'both', 'candidates', 'will', 'have', 'a', 'phd', 'with', 'at', 'least', '2', 'to', '4', 'years', 'of', 'relevant', 'work', 'experience', 'or', 'a', 'technical', 'msc', 'degree', 'with', 'at', 'least', '5', 'to', '7', 'years', 'of', 'experienc', 'e', 'very', 'strong', 'software', 'engineering', 'skills', 'including', 'design', 'and', 'implementation', 'and', 'productization', 'are', 'required', 'in', 'these', 'positions', 'knowledge', 'of', 'c', 'c', '+', '+', 'and', 'unix', 'are', 'preferred', 'a', 'unix', '&', 'c', 'programmer', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'we', 'are', 'looking', 'for', 'an', 'experienced', 'unix', '&', 'c', 'programmer', 'preferably', 'with', 'good', 'industry', 'experience', 'to', 'join', 'us', 'in', 'breaking', 'new', 'frontiers', 'strong', 'knowledge', 'of', 'unix', 'tools', 'compilers', 'linkers', 'make', 'x', '-', 'windows', 'e', '-', 'mac', 'and', 'experience', 'in', 'matlab', 'required', 'sun', 'and', 'silicon', 'graphic', 'experience', 'is', 'an', 'advantage', 'programmers', 'with', 'less', 'than', 'two', 'years', 'industry', 'experience', 'need', 'not', 'apply', 'these', 'positions', 'include', 'interaction', 'with', 'scientists', 'in', 'the', 'national', 'university', 'of', 'singapore', 'and', 'with', 'apple', \"'s\", 'speech', 'research', 'and', 'productization', 'efforts', 'located', 'in', 'cupertino', 'california', 'attendance', 'and', 'publication', 'in', 'international', 'scientific', '/', 'engineering', 'conferences', 'is', 'encouraged', 'benefits', 'include', 'an', 'internationally', 'competitive', 'salary', 'housing', 'subsidy', 'and', 'relocation', 'expenses', 'send', 'a', 'complete', 'resume', 'enclosing', 'personal', 'particulars', 'qualifications', 'experience', 'and', 'contact', 'telephone', 'number', 'to', 'mr', 'jean', '-', 'luc', 'lebrun', 'center', 'manager', 'apple', '-', 'iss', 'research', 'center', 'institute', 'of', 'systems', 'science', 'heng', 'mui', 'keng', 'terrace', 'singapore', '0511', 'tel', '65', '772-6571', 'fax', '65', '776-4005', 'email', 'jllebrun', '@', 'iss', 'nus', 'sg']]\n",
      "\n",
      "\n",
      "Tokens after Vectorization:\n",
      " [[78, 56, 88, 59, 69]]\n",
      "\n",
      "\n",
      "Tokens before Vectorization:\n",
      " [['Subject', 're', '8', '1044', 'disc', 'grammar', 'in', 'schools', 're', 'message', 'from', 'linguist', '@', 'linguistlist', 'org', '>', '>', 'linguist', 'list', 'vol-8', '-', '1044', 'sat', 'jul', '12', '1997', 'issn', '1068-4875', '>', '>', 'subject', '8', '1044', 'disc', 'grammar', 'in', 'schools', '>', '>', 'i', 'know', 'and', 'teach', 'that', 'not', 'all', 'infinitives', 'contain', '`', 'to', \"'\", 'i', 'also', 'give', '>', 'the', 'students', 'examples', 'e', 'g', '`', 'i', 'asked', 'him', 'to', 'kindly', 'apologise', \"'\", 'where', '>', 'placing', 'the', 'adverb', 'anywhere', 'else', 'would', 'cause', 'ambiguity', '>', '>', 'jennifer', 'chew', 'an', 'example', 'i', 'once', 'concocted', 'to', 'justify', 'splitting', 'the', 'infintive', 'or', 'not', 'as', 'the', 'case', 'may', 'be', 'is', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'in', 'this', 'context', 'with', 'the', 'possible', 'exception', 'of', 'the', 'third', 'case', 'the', 'natural', 'and', 'therefore', 'near-enough', 'unambiguous', 'association', 'of', 'the', 'adverb', 'is', 'as', 'follows', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'this', 'was', 'long', 'ago', 'when', 'you', 'got', 'glared', 'at', 'for', 'splitting', 'an', 'infinitive', 'regardless', 'of', 'whether', 'it', 'was', 'the', 'only', 'place', 'to', 'put', 'the', 'adverb', 'so', 'as', 'to', 'express', 'what', 'you', 'meant', 'and', 'not', 'something', 'else', 'this', 'example', 'achieved', 'as', 'nearly', 'as', 'i', 'could', 'three', 'quite', 'distinct', 'and', 'natural', 'meanings', 'for', 'slowly', 'to', 'go', 'home', 'to', 'slowly', 'go', 'home', 'and', 'to', 'go', 'home', 'slowly', 'i', \"'\", 'm', 'not', '100', 'per', 'cent', 'happy', 'with', 'it', 'for', 'obvious', 'reasons', 'and', 'it', 'would', 'be', 'interesting', 'to', 'see', 'if', 'anyone', 'can', 'come', 'up', 'with', 'a', 'better', 'more', 'clear-cut', 'one', 'and', 'to', 'really', 'epater', 'les', 'bourgeois', 'i', 'reckon', 'you', 'could', 'even', 'make', 'a', 'case', 'for', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'the', 'implication', 'being', 'that', 'the', 'meal', 'was', 'so', 'very', 'heavy', 'that', 'the', 'walk', 'home', 'should', 'be', 'correpondingly', 'delicate', 'as', 'emphasised', 'by', 'the', 'pause', 'in', 'rhythm', 'marked', 'by', 'the', 'commas', 'ted', 'ted', 'harding', '@', 'nessie', 'mcc', 'ac', 'uk']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tokens after Vectorization:\n",
      " [[78, 56, 88, 59, 69]]\n",
      "\n",
      "\n",
      "Train and Test turned into Vectors of Fixed Dimensions and Target labelled\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data are now like:\n",
      " [LabeledPoint(0.0, [0.0,0.01040256683797447,0.0,0.0,0.0,0.0,0.0,0.01040256683797447,0.0,0.01040256683797447,0.0,0.02080513367594894,0.0,0.0,0.031207700513923405,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01040256683797447,0.0,0.0,0.02080513367594894,0.0,0.0,0.0,0.01040256683797447,0.0,0.0,0.02080513367594894,0.0,0.0,0.0,0.0,0.031207700513923405,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.031207700513923405,0.02080513367594894,0.01040256683797447,0.0,0.01040256683797447,0.09362310154177023,0.031207700513923405,0.02080513367594894,0.0,0.04161026735189788,0.01040256683797447,0.0,0.0,0.01040256683797447,0.0,0.0,0.01040256683797447,0.01040256683797447,0.0,0.0,0.01040256683797447,0.0,0.0,0.07281796786582129,0.0,0.05201283418987235,0.0,0.0,0.01040256683797447,0.02080513367594894,0.01040256683797447,0.0,0.0,0.14563593573164257,0.01040256683797447,0.02080513367594894,0.01040256683797447,0.0,0.0,0.01040256683797447,0.0,0.06241540102784681,0.0,0.0,0.0,0.01040256683797447,0.0,0.0,0.0,0.0,0.01040256683797447,0.01040256683797447,0.01040256683797447,0.01040256683797447,0.18724620308354045,0.0,0.0,0.01040256683797447,0.07281796786582129,0.0,0.0,0.0,0.0,0.05201283418987235,0.9050233149037789,0.01040256683797447,0.02080513367594894,0.01040256683797447,0.0,0.0,0.0,0.0,0.07281796786582129,0.02080513367594894,0.01040256683797447,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01040256683797447,0.01040256683797447,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01040256683797447,0.0,0.0,0.02080513367594894,0.0,0.02080513367594894,0.0,0.0,0.02080513367594894,0.01040256683797447,0.0,0.0,0.02080513367594894,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02080513367594894,0.0,0.0,0.0,0.01040256683797447,0.0,0.0,0.01040256683797447,0.01040256683797447,0.01040256683797447,0.0,0.01040256683797447,0.01040256683797447,0.01040256683797447,0.0,0.0,0.0,0.01040256683797447,0.01040256683797447,0.01040256683797447,0.0,0.031207700513923405,0.01040256683797447,0.0,0.0,0.0,0.0,0.0,0.0,0.01040256683797447,0.0,0.0,0.0,0.0,0.02080513367594894,0.0,0.02080513367594894,0.0,0.0,0.0,0.01040256683797447,0.0,0.02080513367594894,0.01040256683797447,0.0,0.01040256683797447,0.0,0.01040256683797447,0.0,0.01040256683797447,0.0,0.02080513367594894,0.0,0.0,0.0,0.0,0.0,0.01040256683797447,0.0,0.04161026735189788,0.0,0.05201283418987235,0.0,0.0,0.04161026735189788,0.0,0.0,0.0,0.02080513367594894,0.0,0.01040256683797447,0.0,0.0,0.0,0.0,0.01040256683797447,0.01040256683797447,0.0,0.01040256683797447,0.0,0.0,0.01040256683797447,0.0,0.031207700513923405,0.0,0.0,0.0,0.0,0.0,0.01040256683797447,0.01040256683797447,0.0,0.0,0.01040256683797447,0.0,0.02080513367594894,0.05201283418987235,0.01040256683797447,0.02080513367594894,0.01040256683797447,0.0,0.01040256683797447,0.0,0.0,0.0,0.02080513367594894,0.0,0.01040256683797447,0.0,0.0,0.01040256683797447,0.0,0.01040256683797447,0.02080513367594894,0.02080513367594894,0.0,0.0,0.0,0.01040256683797447,0.02080513367594894,0.0,0.01040256683797447,0.0,0.02080513367594894,0.09362310154177023,0.0,0.02080513367594894,0.0,0.0,0.01040256683797447,0.0,0.0,0.0,0.0,0.0,0.07281796786582129,0.02080513367594894,0.0,0.0,0.031207700513923405,0.0,0.0,0.01040256683797447,0.01040256683797447,0.0,0.01040256683797447,0.0,0.02080513367594894,0.0,0.0,0.0,0.0,0.07281796786582129,0.01040256683797447,0.0,0.01040256683797447,0.02080513367594894,0.01040256683797447,0.0,0.0,0.0,0.0,0.02080513367594894,0.0,0.0,0.0,0.01040256683797447,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06241540102784681,0.08322053470379576,0.01040256683797447,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01040256683797447,0.01040256683797447,0.01040256683797447,0.0,0.0,0.01040256683797447,0.01040256683797447,0.0,0.0,0.0,0.0,0.0,0.05201283418987235,0.0,0.0,0.0,0.01040256683797447,0.06241540102784681,0.01040256683797447,0.0,0.01040256683797447,0.01040256683797447,0.0,0.0,0.0,0.0,0.01040256683797447,0.0,0.0,0.0,0.0,0.0,0.01040256683797447,0.0,0.01040256683797447,0.0,0.01040256683797447,0.0,0.02080513367594894,0.031207700513923405,0.0,0.01040256683797447,0.02080513367594894,0.031207700513923405,0.0,0.0,0.031207700513923405,0.0,0.01040256683797447,0.0,0.031207700513923405,0.0,0.0,0.0,0.0,0.01040256683797447,0.01040256683797447,0.0,0.0,0.01040256683797447,0.0,0.02080513367594894,0.0,0.0,0.0,0.01040256683797447,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.031207700513923405,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02080513367594894,0.0,0.0,0.01040256683797447,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02080513367594894,0.0,0.0,0.0,0.0,0.0,0.01040256683797447,0.0,0.04161026735189788,0.01040256683797447,0.0,0.0,0.0,0.0,0.0,0.01040256683797447,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02080513367594894,0.01040256683797447,0.01040256683797447,0.01040256683797447,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02080513367594894,0.02080513367594894,0.0,0.02080513367594894,0.01040256683797447,0.04161026735189788,0.0,0.0]), LabeledPoint(0.0, [0.0,0.0,0.03394221166510653,0.06788442333021306,0.0,0.0,0.06788442333021306,0.0,0.13576884666042613,0.0,0.0,0.0,0.0,0.0,0.1018266349953196,0.0,0.0,0.0,0.1018266349953196,0.0,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.03394221166510653,0.0,0.0,0.0,0.0,0.06788442333021306,0.0,0.1018266349953196,0.0,0.1018266349953196,0.1018266349953196,0.03394221166510653,0.03394221166510653,0.0,0.06788442333021306,0.06788442333021306,0.03394221166510653,0.0,0.03394221166510653,0.0,0.03394221166510653,0.0,0.1018266349953196,0.0,0.0,0.0,0.2036532699906392,0.16971105832553268,0.0,0.0,0.0,0.0,0.06788442333021306,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.23759548165574573,0.0,0.0,0.1018266349953196,0.03394221166510653,0.0,0.0,0.0,0.0,0.0,0.27153769332085226,0.0,0.0,0.06788442333021306,0.03394221166510653,0.0,0.0,0.0,0.0,0.0,0.1018266349953196,0.0,0.0,0.03394221166510653,0.06788442333021306,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.30547990498595884,0.0,0.0,0.03394221166510653,0.0,0.0,0.03394221166510653,0.0,0.0,0.03394221166510653,0.2036532699906392,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.03394221166510653,0.23759548165574573,0.03394221166510653,0.03394221166510653,0.0,0.03394221166510653,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.03394221166510653,0.03394221166510653,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.0,0.0,0.1018266349953196,0.03394221166510653,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06788442333021306,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.06788442333021306,0.0,0.1018266349953196,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.03394221166510653,0.03394221166510653,0.0,0.03394221166510653,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.1018266349953196,0.0,0.0,0.0,0.0,0.0,0.06788442333021306,0.03394221166510653,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.06788442333021306,0.0,0.03394221166510653,0.0,0.0,0.0,0.06788442333021306,0.13576884666042613,0.03394221166510653,0.06788442333021306,0.0,0.03394221166510653,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1018266349953196,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.1018266349953196,0.03394221166510653,0.0,0.1018266349953196,0.0,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.03394221166510653,0.0,0.0,0.06788442333021306,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.16971105832553268,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.03394221166510653,0.0,0.06788442333021306,0.0,0.03394221166510653,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.0,0.1018266349953196,0.0,0.0,0.0,0.06788442333021306,0.33942211665106536,0.03394221166510653,0.0,0.0,0.06788442333021306,0.03394221166510653,0.0,0.0,0.0,0.03394221166510653,0.06788442333021306,0.0,0.06788442333021306,0.0,0.0,0.0,0.0,0.0,0.06788442333021306,0.06788442333021306,0.0,0.03394221166510653,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.03394221166510653,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.0,0.1018266349953196,0.03394221166510653,0.03394221166510653,0.06788442333021306,0.0,0.1018266349953196,0.0,0.0,0.06788442333021306,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.03394221166510653,0.0,0.0,0.0,0.03394221166510653,0.0,0.1018266349953196,0.06788442333021306,0.0,0.03394221166510653,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.0,0.06788442333021306,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06788442333021306,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.03394221166510653,0.0,0.0,0.0,0.0,0.1018266349953196,0.03394221166510653,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.1018266349953196,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.0,0.06788442333021306,0.0,0.0,0.0,0.0,0.0,0.0,0.03394221166510653,0.0,0.0,0.0,0.03394221166510653,0.0,0.03394221166510653,0.0,0.0,0.0,0.0]), LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.03913302400974161,0.0,0.0,0.11739907202922485,0.03913302400974161,0.0,0.0,0.07826604801948323,0.11739907202922485,0.0,0.0,0.0,0.11739907202922485,0.0,0.03913302400974161,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03913302400974161,0.0,0.07826604801948323,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11739907202922485,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03913302400974161,0.0,0.0,0.0,0.0,0.2739311680681913,0.07826604801948323,0.0,0.0,0.03913302400974161,0.03913302400974161,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03913302400974161,0.0,0.0,0.03913302400974161,0.0,0.03913302400974161,0.0,0.0,0.0,0.0,0.0,0.07826604801948323,0.0,0.07826604801948323,0.03913302400974161,0.15653209603896645,0.0,0.0,0.0,0.07826604801948323,0.0,0.0,0.0,0.07826604801948323,0.03913302400974161,0.0,0.0,0.0,0.03913302400974161,0.11739907202922485,0.0,0.0,0.0,0.0,0.0,0.0,0.15653209603896645,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2739311680681913,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07826604801948323,0.03913302400974161,0.0,0.0,0.0,0.0,0.07826604801948323,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03913302400974161,0.0,0.03913302400974161,0.0,0.0,0.03913302400974161,0.07826604801948323,0.15653209603896645,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07826604801948323,0.0,0.0,0.0,0.03913302400974161,0.0,0.0,0.0,0.03913302400974161,0.03913302400974161,0.0,0.03913302400974161,0.03913302400974161,0.0,0.0,0.0,0.0,0.0,0.03913302400974161,0.03913302400974161,0.0,0.0,0.0,0.03913302400974161,0.0,0.07826604801948323,0.1956651200487081,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11739907202922485,0.0,0.0,0.07826604801948323,0.0,0.0,0.03913302400974161,0.0,0.0,0.0,0.0,0.07826604801948323,0.0,0.03913302400974161,0.0,0.0,0.11739907202922485,0.0,0.0,0.03913302400974161,0.0,0.07826604801948323,0.0,0.03913302400974161,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03913302400974161,0.0,0.11739907202922485,0.03913302400974161,0.11739907202922485,0.0,0.11739907202922485,0.0,0.0,0.0,0.0,0.0,0.0,0.15653209603896645,0.0,0.0,0.0,0.07826604801948323,0.03913302400974161,0.07826604801948323,0.0,0.0,0.0,0.0,0.0,0.0,0.11739907202922485,0.03913302400974161,0.0,0.0,0.03913302400974161,0.0,0.15653209603896645,0.0,0.0,0.0,0.0,0.0,0.0,0.07826604801948323,0.0,0.03913302400974161,0.0,0.0,0.0,0.03913302400974161,0.0,0.07826604801948323,0.0,0.07826604801948323,0.03913302400974161,0.15653209603896645,0.0,0.0,0.0,0.0,0.03913302400974161,0.0,0.0,0.03913302400974161,0.0,0.0,0.07826604801948323,0.0,0.0,0.0,0.11739907202922485,0.0,0.0,0.0,0.0,0.03913302400974161,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2739311680681913,0.0,0.0,0.0,0.03913302400974161,0.0,0.0,0.0,0.0,0.0,0.07826604801948323,0.03913302400974161,0.0,0.0,0.03913302400974161,0.0,0.0,0.0,0.03913302400974161,0.0,0.0,0.0,0.03913302400974161,0.15653209603896645,0.0,0.0,0.0,0.0,0.0,0.0,0.07826604801948323,0.0,0.0,0.0,0.03913302400974161,0.0,0.0,0.0,0.0,0.0,0.03913302400974161,0.0,0.0,0.0,0.0,0.0,0.03913302400974161,0.0,0.1956651200487081,0.0,0.0,0.0,0.0,0.03913302400974161,0.0,0.0,0.07826604801948323,0.0,0.0,0.0,0.0,0.03913302400974161,0.0,0.03913302400974161,0.0,0.03913302400974161,0.0,0.0,0.0,0.0,0.0,0.03913302400974161,0.15653209603896645,0.0,0.0,0.0,0.03913302400974161,0.03913302400974161,0.0,0.0,0.0,0.0,0.0,0.03913302400974161,0.0,0.0,0.11739907202922485,0.0,0.0,0.03913302400974161,0.0,0.07826604801948323,0.0,0.0,0.0,0.03913302400974161,0.03913302400974161,0.0,0.0,0.03913302400974161,0.0,0.0,0.0,0.0,0.03913302400974161,0.07826604801948323,0.0,0.0,0.0,0.0,0.0,0.03913302400974161,0.0,0.07826604801948323,0.0,0.0,0.03913302400974161,0.0,0.0,0.0,0.03913302400974161,0.0,0.0,0.0,0.03913302400974161,0.0,0.0,0.0,0.0,0.03913302400974161,0.07826604801948323,0.0,0.0,0.03913302400974161,0.0,0.0,0.03913302400974161,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11739907202922485,0.03913302400974161,0.0,0.0,0.0,0.03913302400974161,0.03913302400974161,0.0,0.07826604801948323,0.0,0.0,0.0,0.03913302400974161,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03913302400974161,0.0,0.03913302400974161,0.0,0.0,0.03913302400974161,0.0,0.03913302400974161,0.0,0.0,0.0,0.0,0.11739907202922485,0.0,0.0,0.0,0.03913302400974161])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Data are now like:\n",
      " [LabeledPoint(0.0, [0.0,0.0,0.0,0.08695652173913043,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2391304347826087,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.043478260869565216,0.0,0.021739130434782608,0.0,0.15217391304347827,0.0,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.021739130434782608,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.021739130434782608,0.021739130434782608,0.0,0.0,0.0,0.0,0.1956521739130435,0.0,0.0,0.0,0.043478260869565216,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.021739130434782608,0.021739130434782608,0.0,0.06521739130434782,0.0,0.21739130434782608,0.0,0.021739130434782608,0.0,0.021739130434782608,0.0,0.021739130434782608,0.06521739130434782,0.0,0.0,0.08695652173913043,0.021739130434782608,0.021739130434782608,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.043478260869565216,0.0,0.0,0.0,0.021739130434782608,0.0,0.15217391304347827,0.0,0.0,0.021739130434782608,0.043478260869565216,0.0,0.021739130434782608,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.043478260869565216,0.043478260869565216,0.0,0.0,0.0,0.06521739130434782,0.0,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.021739130434782608,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15217391304347827,0.0,0.0,0.0,0.0,0.06521739130434782,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.043478260869565216,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.043478260869565216,0.0,0.21739130434782608,0.0,0.043478260869565216,0.0,0.0,0.0,0.0,0.043478260869565216,0.0,0.021739130434782608,0.021739130434782608,0.021739130434782608,0.0,0.0,0.0,0.0,0.17391304347826086,0.021739130434782608,0.021739130434782608,0.021739130434782608,0.0,0.0,0.0,0.0,0.15217391304347827,0.0,0.0,0.021739130434782608,0.021739130434782608,0.0,0.0,0.043478260869565216,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.021739130434782608,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.043478260869565216,0.0,0.06521739130434782,0.0,0.30434782608695654,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06521739130434782,0.0,0.021739130434782608,0.0,0.0,0.021739130434782608,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.021739130434782608,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.06521739130434782,0.0,0.043478260869565216,0.0,0.0,0.043478260869565216,0.06521739130434782,0.0,0.0,0.021739130434782608,0.0,0.0,0.2391304347826087,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.021739130434782608,0.021739130434782608,0.0,0.0,0.043478260869565216,0.021739130434782608,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.32608695652173914,0.043478260869565216,0.0,0.021739130434782608,0.06521739130434782,0.0,0.0,0.0,0.0,0.0,0.08695652173913043,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.021739130434782608,0.0,0.0,0.021739130434782608,0.0,0.021739130434782608,0.021739130434782608,0.0,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.021739130434782608,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.3695652173913043,0.021739130434782608,0.0,0.0,0.0,0.06521739130434782,0.0,0.0,0.06521739130434782,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.06521739130434782,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.13043478260869565,0.021739130434782608,0.15217391304347827,0.0,0.043478260869565216,0.08695652173913043,0.0,0.0,0.2826086956521739,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.021739130434782608,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.043478260869565216,0.0,0.0,0.0,0.043478260869565216,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.021739130434782608,0.0,0.06521739130434782,0.021739130434782608,0.0,0.043478260869565216,0.0,0.0,0.043478260869565216,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.043478260869565216,0.0,0.043478260869565216,0.0,0.0,0.0,0.0,0.0,0.043478260869565216,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.043478260869565216,0.021739130434782608,0.0,0.0,0.021739130434782608,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.043478260869565216,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.0,0.021739130434782608]), LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.0,0.0,0.26490647141300877,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39735970711951313,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.26490647141300877,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26490647141300877,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26490647141300877,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.26490647141300877,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26490647141300877,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13245323570650439,0.0,0.0,0.0,0.0,0.13245323570650439,0.13245323570650439,0.0,0.0,0.0,0.0]), LabeledPoint(0.0, [0.0,0.013944140108300213,0.0,0.13944140108300213,0.027888280216600426,0.0,0.0,0.0,0.013944140108300213,0.0,0.0,0.08366484064980127,0.0,0.0,0.013944140108300213,0.0,0.0,0.0,0.013944140108300213,0.013944140108300213,0.0,0.013944140108300213,0.0,0.0,0.027888280216600426,0.013944140108300213,0.0,0.0,0.013944140108300213,0.0,0.1115531208664017,0.0,0.041832420324900636,0.027888280216600426,0.0,0.013944140108300213,0.0,0.013944140108300213,0.0,0.041832420324900636,0.013944140108300213,0.0,0.0,0.013944140108300213,0.013944140108300213,0.0,0.013944140108300213,0.013944140108300213,0.0,0.0,0.0,0.0,0.13944140108300213,0.013944140108300213,0.0,0.06972070054150106,0.1115531208664017,0.041832420324900636,0.013944140108300213,0.013944140108300213,0.0,0.027888280216600426,0.013944140108300213,0.041832420324900636,0.027888280216600426,0.013944140108300213,0.0,0.013944140108300213,0.0,0.05577656043320085,0.013944140108300213,0.0,0.0,0.05577656043320085,0.0,0.013944140108300213,0.013944140108300213,0.0,0.0,0.0,0.18127382140790277,0.013944140108300213,0.027888280216600426,0.0976089807581015,0.041832420324900636,0.0,0.013944140108300213,0.0,0.0,0.013944140108300213,0.0,0.0,0.0,0.05577656043320085,0.027888280216600426,0.0,0.0,0.0,0.0,0.0,0.0,0.16732968129960255,0.013944140108300213,0.0,0.0,0.0,0.027888280216600426,0.0,0.013944140108300213,0.0,0.0,0.05577656043320085,0.0,0.0,0.041832420324900636,0.08366484064980127,0.0,0.0,0.013944140108300213,0.06972070054150106,0.041832420324900636,0.041832420324900636,0.013944140108300213,0.0,0.0,0.027888280216600426,0.013944140108300213,0.013944140108300213,0.0,0.06972070054150106,0.013944140108300213,0.013944140108300213,0.0,0.0,0.013944140108300213,0.0,0.0,0.08366484064980127,0.0,0.013944140108300213,0.05577656043320085,0.0,0.0,0.0,0.0,0.0,0.013944140108300213,0.013944140108300213,0.041832420324900636,0.0,0.0,0.013944140108300213,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.013944140108300213,0.0,0.0,0.0,0.0,0.0,0.0,0.06972070054150106,0.0,0.013944140108300213,0.0,0.0,0.0,0.027888280216600426,0.013944140108300213,0.013944140108300213,0.0,0.027888280216600426,0.027888280216600426,0.0,0.0,0.0,0.0,0.013944140108300213,0.013944140108300213,0.013944140108300213,0.0,0.027888280216600426,0.041832420324900636,0.06972070054150106,0.0,0.013944140108300213,0.0,0.0,0.013944140108300213,0.013944140108300213,0.0,0.041832420324900636,0.013944140108300213,0.041832420324900636,0.013944140108300213,0.0,0.0,0.013944140108300213,0.0,0.0,0.041832420324900636,0.0,0.027888280216600426,0.013944140108300213,0.0,0.0,0.0,0.0,0.0,0.027888280216600426,0.041832420324900636,0.0,0.0,0.013944140108300213,0.0,0.013944140108300213,0.0,0.0,0.027888280216600426,0.041832420324900636,0.0,0.0,0.013944140108300213,0.0,0.027888280216600426,0.0,0.041832420324900636,0.0,0.027888280216600426,0.06972070054150106,0.08366484064980127,0.0,0.013944140108300213,0.0,0.06972070054150106,0.0,0.0,0.013944140108300213,0.0,0.0,0.013944140108300213,0.0,0.013944140108300213,0.0,0.0,0.027888280216600426,0.013944140108300213,0.027888280216600426,0.0,0.027888280216600426,0.0,0.0,0.0,0.0,0.0,0.0,0.027888280216600426,0.0,0.0,0.027888280216600426,0.0,0.027888280216600426,0.0,0.027888280216600426,0.027888280216600426,0.0,0.013944140108300213,0.013944140108300213,0.0,0.18127382140790277,0.0,0.013944140108300213,0.013944140108300213,0.013944140108300213,0.0,0.5995980246569091,0.041832420324900636,0.0,0.013944140108300213,0.0,0.013944140108300213,0.0,0.05577656043320085,0.013944140108300213,0.13944140108300213,0.0,0.13944140108300213,0.0,0.013944140108300213,0.013944140108300213,0.013944140108300213,0.013944140108300213,0.0,0.013944140108300213,0.0,0.013944140108300213,0.013944140108300213,0.0,0.0,0.0,0.06972070054150106,0.0,0.0,0.15338554119130235,0.013944140108300213,0.027888280216600426,0.0,0.041832420324900636,0.041832420324900636,0.013944140108300213,0.013944140108300213,0.0,0.0,0.16732968129960255,0.08366484064980127,0.027888280216600426,0.0,0.0,0.0,0.0,0.013944140108300213,0.0,0.013944140108300213,0.0,0.013944140108300213,0.027888280216600426,0.0,0.0,0.0,0.0,0.013944140108300213,0.0,0.013944140108300213,0.0,0.013944140108300213,0.013944140108300213,0.0,0.027888280216600426,0.0,0.041832420324900636,0.0,0.013944140108300213,0.0,0.0,0.0,0.013944140108300213,0.0,0.0,0.0,0.013944140108300213,0.0,0.0976089807581015,0.0,0.041832420324900636,0.0,0.0,0.195217961516203,0.013944140108300213,0.0,0.041832420324900636,0.0,0.0,0.0,0.027888280216600426,0.013944140108300213,0.013944140108300213,0.0,0.027888280216600426,0.08366484064980127,0.0,0.027888280216600426,0.0,0.027888280216600426,0.0,0.041832420324900636,0.0,0.0,0.013944140108300213,0.013944140108300213,0.027888280216600426,0.05577656043320085,0.0,0.027888280216600426,0.0,0.027888280216600426,0.0,0.0,0.041832420324900636,0.013944140108300213,0.08366484064980127,0.05577656043320085,0.013944140108300213,0.013944140108300213,0.013944140108300213,0.041832420324900636,0.05577656043320085,0.013944140108300213,0.0,0.0,0.0,0.0,0.0,0.027888280216600426,0.0,0.013944140108300213,0.0,0.0,0.041832420324900636,0.0,0.013944140108300213,0.0,0.0,0.0,0.2091621016245032,0.027888280216600426,0.0976089807581015,0.013944140108300213,0.013944140108300213,0.0,0.06972070054150106,0.013944140108300213,0.0,0.0,0.041832420324900636,0.0,0.0,0.027888280216600426,0.0,0.0,0.0,0.0,0.027888280216600426,0.013944140108300213,0.0,0.013944140108300213,0.013944140108300213,0.013944140108300213,0.013944140108300213,0.0,0.0,0.0,0.013944140108300213,0.0,0.0,0.0,0.013944140108300213,0.0,0.0,0.1115531208664017,0.013944140108300213,0.15338554119130235,0.027888280216600426,0.0,0.027888280216600426,0.0,0.0,0.0,0.013944140108300213,0.0,0.0,0.06972070054150106,0.0,0.0,0.027888280216600426,0.0,0.013944140108300213,0.0,0.0,0.0,0.0,0.08366484064980127,0.0,0.0,0.0,0.0,0.041832420324900636,0.013944140108300213,0.0,0.0,0.013944140108300213,0.013944140108300213,0.08366484064980127,0.013944140108300213,0.0,0.013944140108300213,0.0])]\n",
      "\n",
      "\n",
      " Data Normalised and LabeledPoints Local Vector set\n",
      "\n",
      "Training and Testing different models:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:\n",
      ". Training data = 1.0 \n",
      ". Test data =  0.9688581314878892\n",
      " - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Naive Bayes Accuracy:\n",
      ". Training data = 0.8529185867895546 \n",
      ". Test data =  0.8442906574394463\n",
      " - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Support Vector Machine Accuracy:\n",
      ". Training data = 0.8337173579109063 \n",
      ". Test data =  0.8339100346020761\n",
      " - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      " - - - - - - - - - -  - - - - - - - - \n",
      "\n",
      "Attempt with VectorSize: 1000\n",
      "Preparing to access to directory: bare \n",
      "\n",
      "Verify if the directory is properly set:\n",
      "bare/part3\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part3\n",
      "Verify if the directory is properly set:\n",
      "bare/part4\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part4\n",
      "Verify if the directory is properly set:\n",
      "bare/part5\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part5\n",
      "Verify if the directory is properly set:\n",
      "bare/part2\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part2\n",
      "Verify if the directory is properly set:\n",
      "bare/part10\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part10\n",
      "Verify if the directory is properly set:\n",
      "bare/part9\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part9\n",
      "Verify if the directory is properly set:\n",
      "bare/part7\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part7\n",
      "Verify if the directory is properly set:\n",
      "bare/part1\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part1\n",
      "Verify if the directory is properly set:\n",
      "bare/part6\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part6\n",
      "Verify if the directory is properly set:\n",
      "bare/part8\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part8\n",
      "\n",
      "\n",
      "\n",
      "Check if split function worked:\n",
      "\n",
      " [('6-266msg3', 'Subject: bisfai deadline extension !\\n\\nbisfai deadline extension ! the deadline for the bar - ilan symposium on foundations of artificial intelligence has been extended to february 27 . the conference itself will take place as scheduled , june 20-22 , in ramat - gan and jerusalem , israel . for more information contact : bisfai @ bimacs . cs . biu . ac . il daniel radzinski tovna translation machines jerusalem , israel dr @ tovna . co . il\\n')]\n",
      "\n",
      "\n",
      "Data accessed and imported correctly\n",
      "\n",
      "Num Rdd used for Train: 9\n",
      "\n",
      "\n",
      "Train and Test split completed\n",
      "\n",
      "\n",
      "\n",
      "Look of data after tokenisation and punctuation removed:\n",
      " [('6-110msg3', ['Subject', 'job', 'posting', '-', 'apple-iss', 'research', 'center', 'content', '-', 'length', '3386', 'apple-iss', 'research', 'center', 'a', 'us', '$', '10', 'million', 'joint', 'venture', 'between', 'apple', 'computer', 'inc', 'and', 'the', 'institute', 'of', 'systems', 'science', 'of', 'the', 'national', 'university', 'of', 'singapore', 'located', 'in', 'singapore', 'is', 'looking', 'for', 'a', 'senior', 'speech', 'scientist', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'in', 'computational', 'linguistics', 'including', 'natural', 'language', 'processing', 'and', '*', '*', 'english', '*', '*', 'and', '*', '*', 'chinese', '*', '*', 'statistical', 'language', 'modeling', 'knowledge', 'of', 'state-of', '-', 'the-art', 'corpus-based', 'n', '-', 'gram', 'language', 'models', 'cache', 'language', 'models', 'and', 'part-of', '-', 'speech', 'language', 'models', 'are', 'required', 'a', 'text', '-', 'to', '-', 'speech', 'project', 'leader', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'expertise', 'in', 'two', 'or', 'more', 'of', 'the', 'following', 'areas', 'computational', 'linguistics', 'including', 'natural', 'language', 'parsing', 'lexical', 'database', 'design', 'and', 'statistical', 'language', 'modeling', 'text', 'tokenization', 'and', 'normalization', 'prosodic', 'analysis', 'substantial', 'knowledge', 'of', 'the', 'phonology', 'syntax', 'and', 'semantics', 'of', 'chinese', 'is', 'required', 'knowledge', 'of', 'acoustic', 'phonetics', 'and', '/', 'or', 'speech', 'signal', 'processing', 'is', 'desirable', 'both', 'candidates', 'will', 'have', 'a', 'phd', 'with', 'at', 'least', '2', 'to', '4', 'years', 'of', 'relevant', 'work', 'experience', 'or', 'a', 'technical', 'msc', 'degree', 'with', 'at', 'least', '5', 'to', '7', 'years', 'of', 'experienc', 'e', 'very', 'strong', 'software', 'engineering', 'skills', 'including', 'design', 'and', 'implementation', 'and', 'productization', 'are', 'required', 'in', 'these', 'positions', 'knowledge', 'of', 'c', 'c', '+', '+', 'and', 'unix', 'are', 'preferred', 'a', 'unix', '&', 'c', 'programmer', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'we', 'are', 'looking', 'for', 'an', 'experienced', 'unix', '&', 'c', 'programmer', 'preferably', 'with', 'good', 'industry', 'experience', 'to', 'join', 'us', 'in', 'breaking', 'new', 'frontiers', 'strong', 'knowledge', 'of', 'unix', 'tools', 'compilers', 'linkers', 'make', 'x', '-', 'windows', 'e', '-', 'mac', 'and', 'experience', 'in', 'matlab', 'required', 'sun', 'and', 'silicon', 'graphic', 'experience', 'is', 'an', 'advantage', 'programmers', 'with', 'less', 'than', 'two', 'years', 'industry', 'experience', 'need', 'not', 'apply', 'these', 'positions', 'include', 'interaction', 'with', 'scientists', 'in', 'the', 'national', 'university', 'of', 'singapore', 'and', 'with', 'apple', \"'s\", 'speech', 'research', 'and', 'productization', 'efforts', 'located', 'in', 'cupertino', 'california', 'attendance', 'and', 'publication', 'in', 'international', 'scientific', '/', 'engineering', 'conferences', 'is', 'encouraged', 'benefits', 'include', 'an', 'internationally', 'competitive', 'salary', 'housing', 'subsidy', 'and', 'relocation', 'expenses', 'send', 'a', 'complete', 'resume', 'enclosing', 'personal', 'particulars', 'qualifications', 'experience', 'and', 'contact', 'telephone', 'number', 'to', 'mr', 'jean', '-', 'luc', 'lebrun', 'center', 'manager', 'apple', '-', 'iss', 'research', 'center', 'institute', 'of', 'systems', 'science', 'heng', 'mui', 'keng', 'terrace', 'singapore', '0511', 'tel', '65', '772-6571', 'fax', '65', '776-4005', 'email', 'jllebrun', '@', 'iss', 'nus', 'sg']), ('6-126msg1', ['Subject', 'lang', 'classification', 'grimes', 'joseph', 'e', 'and', 'barbara', 'f', 'grimes', 'ethnologue', 'language', 'family', 'index', 'pb', 'isbn', '0-88312', '-', '708', '-', '3', 'vi', '116', 'pp', '$', '14', '00', 'summer', 'institute', 'of', 'linguistics', 'this', 'companion', 'volume', 'to', 'ethnologue', 'languages', 'of', 'the', 'world', 'twelfth', 'edition', 'lists', 'language', 'families', 'of', 'the', 'world', 'with', 'sub-groups', 'shown', 'in', 'a', 'tree', 'arrangement', 'under', 'the', 'broadest', 'classification', 'of', 'language', 'family', 'the', 'language', 'family', 'index', 'facilitates', 'locating', 'language', 'names', 'in', 'the', 'ethnologue', 'making', 'the', 'data', 'there', 'more', 'accessible', 'internet', 'academic', 'books', '@', 'sil', 'org', 'languages', 'reference', 'lang', '&', 'culture', 'gregerson', 'marilyn', 'ritual', 'belief', 'and', 'kinship', 'in', 'sulawesi', 'pb', 'isbn', '0-88312', '-', '621', '-', '4', 'ix', '194', 'pp', '$', '25', '00', 'summer', 'institute', 'of', 'linguistics', 'seven', 'articles', 'discuss', 'five', 'language', 'groups', 'in', 'sulawesi', 'indonesia', 'the', 'primary', 'focus', 'is', 'on', 'cultural', 'matters', 'with', 'some', 'linguistic', 'content', 'topics', 'include', 'traditional', 'religion', 'and', 'beliefs', 'certain', 'ceremonies', 'and', 'kinship', 'internet', 'academic', 'books', '@', 'sil', 'org', 'language', 'and', 'society', 'indonesia', 'computers', '&', 'ling', 'weber', 'david', 'j', 'stephen', 'r', 'mcconnel', 'diana', 'd', 'weber', 'and', 'beth', 'j', 'bryson', 'primer', 'a', 'tool', 'for', 'developing', 'early', 'reading', 'materials', 'pb', 'isbn', '0-88313', '-', '678', '-', '8', 'xvi', '266', 'pp', '+', 'ms-dos', 'software', '$', '26', '00', 'summer', 'institute', 'of', 'linguistics', 'the', 'authors', 'present', 'a', 'computer', 'program', 'and', 'instructions', 'for', 'developing', 'reading', 'materials', 'in', 'languages', 'with', 'little', 'or', 'no', 'background', 'in', 'literacy', 'the', 'book', 'is', 'structured', 'as', 'a', 'how-to', 'manual', 'with', 'step', 'by', 'step', 'procedures', 'to', 'establish', 'an', 'appropriate', 'primer', 'sequence', 'and', 'to', 'organize', 'words', 'phrases', 'and', 'sentences', 'that', 'correlate', 'with', 'the', 'sequence', 'it', 'presupposes', 'a', 'thorough', 'knowledge', 'of', 'linguistics', 'internet', 'academic', 'books', '@', 'sil', 'org', 'literacy', 'computer'])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Look of data after tokenisation and punctuation removed:\n",
      " [('8-1064msg1', ['Subject', 're', '8', '1044', 'disc', 'grammar', 'in', 'schools', 're', 'message', 'from', 'linguist', '@', 'linguistlist', 'org', '>', '>', 'linguist', 'list', 'vol-8', '-', '1044', 'sat', 'jul', '12', '1997', 'issn', '1068-4875', '>', '>', 'subject', '8', '1044', 'disc', 'grammar', 'in', 'schools', '>', '>', 'i', 'know', 'and', 'teach', 'that', 'not', 'all', 'infinitives', 'contain', '`', 'to', \"'\", 'i', 'also', 'give', '>', 'the', 'students', 'examples', 'e', 'g', '`', 'i', 'asked', 'him', 'to', 'kindly', 'apologise', \"'\", 'where', '>', 'placing', 'the', 'adverb', 'anywhere', 'else', 'would', 'cause', 'ambiguity', '>', '>', 'jennifer', 'chew', 'an', 'example', 'i', 'once', 'concocted', 'to', 'justify', 'splitting', 'the', 'infintive', 'or', 'not', 'as', 'the', 'case', 'may', 'be', 'is', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'in', 'this', 'context', 'with', 'the', 'possible', 'exception', 'of', 'the', 'third', 'case', 'the', 'natural', 'and', 'therefore', 'near-enough', 'unambiguous', 'association', 'of', 'the', 'adverb', 'is', 'as', 'follows', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'this', 'was', 'long', 'ago', 'when', 'you', 'got', 'glared', 'at', 'for', 'splitting', 'an', 'infinitive', 'regardless', 'of', 'whether', 'it', 'was', 'the', 'only', 'place', 'to', 'put', 'the', 'adverb', 'so', 'as', 'to', 'express', 'what', 'you', 'meant', 'and', 'not', 'something', 'else', 'this', 'example', 'achieved', 'as', 'nearly', 'as', 'i', 'could', 'three', 'quite', 'distinct', 'and', 'natural', 'meanings', 'for', 'slowly', 'to', 'go', 'home', 'to', 'slowly', 'go', 'home', 'and', 'to', 'go', 'home', 'slowly', 'i', \"'\", 'm', 'not', '100', 'per', 'cent', 'happy', 'with', 'it', 'for', 'obvious', 'reasons', 'and', 'it', 'would', 'be', 'interesting', 'to', 'see', 'if', 'anyone', 'can', 'come', 'up', 'with', 'a', 'better', 'more', 'clear-cut', 'one', 'and', 'to', 'really', 'epater', 'les', 'bourgeois', 'i', 'reckon', 'you', 'could', 'even', 'make', 'a', 'case', 'for', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'the', 'implication', 'being', 'that', 'the', 'meal', 'was', 'so', 'very', 'heavy', 'that', 'the', 'walk', 'home', 'should', 'be', 'correpondingly', 'delicate', 'as', 'emphasised', 'by', 'the', 'pause', 'in', 'rhythm', 'marked', 'by', 'the', 'commas', 'ted', 'ted', 'harding', '@', 'nessie', 'mcc', 'ac', 'uk']), ('6-806msg1', ['Subject', 'swadesh', 'list', 'does', 'anyone', 'have', 'a', 'copy', 'of', 'the', 'swadesh', 'word', 'list', 'at', 'hand', 'i', 'should', 'like', 'to', 'get', 'a', 'copy', 'by', 'email', 'as', 'soon', 'as', 'is', 'practicable', 'thanks', 'in', 'advance', 'adams', 'bodomo', 'bodomo', '@', 'csli', 'stanford', 'edu'])]\n",
      "\n",
      "\n",
      "Punctuation Removed and Tokenisation completed\n",
      "\n",
      "\n",
      "\n",
      "Tokens before Vectorization:\n",
      " [['Subject', 'job', 'posting', '-', 'apple-iss', 'research', 'center', 'content', '-', 'length', '3386', 'apple-iss', 'research', 'center', 'a', 'us', '$', '10', 'million', 'joint', 'venture', 'between', 'apple', 'computer', 'inc', 'and', 'the', 'institute', 'of', 'systems', 'science', 'of', 'the', 'national', 'university', 'of', 'singapore', 'located', 'in', 'singapore', 'is', 'looking', 'for', 'a', 'senior', 'speech', 'scientist', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'in', 'computational', 'linguistics', 'including', 'natural', 'language', 'processing', 'and', '*', '*', 'english', '*', '*', 'and', '*', '*', 'chinese', '*', '*', 'statistical', 'language', 'modeling', 'knowledge', 'of', 'state-of', '-', 'the-art', 'corpus-based', 'n', '-', 'gram', 'language', 'models', 'cache', 'language', 'models', 'and', 'part-of', '-', 'speech', 'language', 'models', 'are', 'required', 'a', 'text', '-', 'to', '-', 'speech', 'project', 'leader', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'expertise', 'in', 'two', 'or', 'more', 'of', 'the', 'following', 'areas', 'computational', 'linguistics', 'including', 'natural', 'language', 'parsing', 'lexical', 'database', 'design', 'and', 'statistical', 'language', 'modeling', 'text', 'tokenization', 'and', 'normalization', 'prosodic', 'analysis', 'substantial', 'knowledge', 'of', 'the', 'phonology', 'syntax', 'and', 'semantics', 'of', 'chinese', 'is', 'required', 'knowledge', 'of', 'acoustic', 'phonetics', 'and', '/', 'or', 'speech', 'signal', 'processing', 'is', 'desirable', 'both', 'candidates', 'will', 'have', 'a', 'phd', 'with', 'at', 'least', '2', 'to', '4', 'years', 'of', 'relevant', 'work', 'experience', 'or', 'a', 'technical', 'msc', 'degree', 'with', 'at', 'least', '5', 'to', '7', 'years', 'of', 'experienc', 'e', 'very', 'strong', 'software', 'engineering', 'skills', 'including', 'design', 'and', 'implementation', 'and', 'productization', 'are', 'required', 'in', 'these', 'positions', 'knowledge', 'of', 'c', 'c', '+', '+', 'and', 'unix', 'are', 'preferred', 'a', 'unix', '&', 'c', 'programmer', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'we', 'are', 'looking', 'for', 'an', 'experienced', 'unix', '&', 'c', 'programmer', 'preferably', 'with', 'good', 'industry', 'experience', 'to', 'join', 'us', 'in', 'breaking', 'new', 'frontiers', 'strong', 'knowledge', 'of', 'unix', 'tools', 'compilers', 'linkers', 'make', 'x', '-', 'windows', 'e', '-', 'mac', 'and', 'experience', 'in', 'matlab', 'required', 'sun', 'and', 'silicon', 'graphic', 'experience', 'is', 'an', 'advantage', 'programmers', 'with', 'less', 'than', 'two', 'years', 'industry', 'experience', 'need', 'not', 'apply', 'these', 'positions', 'include', 'interaction', 'with', 'scientists', 'in', 'the', 'national', 'university', 'of', 'singapore', 'and', 'with', 'apple', \"'s\", 'speech', 'research', 'and', 'productization', 'efforts', 'located', 'in', 'cupertino', 'california', 'attendance', 'and', 'publication', 'in', 'international', 'scientific', '/', 'engineering', 'conferences', 'is', 'encouraged', 'benefits', 'include', 'an', 'internationally', 'competitive', 'salary', 'housing', 'subsidy', 'and', 'relocation', 'expenses', 'send', 'a', 'complete', 'resume', 'enclosing', 'personal', 'particulars', 'qualifications', 'experience', 'and', 'contact', 'telephone', 'number', 'to', 'mr', 'jean', '-', 'luc', 'lebrun', 'center', 'manager', 'apple', '-', 'iss', 'research', 'center', 'institute', 'of', 'systems', 'science', 'heng', 'mui', 'keng', 'terrace', 'singapore', '0511', 'tel', '65', '772-6571', 'fax', '65', '776-4005', 'email', 'jllebrun', '@', 'iss', 'nus', 'sg']]\n",
      "\n",
      "\n",
      "Tokens after Vectorization:\n",
      " [[78, 56, 88, 59, 69]]\n",
      "\n",
      "\n",
      "Tokens before Vectorization:\n",
      " [['Subject', 're', '8', '1044', 'disc', 'grammar', 'in', 'schools', 're', 'message', 'from', 'linguist', '@', 'linguistlist', 'org', '>', '>', 'linguist', 'list', 'vol-8', '-', '1044', 'sat', 'jul', '12', '1997', 'issn', '1068-4875', '>', '>', 'subject', '8', '1044', 'disc', 'grammar', 'in', 'schools', '>', '>', 'i', 'know', 'and', 'teach', 'that', 'not', 'all', 'infinitives', 'contain', '`', 'to', \"'\", 'i', 'also', 'give', '>', 'the', 'students', 'examples', 'e', 'g', '`', 'i', 'asked', 'him', 'to', 'kindly', 'apologise', \"'\", 'where', '>', 'placing', 'the', 'adverb', 'anywhere', 'else', 'would', 'cause', 'ambiguity', '>', '>', 'jennifer', 'chew', 'an', 'example', 'i', 'once', 'concocted', 'to', 'justify', 'splitting', 'the', 'infintive', 'or', 'not', 'as', 'the', 'case', 'may', 'be', 'is', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'in', 'this', 'context', 'with', 'the', 'possible', 'exception', 'of', 'the', 'third', 'case', 'the', 'natural', 'and', 'therefore', 'near-enough', 'unambiguous', 'association', 'of', 'the', 'adverb', 'is', 'as', 'follows', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'this', 'was', 'long', 'ago', 'when', 'you', 'got', 'glared', 'at', 'for', 'splitting', 'an', 'infinitive', 'regardless', 'of', 'whether', 'it', 'was', 'the', 'only', 'place', 'to', 'put', 'the', 'adverb', 'so', 'as', 'to', 'express', 'what', 'you', 'meant', 'and', 'not', 'something', 'else', 'this', 'example', 'achieved', 'as', 'nearly', 'as', 'i', 'could', 'three', 'quite', 'distinct', 'and', 'natural', 'meanings', 'for', 'slowly', 'to', 'go', 'home', 'to', 'slowly', 'go', 'home', 'and', 'to', 'go', 'home', 'slowly', 'i', \"'\", 'm', 'not', '100', 'per', 'cent', 'happy', 'with', 'it', 'for', 'obvious', 'reasons', 'and', 'it', 'would', 'be', 'interesting', 'to', 'see', 'if', 'anyone', 'can', 'come', 'up', 'with', 'a', 'better', 'more', 'clear-cut', 'one', 'and', 'to', 'really', 'epater', 'les', 'bourgeois', 'i', 'reckon', 'you', 'could', 'even', 'make', 'a', 'case', 'for', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'the', 'implication', 'being', 'that', 'the', 'meal', 'was', 'so', 'very', 'heavy', 'that', 'the', 'walk', 'home', 'should', 'be', 'correpondingly', 'delicate', 'as', 'emphasised', 'by', 'the', 'pause', 'in', 'rhythm', 'marked', 'by', 'the', 'commas', 'ted', 'ted', 'harding', '@', 'nessie', 'mcc', 'ac', 'uk']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tokens after Vectorization:\n",
      " [[78, 56, 88, 59, 69]]\n",
      "\n",
      "\n",
      "Train and Test turned into Vectors of Fixed Dimensions and Target labelled\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data are now like:\n",
      " [LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.010457024970856451,0.09411322473770806,0.020914049941712903,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.0,0.1463983495919903,0.0,0.020914049941712903,0.0,0.0,0.0,0.0,0.0,0.0627421498251387,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.010457024970856451,0.010457024970856451,0.1882264494754161,0.0,0.0,0.010457024970856451,0.07319917479599515,0.0,0.0,0.0,0.0,0.05228512485428225,0.9097611724645112,0.0,0.020914049941712903,0.0,0.0,0.0,0.0,0.0,0.07319917479599515,0.020914049941712903,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.020914049941712903,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.010457024970856451,0.0,0.010457024970856451,0.0,0.0,0.0,0.010457024970856451,0.0,0.010457024970856451,0.0,0.03137107491256935,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.041828099883425805,0.0,0.03137107491256935,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.020914049941712903,0.05228512485428225,0.010457024970856451,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.07319917479599515,0.0,0.020914049941712903,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.03137107491256935,0.020914049941712903,0.0,0.0,0.03137107491256935,0.0,0.0,0.010457024970856451,0.010457024970856451,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.07319917479599515,0.010457024970856451,0.0,0.010457024970856451,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0627421498251387,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.010457024970856451,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.05228512485428225,0.0,0.0,0.0,0.0,0.010457024970856451,0.010457024970856451,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.020914049941712903,0.010457024970856451,0.0,0.010457024970856451,0.020914049941712903,0.03137107491256935,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.010457024970856451,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.03137107491256935,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03137107491256935,0.0,0.010457024970856451,0.0,0.0,0.0,0.010457024970856451,0.020914049941712903,0.0,0.041828099883425805,0.010457024970856451,0.0,0.0,0.010457024970856451,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.07319917479599515,0.0,0.05228512485428225,0.0,0.0,0.010457024970856451,0.0,0.010457024970856451,0.0,0.0,0.0,0.010457024970856451,0.0,0.010457024970856451,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.010457024970856451,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.010457024970856451,0.010457024970856451,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.020914049941712903,0.0,0.0,0.0,0.010457024970856451,0.0,0.020914049941712903,0.010457024970856451,0.0,0.010457024970856451,0.0,0.010457024970856451,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.041828099883425805,0.0,0.0,0.0,0.010457024970856451,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.03137107491256935,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.020914049941712903,0.0,0.010457024970856451,0.0,0.0,0.010457024970856451,0.0,0.010457024970856451,0.0,0.020914049941712903,0.0,0.0,0.0,0.010457024970856451,0.020914049941712903,0.0,0.0,0.0,0.020914049941712903,0.020914049941712903,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.041828099883425805,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08365619976685161,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.05228512485428225,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.020914049941712903,0.0,0.0,0.0,0.0,0.0,0.0,0.03137107491256935,0.0,0.0,0.0,0.03137107491256935,0.0,0.0,0.0,0.0,0.010457024970856451,0.010457024970856451,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03137107491256935,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.041828099883425805,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010457024970856451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020914049941712903,0.0,0.020914049941712903,0.0,0.041828099883425805,0.0,0.0]), LabeledPoint(0.0, [0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.10527936095153946,0.0,0.0,0.0,0.10527936095153946,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07018624063435965,0.0,0.10527936095153946,0.0,0.0,0.0,0.0,0.07018624063435965,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.21055872190307892,0.1403724812687193,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.24565184222025876,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.10527936095153946,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.3158380828546184,0.0,0.0,0.03509312031717982,0.0,0.0,0.03509312031717982,0.0,0.0,0.03509312031717982,0.21055872190307892,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.24565184222025876,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.03509312031717982,0.03509312031717982,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07018624063435965,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.10527936095153946,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07018624063435965,0.0,0.0,0.0,0.0,0.0,0.0,0.10527936095153946,0.03509312031717982,0.03509312031717982,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10527936095153946,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.10527936095153946,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1754656015858991,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.03509312031717982,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07018624063435965,0.3509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.10527936095153946,0.03509312031717982,0.03509312031717982,0.07018624063435965,0.0,0.03509312031717982,0.0,0.0,0.07018624063435965,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.10527936095153946,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07018624063435965,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.03509312031717982,0.0,0.0,0.0,0.0,0.10527936095153946,0.03509312031717982,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.10527936095153946,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07018624063435965,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.03509312031717982,0.0,0.0,0.07018624063435965,0.0,0.10527936095153946,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.03509312031717982,0.0,0.0,0.0,0.0,0.07018624063435965,0.0,0.03509312031717982,0.0,0.0,0.10527936095153946,0.03509312031717982,0.03509312031717982,0.0,0.0,0.07018624063435965,0.03509312031717982,0.0,0.0,0.0,0.03509312031717982,0.0,0.10527936095153946,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24565184222025876,0.0,0.0,0.10527936095153946,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.07018624063435965,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.03509312031717982,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.03509312031717982,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10527936095153946,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07018624063435965,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.03509312031717982,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.10527936095153946,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.07018624063435965,0.03509312031717982,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10527936095153946,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.03509312031717982,0.0,0.0,0.07018624063435965,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.03509312031717982,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10527936095153946,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.07018624063435965,0.03509312031717982,0.0,0.0,0.0,0.03509312031717982,0.07018624063435965,0.0,0.07018624063435965,0.0,0.0,0.0,0.0,0.0,0.07018624063435965,0.07018624063435965,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07018624063435965,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.03509312031717982,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.07018624063435965,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.03509312031717982,0.0,0.0,0.0,0.0,0.0,0.0]), LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.12116885875830291,0.0,0.0,0.0,0.12116885875830291,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28272733710270676,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08077923917220194,0.0,0.08077923917220194,0.0,0.16155847834440387,0.0,0.0,0.0,0.08077923917220194,0.0,0.0,0.0,0.08077923917220194,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16155847834440387,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24233771751660582,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08077923917220194,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.04038961958610097,0.08077923917220194,0.16155847834440387,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08077923917220194,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.08077923917220194,0.20194809793050483,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.12116885875830291,0.0,0.0,0.04038961958610097,0.0,0.04038961958610097,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12116885875830291,0.04038961958610097,0.12116885875830291,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16155847834440387,0.0,0.0,0.0,0.08077923917220194,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12116885875830291,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.04038961958610097,0.12116885875830291,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.12116885875830291,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28272733710270676,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08077923917220194,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.20194809793050483,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08077923917220194,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.16155847834440387,0.0,0.0,0.0,0.04038961958610097,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.08077923917220194,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.08077923917220194,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.08077923917220194,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.08077923917220194,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.04038961958610097,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.12116885875830291,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12116885875830291,0.0,0.0,0.0,0.08077923917220194,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08077923917220194,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12116885875830291,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.08077923917220194,0.0,0.0,0.04038961958610097,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.12116885875830291,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.04038961958610097,0.04038961958610097,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12116885875830291,0.0,0.0,0.08077923917220194,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.12116885875830291,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.08077923917220194,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.16155847834440387,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.04038961958610097,0.0,0.0,0.0,0.04038961958610097,0.0,0.04038961958610097,0.0,0.08077923917220194,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08077923917220194,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16155847834440387,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.08077923917220194,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.04038961958610097,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08077923917220194,0.04038961958610097,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04038961958610097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Data are now like:\n",
      " [LabeledPoint(0.0, [0.0,0.0,0.0,0.06661733875264914,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22205779584216379,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.04441155916843276,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1998520162579474,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.22205779584216379,0.0,0.02220577958421638,0.0,0.02220577958421638,0.0,0.0,0.06661733875264914,0.0,0.0,0.0,0.02220577958421638,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04441155916843276,0.0,0.0,0.0,0.02220577958421638,0.0,0.15544045708951465,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.06661733875264914,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06661733875264914,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22205779584216379,0.0,0.04441155916843276,0.0,0.0,0.0,0.0,0.04441155916843276,0.0,0.02220577958421638,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.15544045708951465,0.02220577958421638,0.02220577958421638,0.02220577958421638,0.0,0.0,0.0,0.0,0.15544045708951465,0.0,0.0,0.02220577958421638,0.02220577958421638,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04441155916843276,0.0,0.06661733875264914,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06661733875264914,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04441155916843276,0.0,0.0,0.0,0.0,0.0,0.0,0.24426357542638016,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.02220577958421638,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3330866937632457,0.02220577958421638,0.0,0.0,0.06661733875264914,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.37749825293167844,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.06661733875264914,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.06661733875264914,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.02220577958421638,0.06661733875264914,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04441155916843276,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.06661733875264914,0.02220577958421638,0.0,0.02220577958421638,0.0,0.0,0.04441155916843276,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.04441155916843276,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.04441155916843276,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15544045708951465,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.02220577958421638,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04441155916843276,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.02220577958421638,0.0,0.0,0.06661733875264914,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.08882311833686551,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.04441155916843276,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.04441155916843276,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15544045708951465,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.04441155916843276,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.3108809141790293,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.02220577958421638,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.02220577958421638,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.06661733875264914,0.0,0.04441155916843276,0.0,0.0,0.0,0.06661733875264914,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04441155916843276,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.02220577958421638,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.08882311833686551,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.02220577958421638,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.02220577958421638,0.0,0.0,0.0,0.04441155916843276,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.13323467750529827,0.0,0.15544045708951465,0.0,0.02220577958421638,0.02220577958421638,0.0,0.0,0.26646935501059654,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04441155916843276,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04441155916843276,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04441155916843276,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.02220577958421638,0.0,0.0,0.02220577958421638,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02220577958421638,0.0,0.0,0.0,0.0,0.0,0.0]), LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27472112789737807,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27472112789737807,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27472112789737807,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27472112789737807,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27472112789737807,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27472112789737807,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27472112789737807,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13736056394868904,0.0,0.0,0.0,0.0]), LabeledPoint(0.0, [0.0,0.0,0.0,0.043652817716625,0.02910187847775,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.014550939238875,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.014550939238875,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.130958453149875,0.014550939238875,0.0,0.072754696194375,0.0,0.014550939238875,0.0,0.0,0.0,0.014550939238875,0.0,0.043652817716625,0.02910187847775,0.0,0.0,0.014550939238875,0.0,0.0582037569555,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.189162210105375,0.0,0.0,0.02910187847775,0.043652817716625,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.160060331627625,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0582037569555,0.0,0.0,0.0,0.02910187847775,0.0,0.0,0.014550939238875,0.072754696194375,0.043652817716625,0.02910187847775,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.072754696194375,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.043652817716625,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.014550939238875,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.043652817716625,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.014550939238875,0.02910187847775,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.014550939238875,0.0,0.0,0.0,0.072754696194375,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.014550939238875,0.02910187847775,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.02910187847775,0.0,0.02910187847775,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.043652817716625,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.02910187847775,0.043652817716625,0.0,0.0,0.0,0.0,0.0,0.0,0.043652817716625,0.0,0.02910187847775,0.014550939238875,0.014550939238875,0.0,0.014550939238875,0.0,0.043652817716625,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.02910187847775,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.014550939238875,0.0,0.625690387271625,0.02910187847775,0.0,0.014550939238875,0.0,0.014550939238875,0.0,0.043652817716625,0.0,0.14550939238875,0.0,0.043652817716625,0.0,0.014550939238875,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.160060331627625,0.014550939238875,0.0,0.0,0.02910187847775,0.043652817716625,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.02910187847775,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.02910187847775,0.0,0.043652817716625,0.0,0.014550939238875,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.014550939238875,0.0,0.101856574672125,0.0,0.043652817716625,0.0,0.0,0.0582037569555,0.014550939238875,0.0,0.043652817716625,0.0,0.0,0.0,0.02910187847775,0.014550939238875,0.014550939238875,0.0,0.02910187847775,0.0,0.0,0.02910187847775,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.02910187847775,0.043652817716625,0.0,0.02910187847775,0.0,0.0,0.0,0.0,0.043652817716625,0.0,0.043652817716625,0.0,0.0,0.0,0.0,0.02910187847775,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.014550939238875,0.0,0.0,0.0,0.218264088583125,0.0,0.101856574672125,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.043652817716625,0.0,0.0,0.02910187847775,0.0,0.0,0.0,0.0,0.014550939238875,0.014550939238875,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08730563543325,0.014550939238875,0.0,0.014550939238875,0.0,0.0,0.014550939238875,0.0,0.101856574672125,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.072754696194375,0.0,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.014550939238875,0.0,0.0,0.0,0.0,0.02910187847775,0.014550939238875,0.0,0.0,0.0,0.0,0.116407513911,0.0,0.02910187847775,0.02910187847775,0.0,0.014550939238875,0.0,0.0,0.0,0.043652817716625,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.014550939238875,0.014550939238875,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.116407513911,0.02910187847775,0.014550939238875,0.014550939238875,0.0,0.014550939238875,0.014550939238875,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.043652817716625,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.02910187847775,0.072754696194375,0.0,0.0,0.014550939238875,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.02910187847775,0.02910187847775,0.0,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.043652817716625,0.0582037569555,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.02910187847775,0.014550939238875,0.014550939238875,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.014550939238875,0.0,0.0,0.08730563543325,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.0,0.0,0.0,0.0,0.02910187847775,0.014550939238875,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.02910187847775,0.043652817716625,0.0,0.0,0.014550939238875,0.0,0.0,0.014550939238875,0.0,0.0,0.043652817716625,0.0,0.014550939238875,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.02910187847775,0.0,0.0,0.0,0.0,0.0582037569555,0.072754696194375,0.0,0.0,0.0,0.02910187847775,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.014550939238875,0.0,0.0,0.02910187847775,0.0,0.0,0.0,0.02910187847775,0.0,0.0,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.02910187847775,0.0,0.0,0.0,0.014550939238875,0.0,0.189162210105375,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.014550939238875,0.0,0.0,0.101856574672125,0.0,0.0,0.014550939238875,0.0,0.014550939238875,0.0,0.014550939238875,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0582037569555,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.014550939238875,0.0,0.0,0.014550939238875,0.0,0.0,0.1746112708665,0.08730563543325,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.014550939238875,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08730563543325,0.0,0.0,0.0,0.014550939238875,0.0,0.043652817716625,0.0,0.0,0.014550939238875,0.014550939238875,0.0,0.014550939238875,0.0,0.0,0.0,0.02910187847775,0.0,0.0,0.0,0.014550939238875,0.043652817716625,0.0582037569555,0.014550939238875,0.014550939238875,0.014550939238875,0.014550939238875,0.043652817716625,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.014550939238875,0.0,0.0,0.02910187847775,0.0,0.0,0.0,0.0,0.0,0.0,0.02910187847775,0.0,0.014550939238875,0.014550939238875,0.0,0.072754696194375,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.014550939238875,0.014550939238875,0.014550939238875,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.0,0.0,0.0,0.0,0.116407513911,0.014550939238875,0.160060331627625,0.02910187847775,0.0,0.0,0.0,0.0,0.0,0.014550939238875,0.0,0.0,0.072754696194375,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0582037569555,0.0,0.0,0.0,0.0,0.043652817716625,0.014550939238875,0.0,0.0,0.014550939238875,0.014550939238875,0.0,0.0,0.0,0.0,0.0])]\n",
      "\n",
      "\n",
      " Data Normalised and LabeledPoints Local Vector set\n",
      "\n",
      "Training and Testing different models:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:\n",
      ". Training data = 1.0 \n",
      ". Test data =  0.9688581314878892\n",
      " - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Naive Bayes Accuracy:\n",
      ". Training data = 0.8870967741935484 \n",
      ". Test data =  0.8615916955017301\n",
      " - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Support Vector Machine Accuracy:\n",
      ". Training data = 0.8337173579109063 \n",
      ". Test data =  0.8339100346020761\n",
      " - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      " - - - - - - - - - -  - - - - - - - - \n",
      "\n",
      "Attempt with VectorSize: 1500\n",
      "Preparing to access to directory: bare \n",
      "\n",
      "Verify if the directory is properly set:\n",
      "bare/part3\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part3\n",
      "Verify if the directory is properly set:\n",
      "bare/part4\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part4\n",
      "Verify if the directory is properly set:\n",
      "bare/part5\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part5\n",
      "Verify if the directory is properly set:\n",
      "bare/part2\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part2\n",
      "Verify if the directory is properly set:\n",
      "bare/part10\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part10\n",
      "Verify if the directory is properly set:\n",
      "bare/part9\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part9\n",
      "Verify if the directory is properly set:\n",
      "bare/part7\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part7\n",
      "Verify if the directory is properly set:\n",
      "bare/part1\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part1\n",
      "Verify if the directory is properly set:\n",
      "bare/part6\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part6\n",
      "Verify if the directory is properly set:\n",
      "bare/part8\n",
      "/Users/simonezanetti/Desktop/lingspam_public/bare/part8\n",
      "\n",
      "\n",
      "\n",
      "Check if split function worked:\n",
      "\n",
      " [('6-266msg3', 'Subject: bisfai deadline extension !\\n\\nbisfai deadline extension ! the deadline for the bar - ilan symposium on foundations of artificial intelligence has been extended to february 27 . the conference itself will take place as scheduled , june 20-22 , in ramat - gan and jerusalem , israel . for more information contact : bisfai @ bimacs . cs . biu . ac . il daniel radzinski tovna translation machines jerusalem , israel dr @ tovna . co . il\\n')]\n",
      "\n",
      "\n",
      "Data accessed and imported correctly\n",
      "\n",
      "Num Rdd used for Train: 9\n",
      "\n",
      "\n",
      "Train and Test split completed\n",
      "\n",
      "\n",
      "\n",
      "Look of data after tokenisation and punctuation removed:\n",
      " [('6-110msg3', ['Subject', 'job', 'posting', '-', 'apple-iss', 'research', 'center', 'content', '-', 'length', '3386', 'apple-iss', 'research', 'center', 'a', 'us', '$', '10', 'million', 'joint', 'venture', 'between', 'apple', 'computer', 'inc', 'and', 'the', 'institute', 'of', 'systems', 'science', 'of', 'the', 'national', 'university', 'of', 'singapore', 'located', 'in', 'singapore', 'is', 'looking', 'for', 'a', 'senior', 'speech', 'scientist', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'in', 'computational', 'linguistics', 'including', 'natural', 'language', 'processing', 'and', '*', '*', 'english', '*', '*', 'and', '*', '*', 'chinese', '*', '*', 'statistical', 'language', 'modeling', 'knowledge', 'of', 'state-of', '-', 'the-art', 'corpus-based', 'n', '-', 'gram', 'language', 'models', 'cache', 'language', 'models', 'and', 'part-of', '-', 'speech', 'language', 'models', 'are', 'required', 'a', 'text', '-', 'to', '-', 'speech', 'project', 'leader', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'expertise', 'in', 'two', 'or', 'more', 'of', 'the', 'following', 'areas', 'computational', 'linguistics', 'including', 'natural', 'language', 'parsing', 'lexical', 'database', 'design', 'and', 'statistical', 'language', 'modeling', 'text', 'tokenization', 'and', 'normalization', 'prosodic', 'analysis', 'substantial', 'knowledge', 'of', 'the', 'phonology', 'syntax', 'and', 'semantics', 'of', 'chinese', 'is', 'required', 'knowledge', 'of', 'acoustic', 'phonetics', 'and', '/', 'or', 'speech', 'signal', 'processing', 'is', 'desirable', 'both', 'candidates', 'will', 'have', 'a', 'phd', 'with', 'at', 'least', '2', 'to', '4', 'years', 'of', 'relevant', 'work', 'experience', 'or', 'a', 'technical', 'msc', 'degree', 'with', 'at', 'least', '5', 'to', '7', 'years', 'of', 'experienc', 'e', 'very', 'strong', 'software', 'engineering', 'skills', 'including', 'design', 'and', 'implementation', 'and', 'productization', 'are', 'required', 'in', 'these', 'positions', 'knowledge', 'of', 'c', 'c', '+', '+', 'and', 'unix', 'are', 'preferred', 'a', 'unix', '&', 'c', 'programmer', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'we', 'are', 'looking', 'for', 'an', 'experienced', 'unix', '&', 'c', 'programmer', 'preferably', 'with', 'good', 'industry', 'experience', 'to', 'join', 'us', 'in', 'breaking', 'new', 'frontiers', 'strong', 'knowledge', 'of', 'unix', 'tools', 'compilers', 'linkers', 'make', 'x', '-', 'windows', 'e', '-', 'mac', 'and', 'experience', 'in', 'matlab', 'required', 'sun', 'and', 'silicon', 'graphic', 'experience', 'is', 'an', 'advantage', 'programmers', 'with', 'less', 'than', 'two', 'years', 'industry', 'experience', 'need', 'not', 'apply', 'these', 'positions', 'include', 'interaction', 'with', 'scientists', 'in', 'the', 'national', 'university', 'of', 'singapore', 'and', 'with', 'apple', \"'s\", 'speech', 'research', 'and', 'productization', 'efforts', 'located', 'in', 'cupertino', 'california', 'attendance', 'and', 'publication', 'in', 'international', 'scientific', '/', 'engineering', 'conferences', 'is', 'encouraged', 'benefits', 'include', 'an', 'internationally', 'competitive', 'salary', 'housing', 'subsidy', 'and', 'relocation', 'expenses', 'send', 'a', 'complete', 'resume', 'enclosing', 'personal', 'particulars', 'qualifications', 'experience', 'and', 'contact', 'telephone', 'number', 'to', 'mr', 'jean', '-', 'luc', 'lebrun', 'center', 'manager', 'apple', '-', 'iss', 'research', 'center', 'institute', 'of', 'systems', 'science', 'heng', 'mui', 'keng', 'terrace', 'singapore', '0511', 'tel', '65', '772-6571', 'fax', '65', '776-4005', 'email', 'jllebrun', '@', 'iss', 'nus', 'sg']), ('6-126msg1', ['Subject', 'lang', 'classification', 'grimes', 'joseph', 'e', 'and', 'barbara', 'f', 'grimes', 'ethnologue', 'language', 'family', 'index', 'pb', 'isbn', '0-88312', '-', '708', '-', '3', 'vi', '116', 'pp', '$', '14', '00', 'summer', 'institute', 'of', 'linguistics', 'this', 'companion', 'volume', 'to', 'ethnologue', 'languages', 'of', 'the', 'world', 'twelfth', 'edition', 'lists', 'language', 'families', 'of', 'the', 'world', 'with', 'sub-groups', 'shown', 'in', 'a', 'tree', 'arrangement', 'under', 'the', 'broadest', 'classification', 'of', 'language', 'family', 'the', 'language', 'family', 'index', 'facilitates', 'locating', 'language', 'names', 'in', 'the', 'ethnologue', 'making', 'the', 'data', 'there', 'more', 'accessible', 'internet', 'academic', 'books', '@', 'sil', 'org', 'languages', 'reference', 'lang', '&', 'culture', 'gregerson', 'marilyn', 'ritual', 'belief', 'and', 'kinship', 'in', 'sulawesi', 'pb', 'isbn', '0-88312', '-', '621', '-', '4', 'ix', '194', 'pp', '$', '25', '00', 'summer', 'institute', 'of', 'linguistics', 'seven', 'articles', 'discuss', 'five', 'language', 'groups', 'in', 'sulawesi', 'indonesia', 'the', 'primary', 'focus', 'is', 'on', 'cultural', 'matters', 'with', 'some', 'linguistic', 'content', 'topics', 'include', 'traditional', 'religion', 'and', 'beliefs', 'certain', 'ceremonies', 'and', 'kinship', 'internet', 'academic', 'books', '@', 'sil', 'org', 'language', 'and', 'society', 'indonesia', 'computers', '&', 'ling', 'weber', 'david', 'j', 'stephen', 'r', 'mcconnel', 'diana', 'd', 'weber', 'and', 'beth', 'j', 'bryson', 'primer', 'a', 'tool', 'for', 'developing', 'early', 'reading', 'materials', 'pb', 'isbn', '0-88313', '-', '678', '-', '8', 'xvi', '266', 'pp', '+', 'ms-dos', 'software', '$', '26', '00', 'summer', 'institute', 'of', 'linguistics', 'the', 'authors', 'present', 'a', 'computer', 'program', 'and', 'instructions', 'for', 'developing', 'reading', 'materials', 'in', 'languages', 'with', 'little', 'or', 'no', 'background', 'in', 'literacy', 'the', 'book', 'is', 'structured', 'as', 'a', 'how-to', 'manual', 'with', 'step', 'by', 'step', 'procedures', 'to', 'establish', 'an', 'appropriate', 'primer', 'sequence', 'and', 'to', 'organize', 'words', 'phrases', 'and', 'sentences', 'that', 'correlate', 'with', 'the', 'sequence', 'it', 'presupposes', 'a', 'thorough', 'knowledge', 'of', 'linguistics', 'internet', 'academic', 'books', '@', 'sil', 'org', 'literacy', 'computer'])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Look of data after tokenisation and punctuation removed:\n",
      " [('8-1064msg1', ['Subject', 're', '8', '1044', 'disc', 'grammar', 'in', 'schools', 're', 'message', 'from', 'linguist', '@', 'linguistlist', 'org', '>', '>', 'linguist', 'list', 'vol-8', '-', '1044', 'sat', 'jul', '12', '1997', 'issn', '1068-4875', '>', '>', 'subject', '8', '1044', 'disc', 'grammar', 'in', 'schools', '>', '>', 'i', 'know', 'and', 'teach', 'that', 'not', 'all', 'infinitives', 'contain', '`', 'to', \"'\", 'i', 'also', 'give', '>', 'the', 'students', 'examples', 'e', 'g', '`', 'i', 'asked', 'him', 'to', 'kindly', 'apologise', \"'\", 'where', '>', 'placing', 'the', 'adverb', 'anywhere', 'else', 'would', 'cause', 'ambiguity', '>', '>', 'jennifer', 'chew', 'an', 'example', 'i', 'once', 'concocted', 'to', 'justify', 'splitting', 'the', 'infintive', 'or', 'not', 'as', 'the', 'case', 'may', 'be', 'is', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'in', 'this', 'context', 'with', 'the', 'possible', 'exception', 'of', 'the', 'third', 'case', 'the', 'natural', 'and', 'therefore', 'near-enough', 'unambiguous', 'association', 'of', 'the', 'adverb', 'is', 'as', 'follows', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'this', 'was', 'long', 'ago', 'when', 'you', 'got', 'glared', 'at', 'for', 'splitting', 'an', 'infinitive', 'regardless', 'of', 'whether', 'it', 'was', 'the', 'only', 'place', 'to', 'put', 'the', 'adverb', 'so', 'as', 'to', 'express', 'what', 'you', 'meant', 'and', 'not', 'something', 'else', 'this', 'example', 'achieved', 'as', 'nearly', 'as', 'i', 'could', 'three', 'quite', 'distinct', 'and', 'natural', 'meanings', 'for', 'slowly', 'to', 'go', 'home', 'to', 'slowly', 'go', 'home', 'and', 'to', 'go', 'home', 'slowly', 'i', \"'\", 'm', 'not', '100', 'per', 'cent', 'happy', 'with', 'it', 'for', 'obvious', 'reasons', 'and', 'it', 'would', 'be', 'interesting', 'to', 'see', 'if', 'anyone', 'can', 'come', 'up', 'with', 'a', 'better', 'more', 'clear-cut', 'one', 'and', 'to', 'really', 'epater', 'les', 'bourgeois', 'i', 'reckon', 'you', 'could', 'even', 'make', 'a', 'case', 'for', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'the', 'implication', 'being', 'that', 'the', 'meal', 'was', 'so', 'very', 'heavy', 'that', 'the', 'walk', 'home', 'should', 'be', 'correpondingly', 'delicate', 'as', 'emphasised', 'by', 'the', 'pause', 'in', 'rhythm', 'marked', 'by', 'the', 'commas', 'ted', 'ted', 'harding', '@', 'nessie', 'mcc', 'ac', 'uk']), ('6-806msg1', ['Subject', 'swadesh', 'list', 'does', 'anyone', 'have', 'a', 'copy', 'of', 'the', 'swadesh', 'word', 'list', 'at', 'hand', 'i', 'should', 'like', 'to', 'get', 'a', 'copy', 'by', 'email', 'as', 'soon', 'as', 'is', 'practicable', 'thanks', 'in', 'advance', 'adams', 'bodomo', 'bodomo', '@', 'csli', 'stanford', 'edu'])]\n",
      "\n",
      "\n",
      "Punctuation Removed and Tokenisation completed\n",
      "\n",
      "\n",
      "\n",
      "Tokens before Vectorization:\n",
      " [['Subject', 'job', 'posting', '-', 'apple-iss', 'research', 'center', 'content', '-', 'length', '3386', 'apple-iss', 'research', 'center', 'a', 'us', '$', '10', 'million', 'joint', 'venture', 'between', 'apple', 'computer', 'inc', 'and', 'the', 'institute', 'of', 'systems', 'science', 'of', 'the', 'national', 'university', 'of', 'singapore', 'located', 'in', 'singapore', 'is', 'looking', 'for', 'a', 'senior', 'speech', 'scientist', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'in', 'computational', 'linguistics', 'including', 'natural', 'language', 'processing', 'and', '*', '*', 'english', '*', '*', 'and', '*', '*', 'chinese', '*', '*', 'statistical', 'language', 'modeling', 'knowledge', 'of', 'state-of', '-', 'the-art', 'corpus-based', 'n', '-', 'gram', 'language', 'models', 'cache', 'language', 'models', 'and', 'part-of', '-', 'speech', 'language', 'models', 'are', 'required', 'a', 'text', '-', 'to', '-', 'speech', 'project', 'leader', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'the', 'successful', 'candidate', 'will', 'have', 'research', 'expertise', 'expertise', 'in', 'two', 'or', 'more', 'of', 'the', 'following', 'areas', 'computational', 'linguistics', 'including', 'natural', 'language', 'parsing', 'lexical', 'database', 'design', 'and', 'statistical', 'language', 'modeling', 'text', 'tokenization', 'and', 'normalization', 'prosodic', 'analysis', 'substantial', 'knowledge', 'of', 'the', 'phonology', 'syntax', 'and', 'semantics', 'of', 'chinese', 'is', 'required', 'knowledge', 'of', 'acoustic', 'phonetics', 'and', '/', 'or', 'speech', 'signal', 'processing', 'is', 'desirable', 'both', 'candidates', 'will', 'have', 'a', 'phd', 'with', 'at', 'least', '2', 'to', '4', 'years', 'of', 'relevant', 'work', 'experience', 'or', 'a', 'technical', 'msc', 'degree', 'with', 'at', 'least', '5', 'to', '7', 'years', 'of', 'experienc', 'e', 'very', 'strong', 'software', 'engineering', 'skills', 'including', 'design', 'and', 'implementation', 'and', 'productization', 'are', 'required', 'in', 'these', 'positions', 'knowledge', 'of', 'c', 'c', '+', '+', 'and', 'unix', 'are', 'preferred', 'a', 'unix', '&', 'c', 'programmer', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'we', 'are', 'looking', 'for', 'an', 'experienced', 'unix', '&', 'c', 'programmer', 'preferably', 'with', 'good', 'industry', 'experience', 'to', 'join', 'us', 'in', 'breaking', 'new', 'frontiers', 'strong', 'knowledge', 'of', 'unix', 'tools', 'compilers', 'linkers', 'make', 'x', '-', 'windows', 'e', '-', 'mac', 'and', 'experience', 'in', 'matlab', 'required', 'sun', 'and', 'silicon', 'graphic', 'experience', 'is', 'an', 'advantage', 'programmers', 'with', 'less', 'than', 'two', 'years', 'industry', 'experience', 'need', 'not', 'apply', 'these', 'positions', 'include', 'interaction', 'with', 'scientists', 'in', 'the', 'national', 'university', 'of', 'singapore', 'and', 'with', 'apple', \"'s\", 'speech', 'research', 'and', 'productization', 'efforts', 'located', 'in', 'cupertino', 'california', 'attendance', 'and', 'publication', 'in', 'international', 'scientific', '/', 'engineering', 'conferences', 'is', 'encouraged', 'benefits', 'include', 'an', 'internationally', 'competitive', 'salary', 'housing', 'subsidy', 'and', 'relocation', 'expenses', 'send', 'a', 'complete', 'resume', 'enclosing', 'personal', 'particulars', 'qualifications', 'experience', 'and', 'contact', 'telephone', 'number', 'to', 'mr', 'jean', '-', 'luc', 'lebrun', 'center', 'manager', 'apple', '-', 'iss', 'research', 'center', 'institute', 'of', 'systems', 'science', 'heng', 'mui', 'keng', 'terrace', 'singapore', '0511', 'tel', '65', '772-6571', 'fax', '65', '776-4005', 'email', 'jllebrun', '@', 'iss', 'nus', 'sg']]\n",
      "\n",
      "\n",
      "Tokens after Vectorization:\n",
      " [[78, 56, 88, 59, 69]]\n",
      "\n",
      "\n",
      "Tokens before Vectorization:\n",
      " [['Subject', 're', '8', '1044', 'disc', 'grammar', 'in', 'schools', 're', 'message', 'from', 'linguist', '@', 'linguistlist', 'org', '>', '>', 'linguist', 'list', 'vol-8', '-', '1044', 'sat', 'jul', '12', '1997', 'issn', '1068-4875', '>', '>', 'subject', '8', '1044', 'disc', 'grammar', 'in', 'schools', '>', '>', 'i', 'know', 'and', 'teach', 'that', 'not', 'all', 'infinitives', 'contain', '`', 'to', \"'\", 'i', 'also', 'give', '>', 'the', 'students', 'examples', 'e', 'g', '`', 'i', 'asked', 'him', 'to', 'kindly', 'apologise', \"'\", 'where', '>', 'placing', 'the', 'adverb', 'anywhere', 'else', 'would', 'cause', 'ambiguity', '>', '>', 'jennifer', 'chew', 'an', 'example', 'i', 'once', 'concocted', 'to', 'justify', 'splitting', 'the', 'infintive', 'or', 'not', 'as', 'the', 'case', 'may', 'be', 'is', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'in', 'this', 'context', 'with', 'the', 'possible', 'exception', 'of', 'the', 'third', 'case', 'the', 'natural', 'and', 'therefore', 'near-enough', 'unambiguous', 'association', 'of', 'the', 'adverb', 'is', 'as', 'follows', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'this', 'was', 'long', 'ago', 'when', 'you', 'got', 'glared', 'at', 'for', 'splitting', 'an', 'infinitive', 'regardless', 'of', 'whether', 'it', 'was', 'the', 'only', 'place', 'to', 'put', 'the', 'adverb', 'so', 'as', 'to', 'express', 'what', 'you', 'meant', 'and', 'not', 'something', 'else', 'this', 'example', 'achieved', 'as', 'nearly', 'as', 'i', 'could', 'three', 'quite', 'distinct', 'and', 'natural', 'meanings', 'for', 'slowly', 'to', 'go', 'home', 'to', 'slowly', 'go', 'home', 'and', 'to', 'go', 'home', 'slowly', 'i', \"'\", 'm', 'not', '100', 'per', 'cent', 'happy', 'with', 'it', 'for', 'obvious', 'reasons', 'and', 'it', 'would', 'be', 'interesting', 'to', 'see', 'if', 'anyone', 'can', 'come', 'up', 'with', 'a', 'better', 'more', 'clear-cut', 'one', 'and', 'to', 'really', 'epater', 'les', 'bourgeois', 'i', 'reckon', 'you', 'could', 'even', 'make', 'a', 'case', 'for', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'the', 'implication', 'being', 'that', 'the', 'meal', 'was', 'so', 'very', 'heavy', 'that', 'the', 'walk', 'home', 'should', 'be', 'correpondingly', 'delicate', 'as', 'emphasised', 'by', 'the', 'pause', 'in', 'rhythm', 'marked', 'by', 'the', 'commas', 'ted', 'ted', 'harding', '@', 'nessie', 'mcc', 'ac', 'uk']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tokens after Vectorization:\n",
      " [[78, 56, 88, 59, 69]]\n",
      "\n",
      "\n",
      "Train and Test turned into Vectors of Fixed Dimensions and Target labelled\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data are now like:\n",
      " [LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0419613765928144,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0104903441482036,0.0104903441482036,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0209806882964072,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0209806882964072,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0419613765928144,0.0,0.0,0.0,0.0104903441482036,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0314710324446108,0.0104903441482036,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0629420648892216,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0314710324446108,0.0,0.0,0.0,0.0314710324446108,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0314710324446108,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0419613765928144,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0209806882964072,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0734324090374252,0.0,0.05245172074101801,0.0,0.0,0.0,0.0209806882964072,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.18882619466766482,0.0,0.0,0.0,0.0629420648892216,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0104903441482036,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0104903441482036,0.0,0.0,0.0,0.0104903441482036,0.0,0.0104903441482036,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05245172074101801,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0419613765928144,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0104903441482036,0.0209806882964072,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0314710324446108,0.0104903441482036,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0314710324446108,0.0839227531856288,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.05245172074101801,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0104903441482036,0.0104903441482036,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0104903441482036,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0314710324446108,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0104903441482036,0.0,0.0419613765928144,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0314710324446108,0.0104903441482036,0.0104903441482036,0.0,0.0104903441482036,0.09441309733383241,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1468648180748504,0.0104903441482036,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0629420648892216,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.05245172074101801,0.9126599408937134,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0629420648892216,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0419613765928144,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0104903441482036,0.0,0.0,0.0104903441482036,0.0,0.0209806882964072,0.0104903441482036,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0104903441482036,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0734324090374252,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0104903441482036,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0734324090374252,0.0104903441482036,0.0,0.0104903441482036,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0104903441482036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0209806882964072,0.0,0.0104903441482036,0.0,0.0,0.0,0.0]), LabeledPoint(0.0, [0.0,0.0,0.03575992699260758,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.07151985398521515,0.0,0.03575992699260758,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.07151985398521515,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.10727978097782273,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07151985398521515,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10727978097782273,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07151985398521515,0.0,0.0,0.0,0.0,0.0,0.07151985398521515,0.1430397079704303,0.03575992699260758,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.10727978097782273,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.07151985398521515,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.07151985398521515,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.03575992699260758,0.07151985398521515,0.0,0.07151985398521515,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.07151985398521515,0.03575992699260758,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.07151985398521515,0.0,0.10727978097782273,0.0,0.0,0.0,0.0,0.0,0.07151985398521515,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07151985398521515,0.0,0.0,0.07151985398521515,0.03575992699260758,0.03575992699260758,0.0,0.0,0.07151985398521515,0.03575992699260758,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.1430397079704303,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.25031948894825307,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.3218393429334682,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.03575992699260758,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.03575992699260758,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07151985398521515,0.0,0.10727978097782273,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10727978097782273,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.07151985398521515,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10727978097782273,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07151985398521515,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.07151985398521515,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07151985398521515,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.10727978097782273,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10727978097782273,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.10727978097782273,0.0,0.0,0.0,0.21455956195564546,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10727978097782273,0.0,0.0,0.0,0.0,0.0,0.0,0.25031948894825307,0.0,0.0,0.07151985398521515,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.21455956195564546,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17879963496303788,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07151985398521515,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.10727978097782273,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10727978097782273,0.0,0.0,0.0,0.0,0.0,0.0,0.10727978097782273,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17879963496303788,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07151985398521515,0.35759926992607577,0.0,0.0,0.0,0.07151985398521515,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.07151985398521515,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.03575992699260758,0.0,0.0,0.0,0.0,0.10727978097782273,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10727978097782273,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.03575992699260758,0.0,0.0,0.0,0.0,0.0,0.0]), LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08226900697897271,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08226900697897271,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.04113450348948636,0.04113450348948636,0.16453801395794543,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.08226900697897271,0.20567251744743176,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.08226900697897271,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.12340351046845906,0.0,0.12340351046845906,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08226900697897271,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.08226900697897271,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.12340351046845906,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.08226900697897271,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08226900697897271,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.08226900697897271,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16453801395794543,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.08226900697897271,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.04113450348948636,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.08226900697897271,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.08226900697897271,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.08226900697897271,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12340351046845906,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08226900697897271,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.08226900697897271,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12340351046845906,0.0,0.0,0.0,0.0,0.0,0.0,0.12340351046845906,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12340351046845906,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12340351046845906,0.0,0.0,0.0,0.04113450348948636,0.0,0.16453801395794543,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.04113450348948636,0.0,0.0,0.0,0.04113450348948636,0.0,0.04113450348948636,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.16453801395794543,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.20567251744743176,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.08226900697897271,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12340351046845906,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.08226900697897271,0.0,0.0,0.0,0.0,0.12340351046845906,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12340351046845906,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24680702093691811,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08226900697897271,0.0,0.16453801395794543,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.08226900697897271,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24680702093691811,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08226900697897271,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08226900697897271,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12340351046845906,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12340351046845906,0.0,0.0,0.0,0.0,0.08226900697897271,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16453801395794543,0.0,0.0,0.0,0.08226900697897271,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12340351046845906,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28794152442640447,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.04113450348948636,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08226900697897271,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08226900697897271,0.04113450348948636,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.04113450348948636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Data are now like:\n",
      " [LabeledPoint(0.0, [0.0,0.0,0.0,0.09081532183729997,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.06811149137797498,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15892681321527494,0.0,0.0,0.0,0.0,0.06811149137797498,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.022703830459324992,0.022703830459324992,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.06811149137797498,0.0,0.3178536264305499,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.06811149137797498,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.09081532183729997,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.06811149137797498,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.15892681321527494,0.0,0.0,0.06811149137797498,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24974213505257492,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15892681321527494,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.15892681321527494,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15892681321527494,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.06811149137797498,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3859651178085249,0.022703830459324992,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06811149137797498,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.022703830459324992,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09081532183729997,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06811149137797498,0.0,0.22703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.06811149137797498,0.0,0.0,0.09081532183729997,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.06811149137797498,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.22703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.13622298275594996,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.22703830459324992,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3405574568898749,0.0,0.0,0.0,0.06811149137797498,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13622298275594996,0.022703830459324992,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.24974213505257492,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.045407660918649985,0.022703830459324992,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045407660918649985,0.0,0.0,0.0,0.0,0.0,0.022703830459324992,0.0,0.0,0.0,0.0,0.0,0.0]), LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.28005601680560194,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.28005601680560194,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28005601680560194,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28005601680560194,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28005601680560194,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28005601680560194,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0,0.0,0.14002800840280097,0.0,0.0,0.0,0.0]), LabeledPoint(0.0, [0.0,0.0,0.0,0.044836087446915626,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.02989072496461042,0.01494536248230521,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.02989072496461042,0.02989072496461042,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.11956289985844168,0.0,0.01494536248230521,0.0,0.0,0.01494536248230521,0.0,0.044836087446915626,0.01494536248230521,0.0,0.0,0.0,0.0,0.05978144992922084,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.044836087446915626,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.02989072496461042,0.044836087446915626,0.01494536248230521,0.0,0.0,0.0,0.02989072496461042,0.01494536248230521,0.01494536248230521,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.01494536248230521,0.0,0.0,0.08967217489383125,0.0,0.01494536248230521,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.044836087446915626,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.01494536248230521,0.0,0.01494536248230521,0.01494536248230521,0.0,0.0,0.0,0.0,0.01494536248230521,0.01494536248230521,0.0,0.0,0.0,0.0,0.07472681241152605,0.0,0.01494536248230521,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.01494536248230521,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.01494536248230521,0.0,0.01494536248230521,0.0,0.02989072496461042,0.0,0.02989072496461042,0.01494536248230521,0.07472681241152605,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.01494536248230521,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.01494536248230521,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.01494536248230521,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.01494536248230521,0.01494536248230521,0.02989072496461042,0.0,0.01494536248230521,0.0,0.01494536248230521,0.0,0.0,0.0,0.1793443497876625,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.01494536248230521,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.044836087446915626,0.0,0.0,0.20923507475227293,0.0,0.0,0.044836087446915626,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.08967217489383125,0.0,0.02989072496461042,0.0,0.01494536248230521,0.0,0.044836087446915626,0.0,0.0,0.0,0.01494536248230521,0.0,0.044836087446915626,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07472681241152605,0.05978144992922084,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10461753737613647,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.044836087446915626,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.01494536248230521,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08967217489383125,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.01494536248230521,0.0,0.01494536248230521,0.0,0.01494536248230521,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.01494536248230521,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.1643989873053573,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.044836087446915626,0.05978144992922084,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.01494536248230521,0.0,0.0,0.05978144992922084,0.0,0.0,0.01494536248230521,0.0,0.044836087446915626,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.1793443497876625,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.01494536248230521,0.0,0.0,0.08967217489383125,0.0,0.01494536248230521,0.01494536248230521,0.01494536248230521,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.05978144992922084,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.044836087446915626,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10461753737613647,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.01494536248230521,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.01494536248230521,0.01494536248230521,0.05978144992922084,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.22418043723457814,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.08967217489383125,0.01494536248230521,0.1643989873053573,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.08967217489383125,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.10461753737613647,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.07472681241152605,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.01494536248230521,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.044836087446915626,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.1345082623407469,0.0,0.0,0.044836087446915626,0.0,0.02989072496461042,0.0,0.0,0.0,0.01494536248230521,0.01494536248230521,0.0,0.0,0.01494536248230521,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.05978144992922084,0.0,0.0,0.0,0.0,0.0,0.0,0.19428971226996772,0.0,0.02989072496461042,0.07472681241152605,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.044836087446915626,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.05978144992922084,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.044836087446915626,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.07472681241152605,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.01494536248230521,0.01494536248230521,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.01494536248230521,0.0,0.0,0.0,0.01494536248230521,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.02989072496461042,0.044836087446915626,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.044836087446915626,0.0,0.02989072496461042,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.044836087446915626,0.0,0.01494536248230521,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.044836087446915626,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.01494536248230521,0.0,0.6127598617745136,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.02989072496461042,0.0,0.1494536248230521,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.01494536248230521,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.1494536248230521,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.05978144992922084,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.01494536248230521,0.0,0.0,0.0,0.044836087446915626,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.044836087446915626,0.01494536248230521,0.01494536248230521,0.0,0.01494536248230521,0.0,0.0,0.02989072496461042,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.01494536248230521,0.0,0.07472681241152605,0.0,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.01494536248230521,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.02989072496461042,0.0,0.0,0.0,0.0,0.0,0.0,0.07472681241152605,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05978144992922084,0.0,0.0,0.0,0.0,0.044836087446915626,0.0,0.0,0.0,0.01494536248230521,0.0,0.0,0.0,0.0,0.0,0.0])]\n",
      "\n",
      "\n",
      " Data Normalised and LabeledPoints Local Vector set\n",
      "\n",
      "Training and Testing different models:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:\n",
      ". Training data = 1.0 \n",
      ". Test data =  0.9619377162629758\n",
      " - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Naive Bayes Accuracy:\n",
      ". Training data = 0.8940092165898618 \n",
      ". Test data =  0.8685121107266436\n",
      " - - - - - - - - - - - - - - - - - - - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is going to lead to an excessively verbose output. I have already mention the necessity of defining in the \n",
    "# function a parameter 'print = True' ( see the conclusions for more ).\n",
    "# on the other side, this has its advantages, since it allows to check if any mistake has been done,\n",
    "# and also check the shape of the vector.\n",
    "# ( APOLOGIES for the mess you are going to see !) \n",
    "\n",
    "modify_vector_size('bare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To overcome my mistake, I am going to manually paste the result in here:\n",
    "In the future.. ( see conclusions ) \n",
    "\n",
    "**VECTOR SIZE**: 100\n",
    "\n",
    "Logistic Regression Accuracy:\n",
    ". Training data = 0.9746543778801844 \n",
    ". Test data =  0.9550173010380623\n",
    " - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "Naive Bayes Accuracy:\n",
    ". Training data = 0.8337173579109063 \n",
    ". Test data =  0.8339100346020761\n",
    " - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "Support Vector Machine Accuracy:\n",
    ". Training data = 0.8337173579109063 \n",
    ". Test data =  0.8339100346020761\n",
    " - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "**VECTOR SIZE: 500** <- *******\n",
    " \n",
    "**Logistic Regression Accuracy:\n",
    ". Training data = 1.0 \n",
    ". Test data =  0.9688581314878892**       <- ********************************\n",
    " - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "Naive Bayes Accuracy:\n",
    ". Training data = 0.8529185867895546 \n",
    ". Test data =  0.8442906574394463\n",
    " - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "Support Vector Machine Accuracy:\n",
    ". Training data = 0.8337173579109063 \n",
    ". Test data =  0.8339100346020761\n",
    " - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "\n",
    "**VECTOR SIZE**: 1000\n",
    " \n",
    " Logistic Regression Accuracy:\n",
    ". Training data = 1.0 \n",
    ". Test data =  0.9688581314878892\n",
    " - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "Naive Bayes Accuracy:\n",
    ". Training data = 0.8870967741935484 \n",
    ". Test data =  0.8615916955017301\n",
    " - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "Support Vector Machine Accuracy:\n",
    ". Training data = 0.8337173579109063 \n",
    ". Test data =  0.8339100346020761\n",
    " - - - - - - - - - - - - - - - - - - - \n",
    " \n",
    " \n",
    "**VECTOR SIZE**: 1500\n",
    " \n",
    "\n",
    "Logistic Regression Accuracy:\n",
    ". Training data = 1.0 \n",
    ". Test data =  0.9619377162629758\n",
    " - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "Naive Bayes Accuracy:\n",
    ". Training data = 0.8940092165898618 \n",
    ". Test data =  0.8685121107266436\n",
    " - - - - - - - - - - - - - - - - - - -\n",
    " Support Vector Machine Accuracy:\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This script and analysis have shown a complete process of Spam Detection modelling from the beginning to the end.\n",
    "In particular, data have been accessed and imported. They have been cleaned using NLP techniques of Tokenization and removed of their punctuations ( keeping some of them that can be a discriminant to identify spam emails such as '?' or '!'. The Text files have been converted into Vectors of fixed dimensions and Encode the Target into a Binary variable. Then, their values have been normalised and turn the tuple Label/Dense Vector into LabeledPoints local vectors. Different classification models have been run. In particular, Logistic Regression, Naive Bayes and Support Vector Machines. In this context, Logistic Regression seems to have slightly given better prediction in the accuracy than the other two models, with the other two models that have struggled to increase their accuracy above the baseline when the vector size was still too small. Moreover, I have tried to verify how an increase of the vector size inputted into the model could change its accuracy, and I can conclude that the best size to train the data is 500 ( no difference with 1000 ) , with the best model to chose as final the Logistic Regression,\n",
    "with an accuracy of 96.89 on the test data.\n",
    "For future analysis I would suggest to continue the analysis by eventually including other methods such as Tree Based methods or Neural Networks algorithms, and/or continuing the analysing by playing with the parameters of each methods to optimise its accuracy. \n",
    "Moreover, due to an highly unbalanced data, for future analysis it may be necessary (if not fundamental) to work on methods of data balancing, in which I have to intervene on the data structure to have the outputs 0 and 1 more balanced. At this purpose, techniques like down-sampling or up-sampling, or hybrid ones such as SMOTE, are fundamental.\n",
    "Lastly, it can be very useful for future analysis to work also on post-processing methods that can allow to increase the accuracy, by chosing different evaluation methods such as the AUC, and work by defining the best point in the ROC curve to increase the performance of the algorithm.\n",
    "I could also intervene on the NLP operation, by cleaning the data in a different way, or trying removing stop words or different type of punctuation.\n",
    "From a stylistical point of view, in the future I strongly believe it will be necessary to set a parameter 'print = True' ( usually defined as 'verbose') in each of the function in order to avoid the function to print out longer outputs when these are not required, such as when I looped the function over different size vectors to verify the evolution of the accuracy over different models. Alternatively, I could have found a way to summarise the result at the end of the process, but this would not have been a very efficient way to intervene on this set of codes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
